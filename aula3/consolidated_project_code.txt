--- Conteúdo de src/agents/extraction_agent.py ---

# src/agents/extraction_agent.py
import json
import re
import warnings
# LLM IMPORTS
from langchain_openai import OpenAI as OpenAIStandard
from langchain_openai import AzureOpenAI            # Para OpenAI legada (gpt-3.5-turbo-instruct)
from langchain_google_genai import ChatGoogleGenerativeAI # Para Gemini
# from langchain_openai import ChatOpenAI # Se fosse usar modelos de chat da OpenAI como gpt-3.5-turbo

import logging
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import openai # Para tipos de exceção específicos da OpenAI
import requests # Para tipos de exceção específicos de requests
# Google API Core Exceptions (para Gemini)
try:
    from google.api_core import exceptions as google_api_exceptions
except ImportError:
    google_api_exceptions = None # Permite rodar sem google-generativeai instalado se não for usado

import config

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)

# Define quais exceções devem acionar uma retentativa
RETRYABLE_EXCEPTIONS_OPENAI = (
    openai.RateLimitError,
    openai.APITimeoutError,
    openai.APIConnectionError,
    openai.InternalServerError, # Inclui 500, 502, 503, 504
)

RETRYABLE_EXCEPTIONS_GOOGLE = ()
if google_api_exceptions:
    RETRYABLE_EXCEPTIONS_GOOGLE = (
        google_api_exceptions.ResourceExhausted, # Equivalente a RateLimitError
        google_api_exceptions.DeadlineExceeded,  # Equivalente a APITimeoutError
        google_api_exceptions.ServiceUnavailable,# Equivalente a InternalServerError (503)
        google_api_exceptions.InternalServerError, # Erro interno do Google
    )

RETRYABLE_EXCEPTIONS_REQUESTS = (
    requests.exceptions.Timeout,
    requests.exceptions.ConnectionError,
)

# Combina todas as exceções retryable
ALL_RETRYABLE_EXCEPTIONS = RETRYABLE_EXCEPTIONS_OPENAI + \
                           RETRYABLE_EXCEPTIONS_GOOGLE + \
                           RETRYABLE_EXCEPTIONS_REQUESTS

class ExtractionAgent:
    """
    Agente responsável por extrair informações estruturadas de texto bruto
    usando um Large Language Model (LLM), com suporte a contexto.
    """
    def __init__(self, prompt_file="prompts/extraction_prompt.txt"):
        """Inicializa o agente de extração."""
        self._setup_llm()
        self._load_prompt_template(prompt_file)
        self.extraction_chain = LLMChain(llm=self.llm, prompt=self.extraction_prompt)
        logger.info(f"Agente de Extração inicializado com provedor: {config.LLM_PROVIDER}.")

    def _setup_llm(self):
        """Configura a instância do LLM com base nas configurações."""
        provider = config.LLM_PROVIDER

        if provider == "openai": # Mantenha se quiser usar a API OpenAI padrão como fallback
            if not config.OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY não configurada para o provedor OpenAI padrão")
            # Esta parte seria para OpenAI padrão, não Azure.
            # Se você só vai usar Azure, pode remover ou adaptar este bloco.
            # Para usar gpt-3.5-turbo-instruct com OpenAI padrão (não Azure):
            self.llm = OpenAIStandard(
                temperature=config.LLM_TEMPERATURE,
                openai_api_key=config.OPENAI_API_KEY,
                model_name=config.OPENAI_MODEL_NAME, # gpt-3.5-turbo-instruct
                max_tokens=config.LLM_MAX_TOKENS
            )
            logger.info(f"LLM configurado: OpenAI Padrão ({config.OPENAI_MODEL_NAME})")


        elif provider == "azure_openai": # NOVO ou ajustado provedor
            logger.info("--- DEBUG: Entrou no bloco azure_openai ---")
            if not all([config.AZURE_OPENAI_API_KEY, config.AZURE_OPENAI_ENDPOINT,
                        config.AZURE_OPENAI_DEPLOYMENT_NAME, config.AZURE_OPENAI_API_VERSION,  config.AZURE_MODEL_NAME]):
                raise ValueError("Configurações do Azure OpenAI (API_KEY, ENDPOINT, DEPLOYMENT_NAME, API_VERSION) incompletas.")
            logger.info(f"--- DEBUG: Configurações Azure: Endpoint='{config.AZURE_OPENAI_ENDPOINT}', Deployment='{config.AZURE_OPENAI_DEPLOYMENT_NAME}', ModelName='{config.AZURE_MODEL_NAME}', APIVersion='{config.AZURE_OPENAI_API_VERSION}' ---")

            self.llm = AzureOpenAI ( # Usando a classe AzureOpenAI da Langchain
                azure_endpoint=config.AZURE_OPENAI_ENDPOINT,
                api_key=config.AZURE_OPENAI_API_KEY, # Langchain espera api_key aqui, não credential
                azure_deployment=config.AZURE_OPENAI_DEPLOYMENT_NAME, # Nome da sua implantação do gpt-3.5-turbo-instruct
                api_version=config.AZURE_OPENAI_API_VERSION,
                max_tokens=config.LLM_MAX_TOKENS,
                temperature=config.LLM_TEMPERATURE,
                model_name=config.AZURE_MODEL_NAME, # Opcional se deployment_name for específico.
                                                            # Se o deployment puder servir múltiplos modelos, pode ser necessário.
                                                            # Para gpt-3.5-turbo-instruct, deployment_name é suficiente.
            )
            logger.info(f"--- DEBUG TIPO LLM: Tipo de self.llm instanciado para Azure: {type(self.llm)} ---")
            logger.info(f"LLM configurado: Azure OpenAI (Completion Model - Deployment: {config.AZURE_OPENAI_DEPLOYMENT_NAME}, Model: {config.AZURE_MODEL_NAME}, Endpoint: {config.AZURE_OPENAI_ENDPOINT})")

        elif provider == "google_genai":
            if not config.GOOGLE_API_KEY:
                raise ValueError("GOOGLE_API_KEY não configurada no .env ou config.py para o provedor Google GenAI")
            if not ChatGoogleGenerativeAI:
                raise ImportError("Pacote langchain-google-genai não instalado. Execute: pip install langchain-google-genai")
            
            self.llm = ChatGoogleGenerativeAI(
                model=config.GEMINI_MODEL_NAME,
                google_api_key=config.GOOGLE_API_KEY,
                temperature=config.LLM_TEMPERATURE,
                max_output_tokens=config.LLM_MAX_TOKENS, # Para Gemini
            )
            logger.info(f"LLM configurado: Google Gemini ({config.GEMINI_MODEL_NAME})")
        else:
            raise ValueError(f"Provedor LLM '{provider}' não suportado. Verifique LLM_PROVIDER em config.py ou .env.")
        logger.info(f"--- DEBUG FIM _setup_llm: LLM configurado. Tipo final: {type(self.llm)} ---")

    def _load_prompt_template(self, prompt_file: str):
        """Carrega o template do prompt de um arquivo."""
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                prompt_template_string = f.read()
        except FileNotFoundError:
            logger.error(f"Arquivo de prompt não encontrado em: {prompt_file}")
            raise
        except Exception as e:
            logger.error(f"Erro ao ler o arquivo de prompt {prompt_file}: {e}")
            raise IOError(f"Erro ao ler o arquivo de prompt {prompt_file}: {e}")

        logger.info(f"--- DEBUG ANTES LLMChain: Tipo de self.llm: {type(self.llm)} ao carregar prompt ---")
        self.extraction_prompt = PromptTemplate(
            input_variables=["input_text", "context_instruction"],
            template=prompt_template_string
        )

    def _generate_context_instruction(self, context_fields: list[str] = None, custom_instruction: str = None) -> str:
        if custom_instruction:
            logger.debug(f"Instrução de contexto PERSONALIZADA usada:\n---\n{custom_instruction.strip()}\n---")
            return custom_instruction + "\n"
        elif context_fields:
            fields_str = ", ".join(context_fields)
            instruction = (
                f"ATENÇÃO: O usuário está respondendo a uma pergunta específica sobre os seguintes campos: {fields_str}. "
                "Concentre-se em extrair APENAS as informações para estes campos a partir do 'Input do Usuário' abaixo. "
                "Para TODOS os outros campos não mencionados explicitamente no Input do Usuário ATUAL, retorne o valor `null`."
                "Sua única tarefa é analisar o 'Input do Usuário' abaixo e extrair informações **APENAS** para estes campos (`{fields_str}`)."
                "Ignore completamente qualquer outra informação no input que não seja diretamente relevante para estes campos específicos."
                "Para TODOS os outros campos do schema JSON completo que NÃO foram solicitados agora (`{fields_str}`), retorne obrigatoriamente o valor `null`."
                "Se, mesmo focando nos campos solicitados (`{fields_str}`), a informação para um deles não estiver presente no 'Input do Usuário' atual, retorne `null` para esse campo também."
                "\n"
            )
            logger.debug(f"Instrução de contexto (campos) gerada: {instruction.strip()}")
            return instruction
        else:
            logger.debug("Nenhuma instrução de contexto necessária (input geral).")
            return ""


    def _clean_and_load_json(self, llm_response_text: str) -> dict | None:
        if llm_response_text and llm_response_text.strip().startswith("<!DOCTYPE html"):
            logger.error("Resposta do LLM parece ser uma página de erro HTML.")
            logger.debug(f"Resposta HTML completa:\n{llm_response_text[:500]}...")
            return None

        match = re.search(r'\{.*\}', llm_response_text, re.DOTALL)
        if not match:
            logger.error("JSON não encontrado na resposta do LLM.")
            logger.debug(f"Resposta completa sem JSON:\n{llm_response_text}")
            return None

        json_str = match.group(0)
        cleaned_json_str = json_str.replace('\t', '    ') # Substitui tab por 4 espaços

        if cleaned_json_str != json_str:
            logger.debug(f"String JSON foi limpa. Original (início): '{json_str[:200]}...', Limpa (início): '{cleaned_json_str[:200]}...'")

        try:
            # Tenta decodificar a string JSON limpa
            return json.loads(cleaned_json_str)
        except json.JSONDecodeError as json_e:
            # Se a decodificação falhar, registra a string que foi usada para a tentativa (a limpa, se diferente)
            log_json_str_on_error = cleaned_json_str
            logger.error(f"Falha ao decodificar JSON encontrado na resposta: {json_e}. String JSON (usada para parse): '{log_json_str_on_error}'")
            # Se a limpeza alterou a string, também registra a original para comparação no debug
            if cleaned_json_str != json_str:
                logger.debug(f"String JSON ORIGINAL (antes da limpeza): '{json_str}'")
            return None

    def _post_process_extracted_data(self, data: dict) -> dict:
        if 'CNPJ/CPF' in data and data['CNPJ/CPF']:
            data['CNPJ/CPF'] = re.sub(r'[./-]', '', str(data['CNPJ/CPF']))
        for field in ["Preço Frete", "Valor"]:
            if field in data and data[field] is not None:
                value_str = str(data[field])
                logger.debug(f"Pós-processando campo '{field}'. Valor original: '{value_str}'")
                cleaned_str = re.sub(r'[R$\s]', '', value_str).strip()
                logger.debug(f"Após remoção R$/espaços: '{cleaned_str}'")
                if not cleaned_str:
                    logger.warning(f"Campo '{field}' ficou vazio após limpeza inicial de '{value_str}'. Definindo como None.")
                    data[field] = None
                    continue
                normalized_float_str = None
                try:
                    num_dots = cleaned_str.count('.')
                    num_commas = cleaned_str.count(',')
                    if num_commas == 1 and num_dots >= 0:
                        last_dot_pos = cleaned_str.rfind('.')
                        comma_pos = cleaned_str.rfind(',')
                        if comma_pos > last_dot_pos:
                            normalized_float_str = cleaned_str.replace('.', '').replace(',', '.')
                            logger.debug(f"Detectado formato BR provável. Normalizado para: '{normalized_float_str}'")
                        else:
                            normalized_float_str = cleaned_str.replace(',', '')
                            logger.debug(f"Detectado formato US/Intl provável. Normalizado para: '{normalized_float_str}'")
                    elif num_dots >= 1 and num_commas == 0:
                        logger.debug(f"Campo '{field}': {cleaned_str} tem {num_dots} ponto(s) e 0 vírgulas.")
                        parts = cleaned_str.split('.')
                        if num_dots == 1 and len(parts[-1]) == 3 and parts[0] != "":
                            normalized_float_str = "".join(parts)
                            logger.debug(f"Detectado formato tipo 'X.XXX'. Normalizado para: '{normalized_float_str}'")
                        elif len(parts[-1]) < 3:
                            normalized_float_str = "".join(parts[:-1]) + "." + parts[-1]
                            logger.debug(f"Detectado formato com último ponto decimal. Normalizado para: '{normalized_float_str}'")
                        else:
                            normalized_float_str = "".join(parts)
                            logger.debug(f"Detectado formato com múltiplos pontos. Normalizado para: '{normalized_float_str}'")
                    elif num_dots == 0 and num_commas == 0:
                         normalized_float_str = cleaned_str
                         logger.debug(f"Detectado formato inteiro. Normalizado para: '{normalized_float_str}'")
                    else:
                         logger.warning(f"Formato numérico ambíguo para '{cleaned_str}'. Fallback.")
                         normalized_float_str = cleaned_str.replace('.', '').replace(',', '.')
                    if not normalized_float_str:
                         logger.warning(f"Campo '{field}' ficou vazio após normalização de '{cleaned_str}'. Definindo como None.")
                         data[field] = None
                         continue
                    data[field] = float(normalized_float_str)
                    logger.debug(f"Campo '{field}' convertido para float: {data[field]}")
                except (ValueError, TypeError) as e:
                    log_norm_str = normalized_float_str if normalized_float_str is not None else '<Falha na Normalização>'
                    logger.warning(f"Não foi possível converter '{field}' (Original: '{value_str}' -> Limpo: '{cleaned_str}' -> Norm: '{log_norm_str}') para float. Erro: {e}. Definindo como None.")
                    data[field] = None
                except Exception as ex:
                    log_norm_str = normalized_float_str if normalized_float_str is not None else '<Falha na Normalização>'
                    logger.error(f"Erro inesperado ao processar campo numérico '{field}' (Original: '{value_str}' -> Norm: '{log_norm_str}'): {ex}", exc_info=True)
                    data[field] = None
        return data

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type(ALL_RETRYABLE_EXCEPTIONS), # Usar a lista combinada
        reraise=True
    )
    def _invoke_llm_chain_with_retry(self, input_dict: dict) -> dict | str:
        """Invoca a cadeia LLM com lógica de retentativa."""
        logger.debug(f"Invocando LLM chain com provedor: {config.LLM_PROVIDER}...")
        response = self.extraction_chain.invoke(input_dict)
        logger.debug(f"LLM chain invocado com sucesso. Tipo da Resposta: {type(response)}")
        return response

    def extract(self, text: str, context_fields: list[str] = None, custom_instruction: str = None) -> dict | None:
        logger.debug(f"Texto para extração (repr): {repr(text)}")
        context_instruction = self._generate_context_instruction(context_fields, custom_instruction)
        input_payload = {
            'input_text': text,
            'context_instruction': context_instruction
        }

        try:
            response = self._invoke_llm_chain_with_retry(input_payload)
            
            if isinstance(response, dict):
                result_text = response.get('text', '')
                if not result_text and response:
                    for val in response.values():
                        if isinstance(val, str):
                            result_text = val
                            break
            elif isinstance(response, str):
                result_text = response
            else:
                logger.error(f"Resposta inesperada do LLM chain: {type(response)}. Conteúdo: {response}")
                result_text = ""

            logger.debug(f"Resposta BRUTA do LLM (após extração da chain): ---{result_text}---")

            extracted_data = self._clean_and_load_json(result_text)
            if not extracted_data:
                logger.error("Falha ao extrair JSON da resposta do LLM mesmo após retentativas.")
                return None

            if isinstance(extracted_data, dict):
                llm_valor = extracted_data.get("Valor")
                llm_preco = extracted_data.get("Preço") # 'Preço' como chave extraída pelo LLM
            
                if llm_valor is None and llm_preco is not None:
                    logger.info(f"Heurística: 'Valor' é None e 'Preço' ('{llm_preco}') existe. Copiando 'Preço' para 'Valor'.")
                    extracted_data["Valor"] = llm_preco

            processed_data = self._post_process_extracted_data(extracted_data)
            logger.debug(f"Dados extraídos (JSON válido e processado):\n{json.dumps(processed_data, indent=2, ensure_ascii=False)}")
            return processed_data

        except ALL_RETRYABLE_EXCEPTIONS as e:
             logger.error(f"Erro persistente ao chamar LLM após retentativas: {e}", exc_info=True)
             return None
        except Exception as e:
            logger.error(f"Erro inesperado na extração: {e}", exc_info=True)
            return None

--- Conteúdo de src/agents/mapping_agent.py ---

import pandas as pd
import os
import re
import json # Para debug
import logging
from utils.normalization import normalize_string
import config
from thefuzz import fuzz

logger = logging.getLogger(__name__)

class MappingAgent:
    """
    Agente responsável por realizar o mapeamento ('de-para') dos dados extraídos
    com base em planilhas CSV e identificar inconsistências ou ambiguidades.
    """
    def __init__(self, artifacts_dir: str = config.ARTIFACTS_DIR):
        self.artifacts_dir = artifacts_dir
        self.df_precofixo = None
        self.df_material = None
        self.df_condicao = None
        self.df_forma = None
        self.df_planta = None # NOVO: DataFrame para plantas
        self._initialize_empty_sets() # Inicializa conjuntos vazios primeiro
        self.data_loaded_successfully = self._load_and_preprocess_data()
        if self.data_loaded_successfully:
            logger.info("Agente de Mapeamento inicializado e dados carregados.")
            self._preload_valid_codes_and_terms() # Renomeado para clareza
            self._preload_planta_codes() # NOVO: Carregar códigos de planta
        else:
            logger.error("Falha ao carregar dados para o Agente de Mapeamento.")
            # _initialize_empty_sets() já foi chamado

    def _initialize_empty_sets(self):
        """Inicializa conjuntos de códigos e termos como vazios."""
        self.valid_material_codes = set()
        self.valid_condicao_codes = set()
        self.valid_forma_codes = set()
        self.valid_planta_codes = set() # NOVO: Set para códigos de planta
        self.forma_terms_norm = set()
        self.condicao_terms_norm = set()
        self.forma_map_norm_to_code = {} # Mapeia termo normalizado -> código original
        self.condicao_map_norm_to_code = {}
        # NOVO: Para lidar com sinônimos que podem ser ambíguos
        self.ambiguous_forma_terms = set()
        self.forma_keyword_map_norm_to_codes = {} # Mapeia keyword -> lista de códigos MP

    def _preload_planta_codes(self): # NOVO MÉTODO
        """Pré-carrega códigos de planta válidos."""
        if self._is_valid_df(self.df_planta, 'Códigos'):
            # Converte para string, remove NaNs/None, normaliza para maiúsculas e remove espaços
            self.valid_planta_codes = set(
                code.strip().upper()
                for code in self.df_planta['Códigos'].dropna().astype(str)
                if code.strip()
            )
            logger.info(f"Códigos de planta válidos pré-carregados: {self.valid_planta_codes}")
        else:
            logger.warning("DataFrame de planta ou coluna 'Códigos' não encontrada. Mapeamento de planta por CSV desabilitado.")
            self.valid_planta_codes = set()

    def _preload_valid_codes_and_terms(self):
        """Pré-carrega conjuntos de códigos válidos e termos normalizados para otimização."""
        if self._is_valid_df(self.df_material, 'Cód'):
            self.valid_material_codes = set(self.df_material['Cód'].astype(str))

        if self._is_valid_df(self.df_condicao, 'Cond Pagamento'):
            self.valid_condicao_codes = set(self.df_condicao['Cond Pagamento'].astype(str))
            for _, row in self.df_condicao.iterrows():
                code_orig = str(row['Cond Pagamento'])
                code_norm = row.get('Cond_Pagamento_NORMALIZADO') # Termo do próprio código, normalizado
                sig_norm = row.get('Significado_NORMALIZADO')   # Significado da condição, normalizado

                # Adiciona o código original normalizado (se existir) ao set de termos e ao mapa
                if pd.notna(code_norm) and code_norm.strip(): # Verifica se não é NaN e não é vazio
                    self.condicao_terms_norm.add(code_norm)
                    # Prioriza mapear para o código original mais curto se já existir um mapeamento
                    if code_norm not in self.condicao_map_norm_to_code or \
                       len(code_orig) < len(self.condicao_map_norm_to_code[code_norm]):
                        self.condicao_map_norm_to_code[code_norm] = code_orig
                
                # Adiciona o significado normalizado (se existir) ao set de termos e ao mapa
                if pd.notna(sig_norm) and sig_norm.strip(): # Verifica se não é NaN e não é vazio
                    self.condicao_terms_norm.add(sig_norm)
                    if sig_norm not in self.condicao_map_norm_to_code or \
                       len(code_orig) < len(self.condicao_map_norm_to_code[sig_norm]):
                        self.condicao_map_norm_to_code[sig_norm] = code_orig

        if self._is_valid_df(self.df_forma, 'MP'):
            self.valid_forma_codes = set(self.df_forma['MP'].astype(str))
            for _, row in self.df_forma.iterrows():
                code_orig = str(row['MP'])

                # Termos diretos (MP e Significado)
                terms_direct = []
                mp_norm = row.get('MP_NORMALIZADO')
                sig_norm = row.get('Significado_NORMALIZADO')
                if pd.notna(mp_norm) and mp_norm.strip():
                    terms_direct.append(mp_norm)
                if pd.notna(sig_norm) and sig_norm.strip():
                    terms_direct.append(sig_norm)

                for term_norm in terms_direct:
                    self.forma_terms_norm.add(term_norm)
                    # Se já existe e aponta para código diferente, é ambíguo (improvável para MP/Significado)
                    if term_norm in self.forma_map_norm_to_code and self.forma_map_norm_to_code[term_norm] != code_orig:
                        logger.warning(f"Termo direto '{term_norm}' mapeando para múltiplos códigos: {self.forma_map_norm_to_code[term_norm]} e {code_orig}. Verifique dados.")
                        # Decide estratégia: sobrescrever, ignorar, ou marcar como ambíguo
                    self.forma_map_norm_to_code[term_norm] = code_orig

                # Palavras-chave/Sinônimos
                keywords_norm_list = row.get('PalavrasChave_NORMALIZADAS', [])
                for kw_norm in keywords_norm_list:
                    if kw_norm: # Ignora strings vazias que podem vir do split
                        self.forma_terms_norm.add(kw_norm) # Adiciona ao conjunto geral de termos

                        # Construir o mapa de keywords para LISTA de códigos
                        # Isso permite que uma keyword ("ted") possa mapear para múltiplos códigos se necessário
                        if kw_norm not in self.forma_keyword_map_norm_to_codes:
                            self.forma_keyword_map_norm_to_codes[kw_norm] = []

                        # Adiciona o código apenas se ainda não estiver na lista para essa keyword
                        if code_orig not in self.forma_keyword_map_norm_to_codes[kw_norm]:
                            self.forma_keyword_map_norm_to_codes[kw_norm].append(code_orig)

            # Identificar keywords ambíguas (mapeiam para mais de um código)
            for kw, codes in self.forma_keyword_map_norm_to_codes.items():
                if len(codes) > 1:
                    self.ambiguous_forma_terms.add(kw)
                    logger.info(f"Keyword de Forma de Pagamento '{kw}' é ambígua, mapeia para: {codes}")


        logger.debug("Conjuntos de códigos válidos e termos normalizados pré-carregados (Forma de Pagamento).")
        logger.debug(f"Forma - Mapa Termo Direto->Código: {self.forma_map_norm_to_code}")
        logger.debug(f"Forma - Mapa Keyword->Códigos: {self.forma_keyword_map_norm_to_codes}")
        logger.debug(f"Forma - Keywords Ambíguas: {self.ambiguous_forma_terms}")
        
    def _preload_valid_codes(self):
        """Pré-carrega conjuntos de códigos válidos para otimização."""
        self.valid_material_codes = set(self.df_material['Cód'].astype(str)) if self._is_valid_df(self.df_material, 'Cód') else set()
        self.valid_condicao_codes = set(self.df_condicao['Cond Pagamento'].astype(str)) if self._is_valid_df(self.df_condicao, 'Cond Pagamento') else set()
        self.valid_forma_codes = set(self.df_forma['MP'].astype(str)) if self._is_valid_df(self.df_forma, 'MP') else set()
        logger.debug("Conjuntos de códigos válidos pré-carregados.")

    def _is_valid_df(self, df, column_name):
         """Verifica se o DataFrame é válido e contém a coluna."""
         return df is not None and column_name in df.columns

    def _load_and_preprocess_data(self) -> bool:
        """Carrega e pré-processa os dados das planilhas CSV."""
        try:
            # Carrega os DataFrames (como antes)
            self.df_precofixo = self._read_csv("precofixo-de-para.csv", dtype={'CNPJ/CPF': str, 'Cliente': str, 'Nome Cliente': str})
            self.df_material = self._read_csv("material.csv", dtype={'Cód': str, 'Produto': str})
            self.df_condicao = self._read_csv("condicao-de-pagamento.csv", dtype={'Cond Pagamento': str, 'Significado': str})
            self.df_forma = self._read_csv("forma-de-pagamento.csv", dtype={'MP': str, 'Significado': str, 'PalavrasChave': str})
            self.df_planta = self._read_csv("planta.csv", dtype={'Plantas': str, 'Códigos': str}) # NOVO: Carregar planta.csv

    
            # --- Pré-processamento com remoção de hífens para campos relevantes ---
            if self._is_valid_df(self.df_forma, 'PalavrasChave'):
                # Converte NaN para string vazia, normaliza e remove hífens das keywords
                self.df_forma['PalavrasChave_NORMALIZADAS'] = self.df_forma['PalavrasChave'].fillna('').astype(str).apply(
                    lambda x: [normalize_string(kw, remove_hyphens=True) for kw in x.split('|') if kw.strip()] # <--- Flag True aqui
                )
            else:
                logger.warning("(MappingAgent): Coluna 'PalavrasChave' não encontrada no CSV de forma de pagamento.")
                # Garante que a coluna exista mesmo assim, vazia.
                self.df_forma['PalavrasChave_NORMALIZADAS'] = pd.Series([[] for _ in range(len(self.df_forma))])
    
            # CNPJ/CPF já tem hífens removidos por outra lógica, não precisa da flag aqui
            if self._is_valid_df(self.df_precofixo, 'CNPJ/CPF'):
                self.df_precofixo['CNPJ_CPF_NORMALIZADO'] = self.df_precofixo['CNPJ/CPF'].str.replace(r'[./-]', '', regex=True).fillna('')
            else:
                logger.warning("(MappingAgent): Coluna 'CNPJ/CPF' não encontrada para normalização.")
    
            # Campos de texto onde hífens devem ser removidos no pré-processamento do CSV
            self._preprocess_dataframe(self.df_precofixo, 'Nome Cliente', 'Nome_Cliente_NORMALIZADO',
                                    func=normalize_string, remove_hyphens=True) # <--- Flag True
            self._preprocess_dataframe(self.df_material, 'Produto', 'Produto_NORMALIZADO',
                                    func=normalize_string, remove_hyphens=True) # <--- Flag True
            self._preprocess_dataframe(self.df_condicao, 'Significado', 'Significado_NORMALIZADO',
                                    func=normalize_string, remove_hyphens=True) # <--- Flag True
            self._preprocess_dataframe(self.df_condicao, 'Cond Pagamento', 'Cond_Pagamento_NORMALIZADO',
                                    func=normalize_string, remove_hyphens=True) # <--- Flag True (Se Cond Pag pode ter hífen, ex: '30-dias')
            self._preprocess_dataframe(self.df_forma, 'Significado', 'Significado_NORMALIZADO',
                                    func=normalize_string, remove_hyphens=True) # <--- Flag True
            self._preprocess_dataframe(self.df_forma, 'MP', 'MP_NORMALIZADO',
                                    func=normalize_string, remove_hyphens=True) # <--- Flag True (Se MP pode ter hífen, ex: 'C-CRED')
    
            # Campos onde hífens NÃO devem ser removidos (se houvesse outros aqui)
            # Exemplo: self._preprocess_dataframe(self.df_algum, 'OutroCampo', 'OutroCampo_NORMALIZADO',
            #                            func=normalize_string, remove_hyphens=False) # Ou omitir remove_hyphens
    
            logger.debug("(MappingAgent): DataFrames de mapeamento carregados e normalizados (com remoção seletiva de hífens).")
            return True
    
        except FileNotFoundError as e:
            logger.error(f"(MappingAgent): Arquivo CSV não encontrado. Verifique o caminho: {e}")
            return False
        except AttributeError as ae:
            logger.error(f"(MappingAgent): Erro de atributo durante pré-processamento (provável problema com apply/lambda): {ae}", exc_info=True)
            return False
        except Exception as e:
            logger.error(f"(MappingAgent): Erro ao carregar ou processar arquivos CSV: {e}", exc_info=True)
            return False

        
    def _attempt_split_and_remap_payment(self, input_value: str, mapped_data: dict, issues: dict) -> bool:
        """
        Tenta dividir um valor de entrada (ex: 'ted-a-vista') em Forma e Condição de Pagamento.
        Atualiza mapped_data se uma divisão válida for encontrada. Retorna True se bem-sucedido.
        USA normalize_string com remove_hyphens=True.
        """
        if not input_value or not isinstance(input_value, str):
            return False

        # Normaliza o input removendo hífens para comparar com os termos pré-processados
        input_norm = normalize_string(input_value, remove_hyphens=True)
        if not input_norm: # Se a normalização resultar em string vazia
            return False

        logger.debug(f"Tentando dividir valor de pagamento combinado: '{input_value}' (Normalizado s/ hífen: '{input_norm}')")

        best_match_forma_code = None
        best_match_condicao_code = None
        max_combined_len = 0 # Prioriza o match que cobre a maior parte da string original

        # Ordena por comprimento decrescente para encontrar matches mais específicos/longos primeiro
        # Isso é crucial para que "ted" seja tentado antes de "d" (se "d" fosse um termo válido sozinho)
        sorted_forma_terms = sorted(list(self.forma_terms_norm), key=len, reverse=True)
        sorted_condicao_terms = sorted(list(self.condicao_terms_norm), key=len, reverse=True)

        # Cenário 1: Forma de Pagamento primeiro, depois Condição de Pagamento
        logger.debug("Split: Tentando Forma + Condição")
        for forma_term_norm in sorted_forma_terms:
            if forma_term_norm in input_norm:
                # Tenta encontrar o forma_term_norm no início, meio ou fim da string normalizada.
                # Regex para encontrar o termo e capturar o que vem antes e depois.
                # Usamos re.escape para o termo, pois ele pode conter caracteres especiais após a normalização.
                escaped_forma_term = re.escape(forma_term_norm)
                
                # Tentamos encontrar o forma_term como uma palavra inteira, se possível,
                # ou como substring. \b ajuda a casar palavras inteiras.
                # Se o termo for curto (ex: 'd'), \b pode ser restritivo demais.
                # Para termos mais longos, \b é bom.
                # Vamos tentar algumas variações de regex para encontrar o "resto".
                
                # Variação 1: O termo da forma está no início
                match = re.match(rf"({escaped_forma_term})(\b\s*|\s+|$)(.*)", input_norm, re.IGNORECASE)
                if match:
                    remainder_norm = normalize_string(match.group(3), remove_hyphens=True) # O que sobrou depois do termo da forma
                    logger.debug(f"  Forma Candidata (início): '{forma_term_norm}', Remainder: '{remainder_norm}'")
                    if remainder_norm: # Só busca condição se sobrou algo
                        for condicao_term_norm in sorted_condicao_terms:
                            if remainder_norm == condicao_term_norm: # Match exato do restante
                                current_len = len(forma_term_norm) + len(condicao_term_norm)
                                if current_len > max_combined_len:
                                    temp_forma_code = self.forma_map_norm_to_code.get(forma_term_norm)
                                    temp_condicao_code = self.condicao_map_norm_to_code.get(condicao_term_norm)
                                    if temp_forma_code and temp_condicao_code:
                                        max_combined_len = current_len
                                        best_match_forma_code = temp_forma_code
                                        best_match_condicao_code = temp_condicao_code
                                        logger.info(f"    SPLIT ACHADO (F+C, início): '{forma_term_norm}' ({best_match_forma_code}) + '{condicao_term_norm}' ({best_match_condicao_code})")
                                break # Achou a melhor condição para este forma_term

                # Variação 2: O termo da forma está no fim (menos comum para "Forma + Condição")
                match = re.match(rf"(.*)(\b\s*|\s+)({escaped_forma_term})$", input_norm, re.IGNORECASE)
                if match and not best_match_forma_code: # Se ainda não achou um match melhor
                    remainder_norm = normalize_string(match.group(1)) # O que veio antes
                    logger.debug(f"  Forma Candidata (fim): '{forma_term_norm}', Remainder (antes): '{remainder_norm}'")
                    # Isso é mais para o cenário Condição + Forma, mas verificamos
                    # Aqui, o remainder seria a condição
                    if remainder_norm:
                        for condicao_term_norm in sorted_condicao_terms:
                            if remainder_norm == condicao_term_norm:
                                current_len = len(forma_term_norm) + len(condicao_term_norm)
                                if current_len > max_combined_len:
                                    temp_forma_code = self.forma_map_norm_to_code.get(forma_term_norm)
                                    temp_condicao_code = self.condicao_map_norm_to_code.get(condicao_term_norm)
                                    if temp_forma_code and temp_condicao_code:
                                        max_combined_len = current_len
                                        best_match_forma_code = temp_forma_code
                                        best_match_condicao_code = temp_condicao_code
                                        logger.info(f"    SPLIT ACHADO (C+F, com forma no fim): '{condicao_term_norm}' ({best_match_condicao_code}) + '{forma_term_norm}' ({best_match_forma_code})")
                                break
                
                # Variação 3: O termo da forma está no meio (mais complexo, mas pode ser necessário)
                # Ex: "pagar com ted a vista" -> forma_term = "ted", antes="pagar com ", depois=" a vista"
                # Vamos simplificar por agora e focar em início/fim, que cobre "TED a vista" e "a vista TED".


        # Cenário 2: Condição de Pagamento primeiro, depois Forma de Pagamento
        # Só executa se o primeiro cenário não encontrou um match que cobre grande parte da string
        # ou se queremos ter certeza que a outra ordem não produz um match melhor (mais longo)
        logger.debug("Split: Tentando Condição + Forma")
        for condicao_term_norm in sorted_condicao_terms:
            if condicao_term_norm in input_norm:
                escaped_condicao_term = re.escape(condicao_term_norm)

                # Variação 1: Condição no início
                match = re.match(rf"({escaped_condicao_term})(\b\s*|\s+|$)(.*)", input_norm, re.IGNORECASE)
                if match:
                    remainder_norm = normalize_string(match.group(3))
                    logger.debug(f"  Condição Candidata (início): '{condicao_term_norm}', Remainder: '{remainder_norm}'")
                    if remainder_norm:
                        for forma_term_norm in sorted_forma_terms:
                            if remainder_norm == forma_term_norm:
                                current_len = len(condicao_term_norm) + len(forma_term_norm)
                                if current_len > max_combined_len: # Verifica se este é um match melhor
                                    temp_condicao_code = self.condicao_map_norm_to_code.get(condicao_term_norm)
                                    temp_forma_code = self.forma_map_norm_to_code.get(forma_term_norm)
                                    if temp_condicao_code and temp_forma_code:
                                        max_combined_len = current_len
                                        best_match_condicao_code = temp_condicao_code
                                        best_match_forma_code = temp_forma_code
                                        logger.info(f"    SPLIT ACHADO (C+F, início): '{condicao_term_norm}' ({best_match_condicao_code}) + '{forma_term_norm}' ({best_match_forma_code})")
                                break
                
                # Variação 2: Condição no fim (menos comum para "Condição + Forma")
                match = re.match(rf"(.*)(\b\s*|\s+)({escaped_condicao_term})$", input_norm, re.IGNORECASE)
                if match and (not best_match_condicao_code or (len(condicao_term_norm) + len(normalize_string(match.group(1)))) > max_combined_len ) :
                    remainder_norm = normalize_string(match.group(1)) # O que veio antes (seria a forma)
                    logger.debug(f"  Condição Candidata (fim): '{condicao_term_norm}', Remainder (antes): '{remainder_norm}'")
                    if remainder_norm:
                        for forma_term_norm in sorted_forma_terms:
                            if remainder_norm == forma_term_norm:
                                current_len = len(condicao_term_norm) + len(forma_term_norm)
                                if current_len > max_combined_len:
                                    temp_condicao_code = self.condicao_map_norm_to_code.get(condicao_term_norm)
                                    temp_forma_code = self.forma_map_norm_to_code.get(forma_term_norm)
                                    if temp_condicao_code and temp_forma_code:
                                        max_combined_len = current_len
                                        best_match_condicao_code = temp_condicao_code
                                        best_match_forma_code = temp_forma_code
                                        logger.info(f"    SPLIT ACHADO (F+C, com cond no fim): '{forma_term_norm}' ({best_match_forma_code}) + '{condicao_term_norm}' ({best_match_condicao_code})")
                                break


        if best_match_forma_code and best_match_condicao_code:
            logger.info(f"MELHOR SPLIT encontrado para '{input_value}': Forma='{best_match_forma_code}', Condição='{best_match_condicao_code}' (comprimento combinado dos termos: {max_combined_len})")

            # Atualiza os dados mapeados SE eles ainda não foram definidos por um mapeamento direto melhor
            # ou se o valor atual é o input original que falhou no mapeamento direto.
            # Consideramos o split bem-sucedido se ele encontrou AMBOS os códigos.
            # Ele sobrescreve o valor original se este ainda estiver lá.
            if mapped_data.get("Forma de Pagamento") == input_value or mapped_data.get("Forma de Pagamento") is None:
                 mapped_data["Forma de Pagamento"] = best_match_forma_code
            if mapped_data.get("Condição de Pagamento") == input_value or mapped_data.get("Condição de Pagamento") is None:
                 mapped_data["Condição de Pagamento"] = best_match_condicao_code

            # Remove potenciais avisos antigos sobre esse input específico, se houver
            # Esta parte é importante para limpar avisos de "não encontrado" após um split bem-sucedido.
            if "avisos" in issues:
                issues["avisos"] = [
                    aviso for aviso in issues["avisos"]
                    if not (
                        (aviso.get("campo") == "Forma de Pagamento" or aviso.get("campo") == "Condição de Pagamento") and
                        aviso.get("valor_original") == input_value
                    )
                ]
            return True
        else:
            logger.debug(f"Não foi possível dividir '{input_value}' em uma combinação válida e completa de Forma/Condição.")
            return False

    def _read_csv(self, filename: str, dtype: dict = None) -> pd.DataFrame | None:
        """Lê um arquivo CSV do diretório de artefatos."""
        path = os.path.join(self.artifacts_dir, filename)
        if not os.path.exists(path):
            logger.error(f"(MappingAgent): Arquivo não encontrado: {path}")
            raise FileNotFoundError(path)
        try:
            return pd.read_csv(path, dtype=dtype)
        except Exception as e:
            logger.error(f"(MappingAgent): Erro ao ler CSV {filename}: {e}")
            raise # Re-levanta a exceção para ser pega no _load_and_preprocess_data

    def _preprocess_dataframe(self, df: pd.DataFrame | None, col_orig: str, col_norm: str, func, **kwargs):
        """Aplica uma função de normalização a uma coluna se ela existir, passando kwargs."""
        if self._is_valid_df(df, col_orig):
            # Aplica a função passando os argumentos extras (como remove_hyphens=True)
            df[col_norm] = df[col_orig].apply(lambda x: func(x, **kwargs) if pd.notna(x) else None) # <--- Adicionado **kwargs
        else:
            logger.warning(f"(MappingAgent): Coluna '{col_orig}' não encontrada para normalização.")

    def _add_issue(self, issues: dict, type: str, issue_data):
        """Adiciona um problema (erro, aviso, ambiguidade) ao dicionário."""
        if type not in issues: issues[type] = []
        issues[type].append(issue_data)

    # --- Métodos de Mapeamento por Campo ---

    def _map_planta(self, mapped_data: dict, issues: dict, original_input_text: str): # NOVO MÉTODO
        """
        Mapeia o campo 'Planta'.
        Prioridade 1: Valor extraído pelo LLM (se for ou contiver um código válido).
        Prioridade 2: Busca por substring dos códigos válidos no texto original.
        """
        if not self.valid_planta_codes:
            logger.debug("(MappingAgent Planta): Nenhum código de planta válido carregado. Pulando mapeamento de planta.")
            return

        planta_extraida_llm_original = mapped_data.get("Planta")
        planta_final = None
        logger.debug(f"(MappingAgent Planta): Iniciando mapeamento. Extraído LLM: '{planta_extraida_llm_original}'. Texto Original: '{original_input_text[:100]}...'")

        # Etapa 1: Verificar o valor extraído pelo LLM
        if planta_extraida_llm_original:
            planta_extraida_llm_norm_upper = normalize_string(str(planta_extraida_llm_original), remove_hyphens=True)
            if planta_extraida_llm_norm_upper: # Verifica se a normalização não resultou em vazio
                planta_extraida_llm_norm_upper = planta_extraida_llm_norm_upper.upper()

                # Verifica se o valor normalizado é diretamente um código válido
                if planta_extraida_llm_norm_upper in self.valid_planta_codes:
                    planta_final = planta_extraida_llm_norm_upper
                    logger.info(f"(MappingAgent Planta): Planta extraída pelo LLM '{planta_extraida_llm_original}' é um código válido: '{planta_final}'.")
                else:
                    # Verifica se algum código válido é substring do valor extraído pelo LLM
                    # Ex: LLM extraiu "FS PDL" ou "CADÊNCIA LRV"
                    found_codes_in_llm_extraction = []
                    for code in self.valid_planta_codes:
                        # Usar word boundaries para evitar matches parciais indesejados (ex: "D" em "CADENCIA")
                        # No entanto, para "PDL" em "FS PDL", \bPDL\b não funcionaria se não houver espaço antes.
                        # Uma busca por substring simples é mais flexível aqui, dado o exemplo.
                        if code in planta_extraida_llm_norm_upper:
                            found_codes_in_llm_extraction.append(code)
                    
                    if len(found_codes_in_llm_extraction) == 1:
                        planta_final = found_codes_in_llm_extraction[0]
                        logger.info(f"(MappingAgent Planta): Código de planta '{planta_final}' encontrado dentro da extração do LLM ('{planta_extraida_llm_original}').")
                    elif len(found_codes_in_llm_extraction) > 1:
                        # Raro, mas se LLM extrair algo como "PDL ou LRV", trata como ambiguidade
                        # Ou se a normalização de códigos levar a colisões (ex: se um código fosse "P" e outro "PDL")
                        opcoes_ambiguidade = [{"codigo": c, "descricao": f"Código {c} (encontrado em '{planta_extraida_llm_original}')"} for c in found_codes_in_llm_extraction]
                        self._add_issue(issues, "ambiguidades", {
                            "campo": "Planta", "valor_original": planta_extraida_llm_original,
                            "mensagem": f"Múltiplos códigos de planta válidos ({', '.join(found_codes_in_llm_extraction)}) foram encontrados na informação de planta fornecida ('{planta_extraida_llm_original}'). Qual é o correto?",
                            "opcoes": opcoes_ambiguidade
                        })
                        logger.warning(f"(MappingAgent Planta): Ambiguidade na extração LLM para Planta: '{planta_extraida_llm_original}', códigos encontrados: {found_codes_in_llm_extraction}.")
                        # Não define planta_final, deixa a ambiguidade ser resolvida
                        mapped_data["Planta"] = planta_extraida_llm_original # Mantém o valor original para a pergunta de ambiguidade
                        return # Retorna para que a ambiguidade seja tratada

        # Etapa 2: Se não encontrou planta via LLM, buscar no texto original completo
        if not planta_final and original_input_text:
            logger.debug(f"(MappingAgent Planta): Planta não resolvida pela extração LLM. Buscando no texto original.")
            # Normaliza o texto original para busca case-insensitive
            # Não remover hífens aqui, pois os códigos podem não ter hífen e a busca é por substring exata do código.
            # Garantir que os códigos em self.valid_planta_codes já estão em maiúsculas.
            texto_original_upper = original_input_text.upper() 
            
            codigos_encontrados_no_texto = []
            for code_planta in self.valid_planta_codes: # Estes já estão em UPPER
                # Busca pelo código exato (case-insensitive devido ao texto_original_upper)
                # Usar word boundaries \b para garantir que estamos pegando o código isolado
                # e não como parte de outra palavra (ex: "SRS" em "SENSORES").
                # Pattern: \bCODE\b
                # No entanto, se o código estiver junto de outros caracteres sem espaço (ex: "CADENCIALRV"), \b não funciona bem.
                # Uma simples verificação de `code_planta in texto_original_upper` pode ser mais robusta para casos como "CADENCIALRV"
                # Se os códigos são sempre como 'LRV', 'PDL', 'SRS', a chance de falso positivo é menor.
                if code_planta in texto_original_upper:
                    codigos_encontrados_no_texto.append(code_planta)
            
            # Remove duplicatas se um código aparecer várias vezes
            codigos_encontrados_unicos = sorted(list(set(codigos_encontrados_no_texto)))

            if len(codigos_encontrados_unicos) == 1:
                planta_final = codigos_encontrados_unicos[0]
                logger.info(f"(MappingAgent Planta): Código de planta '{planta_final}' encontrado por substring no texto original.")
            elif len(codigos_encontrados_unicos) > 1:
                # Ambiguidade: múltiplos códigos de planta encontrados no texto
                opcoes_ambiguidade = [{"codigo": c, "descricao": f"Código {c} (encontrado no texto)"} for c in codigos_encontrados_unicos]
                self._add_issue(issues, "ambiguidades", {
                    "campo": "Planta", "valor_original": original_input_text[:150] + "...", # Mostra parte do texto original
                    "mensagem": f"Múltiplos códigos de planta ({', '.join(codigos_encontrados_unicos)}) foram encontrados no seu pedido. Qual planta você se refere?",
                    "opcoes": opcoes_ambiguidade
                })
                logger.warning(f"(MappingAgent Planta): Ambiguidade na busca por substring no texto original. Códigos encontrados: {codigos_encontrados_unicos}.")
                # Mantém o campo Planta como None ou o que veio do LLM (que não foi resolvido)
                mapped_data["Planta"] = planta_extraida_llm_original if planta_extraida_llm_original else None
                return

        # Atualiza o campo "Planta" em mapped_data se uma planta final foi determinada
        if planta_final:
            mapped_data["Planta"] = planta_final
            logger.debug(f"(MappingAgent Planta): Campo 'Planta' atualizado para '{planta_final}'.")
        elif planta_extraida_llm_original and not any(issue.get("campo") == "Planta" for issue in issues.get("ambiguidades",[])):
            # Se o LLM extraiu algo, mas não foi um código válido nem continha um,
            # E não foi encontrada por substring, E não gerou ambiguidade acima,
            # Adiciona um aviso que o valor extraído não é uma planta conhecida.
            self._add_issue(issues, "avisos", {
                "campo": "Planta", "valor_original": planta_extraida_llm_original,
                "mensagem": f"A planta informada '{planta_extraida_llm_original}' não é um código de planta reconhecido ({', '.join(self.valid_planta_codes)})."
            })
            logger.debug(f"(MappingAgent Planta): Planta extraída pelo LLM '{planta_extraida_llm_original}' não reconhecida e nenhum código encontrado no texto. Mantendo valor original para aviso.")
            # Mantém o valor original em mapped_data["Planta"] para que o Orchestrator o veja como inválido
        else:
            # Se nada foi extraído pelo LLM e nada encontrado no texto, o campo continua None (ou como estava)
            logger.debug(f"(MappingAgent Planta): Nenhuma planta extraída ou encontrada. Campo 'Planta' permanece: '{mapped_data.get('Planta')}'.")

    def _map_cliente(self, mapped_data: dict, issues: dict):
        cnpj_cpf_extraido = mapped_data.get("CNPJ/CPF")
        nome_cliente_extraido = mapped_data.get("Cliente")
        codigo_cliente_extraido = mapped_data.get("Código do cliente") # Este é o que o LLM extraiu como "Código do cliente"

        cliente_encontrado = False
        cliente_ambiguo = False # Flag para indicar se uma ambiguidade foi detectada

        FUZZY_MATCH_THRESHOLD = 70
        FUZZY_SCORER = fuzz.WRatio
        NUM_WORDS_FOR_TIE_BREAK = 2
        TIE_BREAK_SCORE_THRESHOLD = 85

        # --- 0. VERIFICAÇÃO PRÉVIA: Se já temos um Código do Cliente e um CNPJ/CPF consistentes,
        # e o Nome do Cliente também está presente, podemos pular a busca por CNPJ/CPF
        # para evitar re-gerar a ambiguidade se o CNPJ for duplicado mas já escolhemos um cliente.
        if codigo_cliente_extraido and cnpj_cpf_extraido:
            logger.debug(f"(MappingAgent Cliente Etapa 0): Verificando consistência para Cód.Cliente='{codigo_cliente_extraido}' e CNPJ/CPF='{cnpj_cpf_extraido}'")
            # Procura o cliente na planilha PELO CÓDIGO fornecido
            cliente_info_por_codigo = self.df_precofixo[self.df_precofixo['Cliente'].astype(str) == str(codigo_cliente_extraido)]

            if not cliente_info_por_codigo.empty:
                if len(cliente_info_por_codigo) > 1:
                    # Raro, mas se o código do cliente não for único na planilha, isso é um problema de dados.
                    logger.warning(f"(MappingAgent Cliente Etapa 0): Código do cliente '{codigo_cliente_extraido}' é duplicado na planilha de precofixo. Não é possível validar consistência de forma segura.")
                else:
                    cliente_row_planilha = cliente_info_por_codigo.iloc[0]
                    cnpj_da_planilha_para_codigo = cliente_row_planilha.get("CNPJ_CPF_NORMALIZADO")
                    nome_da_planilha = cliente_row_planilha.get("Nome Cliente") # Nome oficial da planilha
                    
                    cnpj_extraido_normalizado = re.sub(r'[./-]', '', str(cnpj_cpf_extraido))

                    # Verifica se o CNPJ fornecido bate com o CNPJ da planilha PARA O CÓDIGO fornecido
                    if cnpj_extraido_normalizado == cnpj_da_planilha_para_codigo:
                        logger.info(f"(MappingAgent Cliente Etapa 0): Código '{codigo_cliente_extraido}' e CNPJ '{cnpj_extraido_normalizado}' são consistentes com a planilha. Usando dados da planilha.")
                        
                        # Sobrescreve os campos com os dados da planilha, que são a fonte da verdade.
                        mapped_data["Código do cliente"] = str(cliente_row_planilha.get("Cliente")) # Garante que é o da planilha
                        mapped_data["CNPJ/CPF"] = cliente_row_planilha.get("CNPJ_CPF_NORMALIZADO", cnpj_extraido_normalizado) # Usa o da planilha
                        mapped_data["Nome do cliente"] = nome_da_planilha
                        mapped_data["Cliente"] = nome_da_planilha # Atualiza também o campo "Cliente" original
                        
                        cliente_encontrado = True
                        
                        # Log para mostrar qual nome foi usado (planilha vs LLM)
                        if nome_cliente_extraido and nome_cliente_extraido != nome_da_planilha:
                            logger.info(f"(MappingAgent Cliente Etapa 0): Nome do cliente atualizado para '{nome_da_planilha}' (da planilha) em vez de '{nome_cliente_extraido}' (extraído LLM).")
                    else:
                        logger.warning(f"(MappingAgent Cliente Etapa 0): Código do cliente '{codigo_cliente_extraido}' encontrado, mas CNPJ fornecido ('{cnpj_extraido_normalizado}') diverge do CNPJ da planilha ('{cnpj_da_planilha_para_codigo}') para este código. Prosseguindo com outras lógicas de busca.")
                        # cliente_encontrado permanece False, para permitir que outras lógicas (busca por CNPJ isolado, etc.) tentem.
            else:
                logger.warning(f"(MappingAgent Cliente Etapa 0): Código do cliente '{codigo_cliente_extraido}' fornecido, mas não encontrado na planilha. Prosseguindo com outras lógicas de busca.")
        
        # --- 1. Tentativa por CNPJ/CPF (se não encontrado/validado na Etapa 0) ---
        if not cliente_encontrado and cnpj_cpf_extraido and self._is_valid_df(self.df_precofixo, 'CNPJ_CPF_NORMALIZADO'):
            cnpj_cpf_extraido_norm = re.sub(r'[./-]', '', str(cnpj_cpf_extraido))
            logger.debug(f"(MappingAgent Cliente Etapa 1): Buscando por CNPJ_CPF_NORMALIZADO = '{cnpj_cpf_extraido_norm}'")
            cliente_info_cnpj = self.df_precofixo[self.df_precofixo['CNPJ_CPF_NORMALIZADO'] == cnpj_cpf_extraido_norm]
            logger.debug(f"(MappingAgent Cliente Etapa 1): Resultado da busca por CNPJ (len={len(cliente_info_cnpj)}): {cliente_info_cnpj.to_dict('records') if not cliente_info_cnpj.empty else 'DataFrame vazio'}")

            if len(cliente_info_cnpj) == 1:
                cliente_row_data = cliente_info_cnpj.iloc[0]
                codigo_cliente_planilha = str(cliente_row_data.get("Cliente")) if pd.notna(cliente_row_data.get("Cliente")) else None
                cnpj_planilha_match = cliente_row_data.get("CNPJ_CPF_NORMALIZADO")
                nome_cliente_planilha = cliente_row_data.get("Nome Cliente")
                
                mapped_data["Código do cliente"] = codigo_cliente_planilha
                mapped_data["CNPJ/CPF"] = cnpj_planilha_match # Usa o da planilha para garantir formato
                mapped_data["Nome do cliente"] = nome_cliente_planilha
                mapped_data["Cliente"] = nome_cliente_planilha # Atualiza também o campo "Cliente" original
                cliente_encontrado = True
                logger.info(f"(MappingAgent Cliente Etapa 1): CNPJ/CPF '{cnpj_cpf_extraido_norm}' mapeado para Código: {codigo_cliente_planilha}, Nome: {nome_cliente_planilha}.")

            elif len(cliente_info_cnpj) > 1: # CNPJ/CPF DUPLICADO
                logger.info(f"(MappingAgent Cliente Etapa 1): CNPJ/CPF '{cnpj_cpf_extraido_norm}' é DUPLICADO ({len(cliente_info_cnpj)} matches).")
                
                # Verifica se um CÓDIGO DE CLIENTE foi fornecido (pelo usuário ou LLM) para tentar desambiguar
                if codigo_cliente_extraido and str(codigo_cliente_extraido).strip(): # Garante que não é None ou vazio
                    logger.debug(f"(MappingAgent Cliente Etapa 1): Tentando desambiguar CNPJ duplicado com Código do Cliente fornecido: '{codigo_cliente_extraido}'")
                    cliente_row_data_desambiguado = cliente_info_cnpj[cliente_info_cnpj['Cliente'].astype(str) == str(codigo_cliente_extraido)]
                    
                    if len(cliente_row_data_desambiguado) == 1:
                        logger.info(f"(MappingAgent Cliente Etapa 1): CNPJ/CPF duplicado DESAMBIGUADO com sucesso pelo Código do cliente '{codigo_cliente_extraido}'.")
                        cliente_final_row = cliente_row_data_desambiguado.iloc[0]
                        mapped_data["Código do cliente"] = str(cliente_final_row.get("Cliente"))
                        mapped_data["CNPJ/CPF"] = cliente_final_row.get("CNPJ_CPF_NORMALIZADO", cnpj_cpf_extraido_norm)
                        mapped_data["Nome do cliente"] = cliente_final_row.get("Nome Cliente")
                        mapped_data["Cliente"] = cliente_final_row.get("Nome Cliente")
                        cliente_encontrado = True
                    elif len(cliente_row_data_desambiguado) == 0:
                        logger.warning(f"(MappingAgent Cliente Etapa 1): CNPJ/CPF '{cnpj_cpf_extraido_norm}' duplicado, e Código do Cliente '{codigo_cliente_extraido}' fornecido NÃO corresponde a nenhuma das opções. Gerando ambiguidade.")
                        cliente_ambiguo = True # Força a ambiguidade para ser tratada abaixo
                    else: # Mais de um match mesmo com código + CNPJ (problema de dados na planilha)
                         logger.error(f"(MappingAgent Cliente Etapa 1): CNPJ/CPF '{cnpj_cpf_extraido_norm}' e Código '{codigo_cliente_extraido}' resultaram em múltiplos matches na planilha. Problema de dados. Gerando ambiguidade.")
                         cliente_ambiguo = True
                else: # CNPJ duplicado e NENHUM código de cliente foi fornecido (ou era None/vazio)
                    logger.info(f"(MappingAgent Cliente Etapa 1): CNPJ/CPF '{cnpj_cpf_extraido_norm}' duplicado e NENHUM Código do Cliente foi fornecido/extraído para desambiguar. Gerando ambiguidade.")
                    cliente_ambiguo = True
                
                # Se, após as tentativas de desambiguação, ainda for considerado ambíguo:
                if cliente_ambiguo:
                    nomes_planilha = cliente_info_cnpj["Nome Cliente"].tolist()
                    codigos_cliente_planilha_opts = cliente_info_cnpj["Cliente"].astype(str).tolist() # Renomeado para evitar conflito
                    
                    opcoes_ambiguidade = []
                    for nome_p, codigo_p in zip(nomes_planilha, codigos_cliente_planilha_opts):
                        opcoes_ambiguidade.append({
                            "nome": nome_p, 
                            "codigo": codigo_p, 
                            "cnpj_cpf": cnpj_cpf_extraido_norm 
                        })

                    mensagem_ambiguidade = (
                        f"O CNPJ/CPF '{cnpj_cpf_extraido_norm}' está associado a múltiplos clientes. "
                        "Por favor, escolha o correto (informe o número da opção ou o Código do Cliente):\n"
                    )
                    for i, opt in enumerate(opcoes_ambiguidade):
                        mensagem_ambiguidade += f"{i+1}. {opt['nome']} (Código do Cliente: {opt['codigo']})\n"
                    
                    self._add_issue(issues, "ambiguidades", {
                        "campo": "Código do cliente", 
                        "original_field_name": "Cliente", 
                        "valor_original": cnpj_cpf_extraido_norm, 
                        "mensagem": mensagem_ambiguidade.strip(),
                        "opcoes": opcoes_ambiguidade 
                    })
                    logger.info(f"(MappingAgent Cliente Etapa 1): AMBIGUIDADE REALMENTE GERADA por CNPJ/CPF DUPLICADO '{cnpj_cpf_extraido_norm}'. Retornando para Orchestrator.")
                    # IMPORTANTE: Retorna aqui para que o Orchestrator possa lidar com a ambiguidade
                    return 

        # --- 2. Tentativa por Nome Normalizado (str.contains) ---
        # Só executa se não foi encontrado por CNPJ/CPF e não há ambiguidade pendente de CNPJ/CPF
        if not cliente_encontrado and not cliente_ambiguo and nome_cliente_extraido and self._is_valid_df(self.df_precofixo, 'Nome_Cliente_NORMALIZADO'):
            nome_norm_input_contains = normalize_string(nome_cliente_extraido, remove_hyphens=True)
            if nome_norm_input_contains:
                logger.debug(f"(MappingAgent Cliente): Tentando NOME (contains) para: '{nome_norm_input_contains}'")
                try:
                    escaped_name = re.escape(nome_norm_input_contains)
                    cliente_info_nome_contains = self.df_precofixo[
                        self.df_precofixo['Nome_Cliente_NORMALIZADO'].str.contains(escaped_name, na=False, regex=True)
                    ]
                except Exception as e:
                    logger.error(f"Erro NOME (contains): {e}")
                    cliente_info_nome_contains = pd.DataFrame()

                if not cliente_info_nome_contains.empty:
                    if len(cliente_info_nome_contains) == 1:
                        cliente_row_data = cliente_info_nome_contains.iloc[0]
                        codigo_cliente_planilha = str(cliente_row_data.get("Cliente")) if pd.notna(cliente_row_data.get("Cliente")) else None
                        cnpj_planilha_match = cliente_row_data.get("CNPJ_CPF_NORMALIZADO")
                        nome_cliente_planilha = cliente_row_data.get("Nome Cliente")
                        
                        mapped_data["Código do cliente"] = codigo_cliente_planilha
                        if not cnpj_cpf_extraido and cnpj_planilha_match: # Só preenche CNPJ se não veio do LLM
                            mapped_data["CNPJ/CPF"] = cnpj_planilha_match
                        mapped_data["Nome do cliente"] = nome_cliente_planilha
                        mapped_data["Cliente"] = nome_cliente_planilha
                        cliente_encontrado = True
                        logger.info(f"(MappingAgent Cliente): NOME (contains - 1 match): '{nome_cliente_extraido}' -> '{nome_cliente_planilha}'.")
                    else: # Ambiguidade por nome (contains)
                        cliente_ambiguo = True
                        nomes_planilha = cliente_info_nome_contains["Nome Cliente"].tolist()
                        codigos_planilha = cliente_info_nome_contains["Cliente"].astype(str).tolist()
                        cnpjs_planilha = cliente_info_nome_contains["CNPJ_CPF_NORMALIZADO"].tolist()
                        
                        opcoes_ambiguidade = []
                        for n, c, j in zip(nomes_planilha, codigos_planilha, cnpjs_planilha):
                             opcoes_ambiguidade.append({
                                 "nome": n, 
                                 "codigo": c, 
                                 "cnpj_cpf": j if pd.notna(j) else None
                            })
                        
                        mensagem_ambiguidade_partes = [
                            f"Múltiplos clientes contêm '{nome_cliente_extraido}'. Por favor, escolha o correto (informe o número da opção ou o Código do Cliente):"
                        ]
                        for i, opt in enumerate(opcoes_ambiguidade):
                            mensagem_ambiguidade_partes.append(
                                f"{i+1}. {opt['nome']} (Cód: {opt['codigo']}, CNPJ/CPF: {opt.get('cnpj_cpf', 'N/A')})"
                            )
                        
                        self._add_issue(issues, "ambiguidades", {
                            "campo": "Código do cliente", # O campo que será efetivamente preenchido pela escolha
                            "original_field_name": "Cliente", # Usado pelo Orchestrator
                            "valor_original": nome_cliente_extraido,
                            "mensagem": "\n".join(mensagem_ambiguidade_partes),
                            "opcoes": opcoes_ambiguidade 
                        })
                        logger.info(f"(MappingAgent Cliente): AMBIGUIDADE NOME (contains) para '{nome_norm_input_contains}': {len(cliente_info_nome_contains)} matches.")

        # --- 3. Fallback para Fuzzy Matching ---
        if not cliente_encontrado and not cliente_ambiguo and nome_cliente_extraido and self._is_valid_df(self.df_precofixo, 'Nome_Cliente_NORMALIZADO'):
            nome_norm_input_fuzzy = normalize_string(nome_cliente_extraido, remove_hyphens=True)
            
            if nome_norm_input_fuzzy:
                logger.debug(f"(MappingAgent Cliente FUZZY): Tentando FUZZY MATCH para: '{nome_norm_input_fuzzy}' (Thresh: {FUZZY_MATCH_THRESHOLD}%, Scorer: {FUZZY_SCORER.__name__}, TieBreakThresh: {TIE_BREAK_SCORE_THRESHOLD}%)")
                
                potential_fuzzy_matches_raw = []
                valid_clientes_df = self.df_precofixo[self.df_precofixo['Nome_Cliente_NORMALIZADO'].notna() & (self.df_precofixo['Nome_Cliente_NORMALIZADO'] != '')].copy()
                input_first_n_words_str = " ".join(nome_norm_input_fuzzy.split()[:NUM_WORDS_FOR_TIE_BREAK])

                for _, row in valid_clientes_df.iterrows():
                    nome_planilha_norm = row['Nome_Cliente_NORMALIZADO']
                    if not nome_planilha_norm: continue

                    score = FUZZY_SCORER(nome_norm_input_fuzzy, nome_planilha_norm)
                    
                    if score >= FUZZY_MATCH_THRESHOLD:
                        planilha_first_n_words_str = " ".join(nome_planilha_norm.split()[:NUM_WORDS_FOR_TIE_BREAK])
                        tie_break_score = fuzz.ratio(input_first_n_words_str, planilha_first_n_words_str)

                        potential_fuzzy_matches_raw.append({
                            "score": score,
                            "tie_break_score": tie_break_score,
                            "row_data": row,
                            "nome_original_planilha": row.get("Nome Cliente"),
                            "codigo_cliente_planilha": str(row.get("Cliente")) if pd.notna(row.get("Cliente")) else None,
                            "cnpj_planilha": row.get("CNPJ_CPF_NORMALIZADO")
                        })
                
                potential_fuzzy_matches_raw.sort(key=lambda x: (x["score"], x["tie_break_score"]), reverse=True)
                
                final_qualified_matches = []
                if potential_fuzzy_matches_raw:
                    for match_candidate in potential_fuzzy_matches_raw:
                        if match_candidate["tie_break_score"] >= TIE_BREAK_SCORE_THRESHOLD:
                            final_qualified_matches.append(match_candidate)
                
                logger.debug(f"(MappingAgent Cliente FUZZY): Raw matches (score >= {FUZZY_MATCH_THRESHOLD}%): {len(potential_fuzzy_matches_raw)}. Final qualified (tie_break_score >= {TIE_BREAK_SCORE_THRESHOLD}%): {len(final_qualified_matches)}")

                if final_qualified_matches:
                    if len(final_qualified_matches) == 1:
                        best_match = final_qualified_matches[0]
                        cliente_row_data = best_match["row_data"]
                        codigo_cliente_planilha = best_match["codigo_cliente_planilha"]
                        cnpj_planilha_match = best_match["cnpj_planilha"]
                        nome_cliente_planilha = best_match["nome_original_planilha"]
                        
                        mapped_data["Código do cliente"] = codigo_cliente_planilha
                        if not cnpj_cpf_extraido and cnpj_planilha_match:
                            mapped_data["CNPJ/CPF"] = cnpj_planilha_match
                        mapped_data["Nome do cliente"] = nome_cliente_planilha
                        mapped_data["Cliente"] = nome_cliente_planilha
                        cliente_encontrado = True
                        logger.info(f"(MappingAgent Cliente FUZZY): FUZZY MATCH ÚNICO QUALIFICADO (Score: {best_match['score']}%, TieBreak: {best_match['tie_break_score']}%): '{nome_cliente_extraido}' -> '{nome_cliente_planilha}'.")
                    
                    else: # Múltiplos fuzzy matches QUALIFICADOS -> Ambiguidade
                        cliente_ambiguo = True
                        opcoes_ambiguidade = []
                        MAX_AMBIGUITY_OPTIONS = 5 # Definir no topo da classe se usado em mais lugares
                        for match in final_qualified_matches[:MAX_AMBIGUITY_OPTIONS]:
                            opcoes_ambiguidade.append({
                                "nome": match["nome_original_planilha"],
                                "codigo": match["codigo_cliente_planilha"],
                                "cnpj_cpf": match["cnpj_planilha"] if pd.notna(match["cnpj_planilha"]) else None,
                                "similaridade": match["score"],
                                "similaridade_inicio": match["tie_break_score"]
                            })
                        
                        ambiguity_message_parts = [
                            f"Encontrei múltiplos clientes com nomes similares a '{nome_cliente_extraido}' (correspondência aproximada e início similar). Por favor, escolha o correto (informe o número da opção ou o Código do Cliente):"
                        ]
                        for i, opt in enumerate(opcoes_ambiguidade):
                            ambiguity_message_parts.append(
                                f"{i+1}. {opt['nome']} (Cód: {opt['codigo']}, CNPJ: {opt.get('cnpj_cpf', 'N/A')}, Sim: {opt['similaridade']}% / Início: {opt['similaridade_inicio']}%)"
                            )
                        if len(final_qualified_matches) > MAX_AMBIGUITY_OPTIONS:
                            ambiguity_message_parts.append(f"... e mais {len(final_qualified_matches) - MAX_AMBIGUITY_OPTIONS} outros.")
                        
                        self._add_issue(issues, "ambiguidades", {
                            "campo": "Código do cliente", # O campo que será efetivamente preenchido
                            "original_field_name": "Cliente", # Usado pelo Orchestrator
                            "valor_original": nome_cliente_extraido,
                            "mensagem": "\n".join(ambiguity_message_parts),
                            "opcoes": opcoes_ambiguidade
                        })
                        logger.info(f"(MappingAgent Cliente FUZZY): AMBIGUIDADE FUZZY MATCH QUALIFICADO para '{nome_norm_input_fuzzy}'.")
                else: # Nenhum fuzzy match qualificado
                    logger.debug(f"(MappingAgent Cliente FUZZY): Nenhum fuzzy match QUALIFICADO encontrado para '{nome_norm_input_fuzzy}'.")
        
        # --- Fallback Final / Default ---
        if not cliente_encontrado:

            if cliente_ambiguo:
                mapped_data["Nome do cliente"] = nome_cliente_extraido # Mantém o que foi extraído pelo LLM
                mapped_data["Cliente"] = nome_cliente_extraido      # Mantém o que foi extraído pelo LLM
                mapped_data["CNPJ/CPF"] = cnpj_cpf_extraido         # Mantém o que foi extraído pelo LLM
                mapped_data["Código do cliente"] = None # Força None pois é ambíguo
                logger.debug(f"(MappingAgent Cliente): AMBIGUIDADE DETECTADA. Código do cliente indefinido. Nome='{nome_cliente_extraido}', CNPJ='{cnpj_cpf_extraido}'.")
            else:
                # Nenhuma ambiguidade, nenhum cliente encontrado. Adiciona aviso.
                # Mantém os valores extraídos pelo LLM em mapped_data.
                mapped_data["Nome do cliente"] = nome_cliente_extraido
                mapped_data["Cliente"] = nome_cliente_extraido
                mapped_data["CNPJ/CPF"] = cnpj_cpf_extraido
                mapped_data["Código do cliente"] = codigo_cliente_extraido # Mantém o que o LLM extraiu

                if (nome_cliente_extraido or cnpj_cpf_extraido or codigo_cliente_extraido):
                    identificadores_fornecidos = []
                    if nome_cliente_extraido: identificadores_fornecidos.append(f"Nome '{nome_cliente_extraido}'")
                    if cnpj_cpf_extraido: identificadores_fornecidos.append(f"CNPJ/CPF '{cnpj_cpf_extraido}'")
                    if codigo_cliente_extraido: identificadores_fornecidos.append(f"Código '{codigo_cliente_extraido}'")
                    
                    identificador_str = " e ".join(identificadores_fornecidos) if identificadores_fornecidos else "informado"
                    msg = f"Cliente com dados ({identificador_str}) não encontrado (pode ser novo ou dados insuficientes/incorretos)."
                    
                    # Determina o campo principal para o aviso
                    campo_aviso = "Cliente" if nome_cliente_extraido else \
                                  "CNPJ/CPF" if cnpj_cpf_extraido else \
                                  "Código do cliente"
                    valor_original_aviso = nome_cliente_extraido if campo_aviso=="Cliente" else \
                                           cnpj_cpf_extraido if campo_aviso=="CNPJ/CPF" else \
                                           codigo_cliente_extraido
                                           
                    self._add_issue(issues, "avisos", {"campo": campo_aviso, "valor_original": valor_original_aviso, "mensagem": msg})
                    logger.debug(f"(MappingAgent Cliente): {msg}")

        # --- Consistência Final ---
        # Se Nome do cliente foi definido (por mapeamento ou mantido da extração) e Cliente não, copia.
        if mapped_data.get("Nome do cliente") is not None and mapped_data.get("Cliente") is None:
             mapped_data["Cliente"] = mapped_data.get("Nome do cliente")
        # Se Cliente foi definido e Nome do cliente não, copia.
        elif mapped_data.get("Cliente") is not None and mapped_data.get("Nome do cliente") is None:
             mapped_data["Nome do cliente"] = mapped_data.get("Cliente")

        if cliente_encontrado and (not mapped_data.get("Código do cliente") or not mapped_data.get("Nome do cliente")):
            logger.warning(f"(MappingAgent Cliente): Inconsistência! Cliente marcado como encontrado, mas Cód Planilha ou Nome Planilha ausentes em mapped_data. Mapped CNPJ: {mapped_data.get('CNPJ/CPF')}, Mapped Código: {mapped_data.get('Código do cliente')}, Mapped Nome: {mapped_data.get('Nome do cliente')}")


    def _map_material(self, mapped_data: dict, issues: dict):
        """Mapeia o código do material."""
        material_input = mapped_data.get("Código do Material")
        if not material_input: return # Nada a mapear

        input_norm_cod = normalize_string(str(material_input), remove_hyphens=True)
        if input_norm_cod in self.valid_material_codes: # valid_material_codes são strings
            logger.debug(f"(MappingAgent Material): Input '{material_input}' normalizado p/ código válido '{input_norm_cod}'.")
            # Atualiza o mapped_data com o código encontrado no set (que não tem hífens se foram removidos no pré-proc)
            # Pode ser redundante se input_norm_cod == str(material_input), mas garante consistência.
            mapped_data["Código do Material"] = input_norm_cod # Usa o código normalizado/validado
            return # Mapeamento não necessário (ou já feito)

        # Não é código, tentar mapear por nome (REMOVENDO HÍFENS)
        logger.debug(f"(MappingAgent Material): Input '{material_input}' não é código válido. Tentando mapear por nome (s/ hífen).")
        if not self._is_valid_df(self.df_material, 'Produto_NORMALIZADO'): return

        # Normaliza o NOME do material (input) removendo hífens
        material_nome_norm = normalize_string(material_input, remove_hyphens=True)
        if not material_nome_norm:
             self._add_issue(issues, "avisos", {
                 "campo": "Código do Material", "valor_original": material_input,
                 "mensagem": f"Nome do material '{material_input}' normalizou para vazio."
             })
             return

        # Tenta match exato normalizado (sem hífens) com coluna pré-processada (sem hífens)
        material_info = self.df_material[self.df_material['Produto_NORMALIZADO'] == material_nome_norm]

        # Se exato falhou, tenta 'contains' (sem hífens) na coluna pré-processada (sem hífens)
        if material_info.empty:
            try:
                escaped_material = re.escape(material_nome_norm)
                material_info = self.df_material[self.df_material['Produto_NORMALIZADO'].str.contains(escaped_material, na=False, regex=True)]
                if not material_info.empty:
                    logger.debug(f"(MappingAgent Material): Encontrado '{material_input}' via 'contains' (s/ hífen) no nome.")
            except Exception as e:
                logger.error(f"Erro na busca por material (contains): {e}")
                material_info = pd.DataFrame()
        # Processa resultados
        if not material_info.empty:
            if len(material_info) == 1:
                mapped_data["Código do Material"] = str(material_info.iloc[0]['Cód'])
                logger.debug(f"(MappingAgent Material): Nome '{material_input}' mapeado para Código {mapped_data['Código do Material']}")
            else: # Ambiguidade no Material
                # Ambiguidade
                produtos = material_info["Produto"].tolist()
                codigos = material_info["Cód"].astype(str).tolist()
                self._add_issue(issues, "ambiguidades", {
                    "campo": "Código do Material", "valor_original": material_input,
                    "mensagem": f"Encontrei múltiplos materiais que podem corresponder a '{material_input}':\n" + \
                                "\n".join([f"{i+1}. {p} (Código: {c})" for i, (p, c) in enumerate(zip(produtos, codigos))]) + \
                                "\nQual deles é o correto? (Responda com o número ou o código)", # Mensagem completa
                    # CORREÇÃO: Adiciona 'opcoes'
                    "opcoes": [{"produto": p, "codigo": c} for p, c in zip(produtos, codigos)]
                })
                logger.debug(f"(MappingAgent Material): Ambiguidade por nome: '{material_input}' - {len(material_info)} matches")
        else:
            # Não encontrado
            self._add_issue(issues, "avisos", {
                "campo": "Código do Material", "valor_original": material_input,
                "mensagem": f"Material '{material_input}' (código ou nome) não encontrado ou inválido."
            })
            logger.debug(f"(MappingAgent Material): Valor '{material_input}' não encontrado/mapeado.")

    def _map_condicao_pagamento(self, mapped_data: dict, issues: dict):
        condicao_input_original = mapped_data.get("Condição de Pagamento") # Pode ter hífen
        if not condicao_input_original: return

        # 1. Verifica se já é um código válido (normalizado s/ hífen)
        input_norm_cod = normalize_string(str(condicao_input_original), remove_hyphens=True)
        if input_norm_cod in self.valid_condicao_codes:
            logger.debug(f"(MappingAgent CondPagto): Input '{condicao_input_original}' normalizado p/ código válido '{input_norm_cod}'.")
            mapped_data["Condição de Pagamento"] = input_norm_cod # Usa código validado
            return

        logger.debug(f"(MappingAgent CondPagto): Input '{condicao_input_original}' não é código. Tentando mapear (s/ hífen)...")
        # Normaliza o input REMOVENDO HÍFENS para comparar com termos
        condicao_norm = normalize_string(condicao_input_original, remove_hyphens=True)
        condicao_encontrada_direto = False

        # 2. Tenta mapeamento direto por termo normalizado (s/ hífen)
        # (self.condicao_map_norm_to_code já foi populado com chaves sem hífens)
        if condicao_norm and condicao_norm in self.condicao_map_norm_to_code:
            mapped_code = self.condicao_map_norm_to_code[condicao_norm]
            mapped_data["Condição de Pagamento"] = mapped_code # Atualiza com o código mapeado
            condicao_encontrada_direto = True
            logger.debug(f"(MappingAgent CondPagto): Termo '{condicao_input_original}' (norm s/hífen: '{condicao_norm}') mapeado DIRETAMENTE para '{mapped_code}'")

        # 3. Se o mapeamento direto falhou, TENTA DIVIDIR (já usa normalização s/ hífen internamente)
        split_successful = False
        if not condicao_encontrada_direto:
            logger.debug(f"(MappingAgent CondPagto): Mapeamento direto para '{condicao_input_original}' falhou. Tentando SPLIT.")
            # _attempt_split_and_remap_payment foi atualizado para usar remove_hyphens=True
            split_successful = self._attempt_split_and_remap_payment(
                str(condicao_input_original), mapped_data, issues
            )
            if split_successful:
                # Se o split foi bem-sucedido, mapped_data["Condição de Pagamento"] e possivelmente
                # mapped_data["Forma de Pagamento"] já foram atualizados com os códigos corretos.
                # O aviso de "não encontrado" para o input original será removido pelo split.
                logger.info(f"(MappingAgent CondPagto): SPLIT bem-sucedido para '{condicao_input_original}'. Dados atualizados em mapped_data.")
                return # Sai, pois o split resolveu.
            else:
                logger.debug(f"(MappingAgent CondPagto): SPLIT falhou para '{condicao_input_original}'.")

        # 4. Adiciona aviso se NADA funcionou (nem direto, nem split bem-sucedido que resultou em código)
        # Verifica se o valor em mapped_data AINDA é o original (ou seja, não foi alterado para um código)
        if not condicao_encontrada_direto and not split_successful:
            # Se chegou aqui, nem o mapeamento direto funcionou, nem o split conseguiu achar um código para Condição.
            # O valor em mapped_data["Condição de Pagamento"] ainda é o `condicao_input_original`.
            self._add_issue(issues, "avisos", {
                "campo": "Condição de Pagamento", "valor_original": condicao_input_original,
                "mensagem": f"Condição de Pagamento '{condicao_input_original}' não encontrada, inválida ou não pôde ser dividida em partes reconhecidas."
            })
            logger.debug(f"(MappingAgent CondPagto): Valor '{condicao_input_original}' não mapeado diretamente nem por split para um código válido.")


    def _map_forma_pagamento(self, mapped_data: dict, issues: dict):
        forma_input_original = mapped_data.get("Forma de Pagamento")
        if not forma_input_original: return

        if str(forma_input_original) in self.valid_forma_codes:
            logger.debug(f"(MappingAgent FormaPagto): Input '{forma_input_original}' já é um código válido.")
            return

        logger.debug(f"(MappingAgent FormaPagto): Input '{forma_input_original}' não é código. Tentando mapear...")
        forma_norm = normalize_string(forma_input_original, remove_hyphens=True)
        forma_encontrada = False

        # 1. Tenta mapeamento por termo direto (Significado Normalizado ou MP Normalizado)
        if forma_norm and forma_norm in self.forma_map_norm_to_code:
            mapped_code = self.forma_map_norm_to_code[forma_norm]
            mapped_data["Forma de Pagamento"] = mapped_code
            forma_encontrada = True
            logger.debug(f"(MappingAgent FormaPagto): Termo DIRETO '{forma_input_original}' (norm: '{forma_norm}') mapeado para '{mapped_code}'")

        # 2. Se não encontrado por termo direto, tenta por Palavra-Chave
        if not forma_encontrada and forma_norm and forma_norm in self.forma_keyword_map_norm_to_codes:
            possible_codes = self.forma_keyword_map_norm_to_codes[forma_norm]
            if len(possible_codes) == 1:
                mapped_code = possible_codes[0]
                mapped_data["Forma de Pagamento"] = mapped_code
                forma_encontrada = True
                logger.debug(f"(MappingAgent FormaPagto): Keyword '{forma_input_original}' (norm: '{forma_norm}') mapeada para '{mapped_code}'")
            elif len(possible_codes) > 1:
                # AMBIGUIDADE através de Keyword
                # Recupera os significados originais para as opções
                opcoes_ambiguidade = []
                for code_mp in possible_codes:
                    # Encontra a linha no df_forma original para pegar o significado
                    row_df = self.df_forma[self.df_forma['MP'] == code_mp]
                    if not row_df.empty:
                        significado_original = row_df.iloc[0]['Significado']
                        opcoes_ambiguidade.append({"descricao": significado_original, "codigo": code_mp})
                    else: # Fallback se não achar o significado
                        opcoes_ambiguidade.append({"descricao": f"Código {code_mp}", "codigo": code_mp})

                self._add_issue(issues, "ambiguidades", {
                    "campo": "Forma de Pagamento", "valor_original": forma_input_original,
                    "mensagem": f"A forma de pagamento '{forma_input_original}' pode se referir a mais de uma opção. Qual delas você quis dizer?\n" + \
                                "\n".join([f"{i+1}. {opt['descricao']} (Código: {opt['codigo']})" for i, opt in enumerate(opcoes_ambiguidade)]) + \
                                "\n(Responda com o número ou o código)",
                    "opcoes": opcoes_ambiguidade
                })
                logger.debug(f"(MappingAgent FormaPagto): Keyword ambígua '{forma_norm}' para '{forma_input_original}' - Opções: {possible_codes}")
                # Não define forma_encontrada = True, pois a ambiguidade precisa ser resolvida.
                # O valor original permanece em mapped_data["Forma de Pagamento"]
                return # Retorna para que o Orchestrator lide com a ambiguidade

        # 3. Se ainda não encontrado, tenta SPLIT (como antes)
        split_successful = False
        if not forma_encontrada:
            logger.debug(f"(MappingAgent FormaPagto): Mapeamento direto/keyword para '{forma_input_original}' falhou. Tentando SPLIT.")
            split_successful = self._attempt_split_and_remap_payment(
                str(forma_input_original), mapped_data, issues
            )
            if split_successful:
                logger.info(f"(MappingAgent FormaPagto): SPLIT bem-sucedido para '{forma_input_original}'. Dados atualizados.")
                # Verifica se o valor em mapped_data["Forma de Pagamento"] agora é um código válido
                if str(mapped_data.get("Forma de Pagamento")) in self.valid_forma_codes:
                    forma_encontrada = True # O split encontrou um código válido
                # Não retorna aqui ainda, pois pode ter sido split de Condição que continha Forma.
                # A lógica de aviso no final cuidará disso.

        # 4. Fallback para 'contains' (OPCIONAL, pode ser removido se as keywords forem boas)
        # Só executa se nada funcionou ATÉ AGORA e se o valor NÃO foi alterado por um split para um código válido.
        if not forma_encontrada and self._is_valid_df(self.df_forma, 'Significado_NORMALIZADO') and \
        (mapped_data.get("Forma de Pagamento") == forma_input_original or not str(mapped_data.get("Forma de Pagamento")) in self.valid_forma_codes) : # Garante que não foi mapeado por split

            logger.debug(f"(MappingAgent FormaPagto): Mapeamento direto/keyword/split falhou. Tentando 'contains' no significado para '{forma_norm}'.")
            try:
                escaped_forma_norm = re.escape(forma_norm)
                matches = self.df_forma[self.df_forma['Significado_NORMALIZADO'].str.contains(escaped_forma_norm, na=False, regex=True)]
            except Exception as e:
                logger.error(f"Erro na busca por forma de pagamento (contains): {e}")
                matches = pd.DataFrame()

            if not matches.empty:
                if len(matches) == 1:
                    mapped_code = str(matches.iloc[0]['MP'])
                    mapped_data["Forma de Pagamento"] = mapped_code
                    forma_encontrada = True
                    logger.debug(f"(MappingAgent FormaPagto): Significado '{forma_input_original}' (norm: '{forma_norm}') mapeado para '{mapped_code}' (via CONTAINS)")
                else: # Ambiguidade via 'contains'
                    if mapped_data.get("Forma de Pagamento") == forma_input_original: # Só se ainda não resolvido
                        descricoes = matches['Significado'].tolist()
                        codigos_mp = matches['MP'].astype(str).tolist()
                        self._add_issue(issues, "ambiguidades", {
                            "campo": "Forma de Pagamento", "valor_original": forma_input_original,
                            "mensagem": f"Para a Forma de Pagamento '{forma_input_original}', qual destas opções você se refere (encontradas por similaridade)?\n" + \
                                        "\n".join([f"{i+1}. {d} (Código: {c})" for i, (d, c) in enumerate(zip(descricoes, codigos_mp))]) + \
                                        "\n(Responda com o número ou o código)",
                            "opcoes": [{"descricao": d, "codigo": c} for d, c in zip(descricoes, codigos_mp)]
                        })
                        logger.debug(f"(MappingAgent FormaPagto): Ambiguidade por significado (contains): '{forma_input_original}' - {len(matches)} matches")
                        return # Retorna para Orchestrator lidar com a ambiguidade

        # 5. Adiciona aviso se NADA funcionou e o valor AINDA é o original
        # (nem direto, nem keyword, nem split que resultou em código, nem contains que resultou em código único)
        # E não há uma ambiguidade pendente para este campo
        if not forma_encontrada and mapped_data.get("Forma de Pagamento") == forma_input_original and \
        not any(iss.get("campo") == "Forma de Pagamento" and iss.get("tipo") == "ambiguidades" for iss in issues.get("ambiguidades", [])):
            self._add_issue(issues, "avisos", {
                "campo": "Forma de Pagamento", "valor_original": forma_input_original,
                "mensagem": f"Forma de Pagamento '{forma_input_original}' não encontrada ou inválida."
            })
            logger.debug(f"(MappingAgent FormaPagto): Valor '{forma_input_original}' não mapeado.")

    def _ensure_default_keys(self, mapped_data: dict):
        """Garante que todas as chaves esperadas existam no dicionário final."""
        default_keys = [
            "Cliente", "CNPJ/CPF", "Planta", "Condição de Pagamento",
            "Forma de Pagamento", "Código do Material", "Quantidade Total",
            "Cadência", "Vendedor", "Cidade", "Data de Negociação",
            "Incoterms", "Preço Frete", "Valor", "Nome do cliente",
            "Código do cliente", "Email do vendedor", "Campanha"
        ]
        for key in default_keys:
            mapped_data.setdefault(key, None)


    def map(self, extracted_data: dict, original_input_text: str) -> tuple[dict, dict]:
        """
        Realiza o mapeamento ('de-para') dos dados extraídos.

        Args:
            extracted_data (dict): Dicionário com dados extraídos pelo ExtractionAgent.

        Returns:
            tuple[dict, dict]: Uma tupla contendo:
                - mapped_data (dict): Cópia dos dados extraídos com campos mapeados/adicionados.
                - mapping_issues (dict): Dicionário com listas de 'avisos', 'erros', 'ambiguidades'.
        """
        if not self.data_loaded_successfully:
            return extracted_data, {"avisos": [], "erros": ["DataFrames de mapeamento não carregados."], "ambiguidades": []}

        mapped_data = extracted_data.copy()
        mapping_issues = {"avisos": [], "erros": [], "ambiguidades": []}

        # Executa mapeamento para cada campo relevante
        self._map_planta(mapped_data, mapping_issues, original_input_text)
        self._map_cliente(mapped_data, mapping_issues)
        self._map_material(mapped_data, mapping_issues)
        self._map_condicao_pagamento(mapped_data, mapping_issues)
        self._map_forma_pagamento(mapped_data, mapping_issues)

        # Garante chaves padrão e email do vendedor (que não tem mapeamento)
        mapped_data.setdefault("Email do vendedor", None)
        self._ensure_default_keys(mapped_data)

        logger.debug("(MappingAgent): Dados mapeados finais:")
        logger.debug(json.dumps(mapped_data, indent=2, ensure_ascii=False))
        logger.debug("(MappingAgent): Issues de mapeamento finais:")
        logger.debug(json.dumps(mapping_issues, indent=2, ensure_ascii=False))

        return mapped_data, mapping_issues

--- Conteúdo de src/agents/orchestration_agent.py ---

# src/agents/orchestration_agent.py
import json
import logging
import re
from agents.extraction_agent import ExtractionAgent
from agents.mapping_agent import MappingAgent
from utils.formatting import format_cadencia, format_final_summary_text
from utils.normalization import normalize_string
from datetime import datetime

logger = logging.getLogger(__name__)

class OrchestrationAgent:
    """
    Orquestra o fluxo de extração, mapeamento, validação e interação
    com o usuário para processar um pedido.
    """
    def __init__(self, extraction_agent: ExtractionAgent, mapping_agent: MappingAgent):
        self.extraction_agent = extraction_agent
        self.mapping_agent = mapping_agent
        self._reset_state_data() # Inicializa o estado

        # Campos obrigatórios após o mapeamento inicial
        self.mandatory_fields_post_mapping = [
             "CNPJ/CPF",
             "Planta", "Condição de Pagamento", "Forma de Pagamento",
             "Código do Material", "Cadência", "Vendedor", "Cidade",
             "Data de Negociação", "Incoterms", "Valor",
        ]
        # Campos que, se presentes, DEVEM ter sido mapeados com sucesso
        self.fields_requiring_valid_mapping = [
            "Condição de Pagamento", "Forma de Pagamento", "Código do Material"
        ]
        logger.info("Agente Orquestrador inicializado (pronto para carregar estado).")

    # --- Métodos de Gerenciamento de Estado ---
    def _reset_state_data(self):
        """Reseta o estado interno para um novo pedido."""
        self.state = {
            "request_data": {},
            "mapping_issues": {"avisos": [], "erros": [], "ambiguidades": []},
            "last_question_context": None,
            "last_asked_fields": None,
            "pending_confirmation": False,
            "pending_ambiguity": None, # Guarda dados da ambiguidade atual
            "pending_confirmation_payload": None, # Guarda dados do resumo final
            "current_original_input_text": None
        }
        logger.debug("Estrutura de estado resetada para o padrão.")

    def load_state(self, state_data: dict):
        """Carrega um estado previamente salvo."""
        # Garante que todas as chaves padrão existem ao carregar
        default_state = {
            "request_data": {}, "mapping_issues": {"avisos": [], "erros": [], "ambiguidades": []},
            "last_question_context": None, "last_asked_fields": None,
            "pending_confirmation": False, "pending_ambiguity": None,
            "pending_confirmation_payload": None
        }
        default_state.update(state_data)
        self.state = default_state
        logger.debug(f"Estado carregado: {json.dumps(self.state, indent=2, ensure_ascii=False)}")

    def get_state_dict(self) -> dict:
        """Retorna uma cópia do estado atual."""
        return self.state.copy()

    def _update_request_data(self, new_data: dict | None):
        """Atualiza o dicionário request_data no estado de forma segura."""
        if not new_data: return
        logger.debug(f"Atualizando request_data com: {json.dumps(new_data, indent=2, ensure_ascii=False)}")
        current_request_data = self.state.get("request_data", {})
        # Regra: Atualiza se o novo valor não for None,
        # ou se a chave não existe no estado atual,
        # ou se o valor atual é None,
        # ou se o novo valor é uma string vazia (permitindo limpar campos)
        for key, value in new_data.items():
             if value is not None or key not in current_request_data or current_request_data.get(key) is None:
                 current_request_data[key] = value
             elif isinstance(value, str) and value == "" and current_request_data.get(key) is not None:
                 current_request_data[key] = value # Permite limpar com string vazia
        self.state["request_data"] = current_request_data
        logger.debug(f"request_data após atualização: {json.dumps(self.state['request_data'], indent=2, ensure_ascii=False)}")

    def _check_missing_fields(self, field_list: list[str]) -> list[str]:
        """Verifica quais campos da lista estão faltando (None ou vazio) em request_data."""
        missing = []
        request_data = self.state.get("request_data", {})
        for field in field_list:
            value = request_data.get(field)
            if value is None or (isinstance(value, str) and not value.strip()):
                 missing.append(field)
        return missing

    # --- Métodos de Formatação de Resposta ---
    # (Estes métodos são relativamente simples e focados, não precisam de grande refatoração interna)
    def _format_user_question(self, message: str, context: str = None, missing_fields: list = None) -> dict:
        """Formata uma resposta que requer input do usuário."""
        self.state["last_question_context"] = context
        self.state["last_asked_fields"] = missing_fields
        self.state["pending_ambiguity"] = None # Limpa ambiguidade ao fazer nova pergunta
        self.state["pending_confirmation"] = False # Limpa confirmação
        logger.debug(f"Formatando pergunta: Contexto='{context}', Campos Pedidos={missing_fields}")
        return {"status": "needs_input", "message": message}

    def _format_final_summary(self, final_data: dict) -> dict:
        """Formata o resumo final para confirmação do usuário."""
        self.state["pending_confirmation"] = True
        self.state["last_question_context"] = "confirmation_response"
        self.state["last_asked_fields"] = None
        self.state["pending_ambiguity"] = None
        self.state["pending_confirmation_payload"] = final_data.copy() # Armazena payload exato
        logger.debug("Formatando resumo para confirmação final e armazenando payload.")

        cadencia_fmt = final_data.get("Cadencia_Formatada", None) # Usa cadência já formatada
        formatted_summary = format_final_summary_text(final_data, cadencia_fmt)
        confirmation_message = (
            f"Por favor, revise os dados abaixo antes de prosseguir:\n\n"
            f"{formatted_summary}\n\n"
            f"Os dados estão corretos? Responda 'Sim' para confirmar, 'Não' para cancelar, "
            f"ou digite a informação que deseja corrigir (ex: 'Preço Frete é 500', 'Cidade é Cuiabá')."
        )
        return { "status": "needs_confirmation", "message": confirmation_message }

    def _format_success(self, ticket_id="N/A") -> dict:
        """Formata uma resposta de sucesso (usado internamente pelo orchestrator ou pela integração)."""
        logger.info(f"Processo concluído com sucesso (Ticket: {ticket_id}).")
        # Não reseta o estado aqui, quem chama é responsável por isso
        return {"status": "completed", "message": f"Solicitação processada com sucesso! (Ticket: {ticket_id})"}

    def _format_confirmed_for_creation(self) -> dict:
        """Formata a resposta indicando que os dados foram confirmados e estão prontos para criação do ticket."""
        logger.info("Usuário confirmou. Retornando payload para criação do ticket.")
        final_payload = self.state.get("pending_confirmation_payload")
        # Importante: Resetar o estado ANTES de retornar, para evitar reprocessamento
        self._reset_state_data()
        return {
            "status": "confirmed_for_creation",
            "message": "Confirmação recebida. Preparando para criar o chamado...", # Mensagem temporária
            "payload": final_payload # Envia o payload final
        }

    def _format_error(self, error_message: str) -> dict:
        """Formata uma resposta de erro."""
        logger.error(f"Erro no orquestrador: {error_message}")
        # Considerar resetar o estado em caso de erro? Depende da estratégia.
        # self._reset_state_data() # Descomentar se quiser resetar em erro
        return {"status": "error", "message": f"Ocorreu um erro: {error_message}"}

    def _format_abort(self) -> dict:
        """Formata uma resposta de cancelamento pelo usuário."""
        logger.info("Solicitação abortada pelo usuário.")
        original_payload = self.state.get("pending_confirmation_payload") # Pega antes de resetar, se houver
        self._reset_state_data() # Limpa o estado ao abortar
        return {"status": "aborted", "message": "Solicitação cancelada."}

    # --- Métodos de Lógica Principal ---

    def _handle_confirmation_response(self, user_text: str) -> dict | None:
        """Processa a resposta do usuário quando estava pendente de confirmação."""
        if not (self.state.get("pending_confirmation") and self.state.get("last_question_context") == "confirmation_response"):
            return None # Não estava esperando confirmação

        logger.info("Processando resposta no estado 'pending_confirmation'.")
        response_norm = normalize_string(user_text)
        confirmation_keywords = ["sim", "s", "yes", "y", "ok", "correto", "confirmar", "confirmo"]
        cancel_keywords = ["nao", "n", "no", "incorreto", "cancelar"]

        if response_norm in confirmation_keywords:
            return self._format_confirmed_for_creation() # Retorna status e payload para criação

        elif response_norm in cancel_keywords:
            return self._format_abort()

        else:
            # Tratar como EDIÇÃO
            return self._handle_user_edit(user_text)

    def _handle_user_edit(self, user_text: str) -> dict:
        """Processa a entrada do usuário como uma tentativa de edição dos dados do resumo."""
        logger.info(f"Resposta '{user_text}' não é confirmação/cancelamento. Tratando como EDIÇÃO.")
        original_payload = self.state.get("pending_confirmation_payload")
        if not original_payload:
            logger.error("Tentativa de edição, mas pending_confirmation_payload está vazio!")
            self._reset_state_data()
            return self._format_error("Ocorreu um problema ao tentar editar. Por favor, comece novamente.")

        # Prepara instrução de edição para o LLM
        original_data_summary_str = json.dumps(original_payload, ensure_ascii=False, indent=None) # Sem indentação para prompt
        edit_instruction = (
            f"ATENÇÃO: O usuário está CORRIGINDO/EDITANDO os dados que foram apresentados a ele. "
            f"Os dados apresentados foram (aproximadamente): {original_data_summary_str}. "
            f"Analise a CORREÇÃO fornecida pelo usuário no 'Input do Usuário' abaixo. "
            f"Sua tarefa é extrair APENAS os campos que o usuário está explicitamente tentando alterar ou adicionar, com seus NOVOS valores. "
            f"Retorne um JSON contendo SOMENTE os campos corrigidos/adicionados. Ignore campos dos dados originais que não foram mencionados na correção atual."
        )

        logger.debug(f"Chamando ExtractionAgent para EDIÇÃO. Texto: '{user_text}'")
        extracted_edit_data = self.extraction_agent.extract(user_text, custom_instruction=edit_instruction)

        if extracted_edit_data:
            logger.info(f"Dados extraídos da edição: {json.dumps(extracted_edit_data, indent=2, ensure_ascii=False)}")

            # Cria cópia dos dados originais e aplica edições
            updated_data = original_payload.copy()
            # Simula a atualização para usar a lógica de merge segura do _update_request_data
            temp_state_for_update = {"request_data": updated_data}
            self._apply_updates_to_dict(temp_state_for_update["request_data"], extracted_edit_data)
            updated_data = temp_state_for_update["request_data"]

            logger.debug(f"Dados após aplicar edição: {json.dumps(updated_data, indent=2, ensure_ascii=False)}")

            # Limpa estado de confirmação ANTES de revalidar
            self.state["pending_confirmation"] = False
            self.state["last_question_context"] = None
            self.state["pending_confirmation_payload"] = None
            self.state["last_asked_fields"] = None

            # Atualiza o estado principal e revalida
            self.state["request_data"] = updated_data # Atualiza estado real
            return self._run_mapping_and_validation() # Re-executa mapeamento e validação

        else:
            logger.warning("Não foi possível extrair nenhuma edição da resposta do usuário.")
            # Pede para tentar de novo, mantendo o estado de confirmação
            return self._format_user_question(
                "Desculpe, não consegui entender a correção. Poderia tentar digitar novamente a informação que deseja alterar? Ou responda 'Sim'/'Não' para os dados apresentados.",
                context="confirmation_response" # Mantém o contexto
            )

    def _apply_updates_to_dict(self, target_dict: dict, updates: dict):
         """Função auxiliar para aplicar atualizações (similar a _update_request_data mas em dict genérico)."""
         for key, value in updates.items():
             if value is not None or key not in target_dict or target_dict.get(key) is None:
                 target_dict[key] = value
             elif isinstance(value, str) and value == "" and target_dict.get(key) is not None:
                 target_dict[key] = value

    def _resolve_pending_ambiguity(self, user_response: str) -> bool:
        """Tenta resolver uma ambiguidade pendente com base na resposta do usuário."""
        pending_ambiguity_data = self.state.get("pending_ambiguity")
        if not pending_ambiguity_data: return False # Nenhuma ambiguidade pendente

        options = pending_ambiguity_data.get("options", [])
        field_to_update = pending_ambiguity_data.get("field") # Ex: "Código do cliente"
        original_field_name = pending_ambiguity_data.get("original_field_name", field_to_update) # Ex: "Cliente"
        resolved_value = None
        update_dict = {}
        chosen_option = None # Adicionado para ter acesso fora do loop de texto

        logger.debug(f"Tentando resolver ambiguidade para '{original_field_name}' com resposta '{user_response}'. Opções: {options}")

        # Tentativa por índice numérico
        try:
            choice_index = int(user_response.strip()) - 1
            if 0 <= choice_index < len(options):
                chosen_option = options[choice_index] # <<< GUARDA A OPÇÃO ESCOLHIDA
                resolved_value = chosen_option.get("codigo") # O valor a ser usado é sempre 'codigo'
                match_type = "índice numérico"
        except ValueError:
             pass # Não é número, tenta por texto

        # Tentativa por texto (código ou descrição normalizados)
        if resolved_value is None:
            user_response_norm = normalize_string(user_response)
            if user_response_norm:
                for option_item in options: # Renomeado para evitar conflito com a variável 'options' mais externa
                    codigo_option = option_item.get("codigo")
                    desc_option = option_item.get("descricao") or option_item.get("produto") or option_item.get("nome")
                    match_found = False
                    if codigo_option and normalize_string(str(codigo_option)) == user_response_norm:
                        resolved_value = codigo_option
                        match_type = "match de código"
                        match_found = True
                    elif desc_option and normalize_string(desc_option) == user_response_norm:
                        resolved_value = codigo_option # Mesmo se match for na descrição, o valor é o código
                        match_type = "match de descrição"
                        match_found = True

                    if match_found:
                        chosen_option = option_item # <<< GUARDA A OPÇÃO ESCOLHIDA
                        break

        # Se encontrou um valor, atualiza o estado
        if resolved_value is not None and chosen_option is not None: # <<< VERIFICA chosen_option
            logger.debug(f"Ambiguidade resolvida por {match_type} -> Valor '{resolved_value}'")
            update_dict[field_to_update] = resolved_value
            # Se for ambiguidade de cliente, atualiza também nome e CNPJ/CPF
            if original_field_name == "Cliente":
                nome_da_planilha_escolhido = chosen_option.get("nome")
                update_dict["Nome do cliente"] = nome_da_planilha_escolhido
                update_dict["CNPJ/CPF"] = chosen_option.get("cnpj_cpf")
                update_dict["Cliente"] = nome_da_planilha_escolhido


            self._update_request_data(update_dict)
            # Limpa o estado de ambiguidade e contexto
            self.state["pending_ambiguity"] = None
            self.state["last_question_context"] = None
            self.state["last_asked_fields"] = None
            logger.debug("Ambiguidade resolvida. request_data atualizado.")
            return True
        else:
            logger.debug(f"Resposta '{user_response}' não resolveu a ambiguidade.")
            return False

    def _handle_ambiguity_response(self, user_text: str) -> dict | None:
         """Processa a resposta do usuário quando estava pendente de ambiguidade."""
         if not self.state.get("pending_ambiguity"):
             return None 

         # Guardar o campo da ambiguidade que está sendo resolvida
         resolved_ambiguity_field = self.state["pending_ambiguity"].get("original_field_name") or self.state["pending_ambiguity"].get("field")


         if self._resolve_pending_ambiguity(user_text): # Isso já limpa self.state["pending_ambiguity"]
             logger.debug(f"Ambiguidade para '{resolved_ambiguity_field}' resolvida. Limpando issue correspondente de mapping_issues.")
             
             # Limpar a issue de ambiguidade específica que foi resolvida
             if "mapping_issues" in self.state and "ambiguidades" in self.state["mapping_issues"]:
                 current_ambiguities = self.state["mapping_issues"]["ambiguidades"]
                 self.state["mapping_issues"]["ambiguidades"] = [
                     amb for amb in current_ambiguities 
                     if (amb.get("original_field_name") or amb.get("campo")) != resolved_ambiguity_field
                 ]
                 if not self.state["mapping_issues"]["ambiguidades"]: # Se a lista ficou vazia
                     del self.state["mapping_issues"]["ambiguidades"]


             logger.debug("Re-executando mapeamento e validação após resolução de ambiguidade.")
             return self._run_mapping_and_validation()
         else:
             # Falhou em resolver, pergunta novamente
             pending_ambiguity_data = self.state.get("pending_ambiguity", {})
             original_question = pending_ambiguity_data.get('original_question', "Por favor, escolha uma das opções.")
             # Reusa o contexto original da pergunta de ambiguidade
             return self._format_user_question(
                 f"Não consegui entender sua escolha. {original_question}",
                 context=pending_ambiguity_data.get('context')
                 # Não passa missing_fields aqui, pois estamos no fluxo de ambiguidade
             )

    def _filter_extraction_by_context(self, extracted_data: dict | None) -> dict:
        """Filtra os dados extraídos para manter apenas os campos solicitados no contexto."""
        last_context = self.state.get("last_question_context")
        last_asked = self.state.get("last_asked_fields")

        if not (last_context and last_asked and extracted_data):
            logger.debug("[Filtro Contexto] Nenhum contexto/campos pedidos ou extração vazia, pulando filtragem.")
            # Limpa as flags de contexto *mesmo se não filtrou*, pois a interação atual as consumiu
            self.state["last_question_context"] = None
            self.state["last_asked_fields"] = None
            return extracted_data or {} # Retorna dict vazio se extracted_data for None

        logger.debug(f"[Filtro Contexto] Iniciando. Contexto: '{last_context}', Campos Pedidos: {last_asked}")
        logger.debug(f"[Filtro Contexto] Dados extraídos (pré-filtro): {json.dumps(extracted_data, indent=2, ensure_ascii=False)}")

        # Mapeia nomes de campos pedidos para chaves de extração válidas
        valid_extraction_keys = set()
        for field_name in last_asked:
            # Simplifica o nome do campo removendo detalhes como (obrigatório...)
            base_field_name = re.sub(r'\s*\(.*\)\s*', '', field_name).strip()
            # Adiciona mapeamentos específicos (ex: pedir "Cliente" permite extrair "Cliente" ou "CNPJ/CPF")
            if base_field_name in ["Cliente", "Nome do cliente"]:
                valid_extraction_keys.update(["Cliente", "CNPJ/CPF", "Nome do cliente"])
            elif base_field_name == "CNPJ/CPF":
                valid_extraction_keys.add("CNPJ/CPF")
            elif base_field_name == "Código do cliente":
                valid_extraction_keys.add("Código do cliente")
            # Adicionar outros mapeamentos se necessário
            else:
                valid_extraction_keys.add(base_field_name) # Adiciona o nome base como chave válida

        logger.debug(f"[Filtro Contexto] Chaves de extração válidas para este contexto: {valid_extraction_keys}")

        filtered_data = {}
        for key, value in extracted_data.items():
            # Mantém se a chave for válida E o valor não for nulo (LLM pode retornar nulls para campos não pedidos)
            if key in valid_extraction_keys and value is not None:
                filtered_data[key] = value
                logger.debug(f"[Filtro Contexto] Mantendo campo '{key}' (valor: '{value}')")
            elif key in valid_extraction_keys and value is None:
                 logger.debug(f"[Filtro Contexto] Ignorando campo '{key}' pois valor extraído é nulo (esperado no modo contextual).")
            else:
                 logger.debug(f"[Filtro Contexto] Ignorando campo extraído '{key}' pois não está em {valid_extraction_keys}.")

        logger.debug(f"[Filtro Contexto] Dados extraídos APÓS filtragem: {json.dumps(filtered_data, indent=2, ensure_ascii=False)}")

        # Limpa o contexto DEPOIS de usá-lo para filtrar
        self.state["last_question_context"] = None
        self.state["last_asked_fields"] = None

        return filtered_data

    def _run_mapping_and_validation(self) -> dict:
         """Executa o mapeamento e a validação pós-mapeamento."""
         logger.info("Iniciando mapeamento.")
         current_input_text = self.state.get("current_original_input_text", "")
         if not current_input_text:
             logger.warning("_run_mapping_and_validation chamado sem current_original_input_text no estado. Mapeamento de planta por substring do texto original pode ser limitado.")
         mapped_data, mapping_issues_result = self.mapping_agent.map(
             self.state["request_data"].copy(),
             original_input_text=current_input_text # Passa o texto original
         )

         self._update_request_data(mapped_data)
         self.state["mapping_issues"] = mapping_issues_result

         return self._run_post_mapping_validation()

    # --- Métodos de Validação Pós-Mapeamento (Refatorados) ---

    def _validate_mapping_issues(self) -> dict | None:
        """Verifica e lida com erros e ambiguidades do mapeamento."""
        mapping_issues = self.state.get("mapping_issues", {})

        # 5a. Erros Críticos de Mapeamento
        if mapping_issues.get("erros"):
            return self._format_error(f"Erro durante o mapeamento: {'; '.join(mapping_issues['erros'])}")

        # 5b. Ambigüidades
        if mapping_issues.get("ambiguidades"):
            ambiguity = mapping_issues["ambiguidades"][0] # Trata uma por vez
            campo_issue = ambiguity.get("campo") # Ex: "Código do cliente"
            original_field_name_issue = ambiguity.get("original_field_name") # Ex: "Cliente"
            opcoes = ambiguity.get("opcoes", [])

            if not opcoes:
                 logger.error(f"Ambiguidade para '{campo_issue}' sem opções definidas.")
                 return self._format_error(f"Inconsistência interna ao tentar resolver ambiguidade para '{campo_issue}'.")

            question = ambiguity.get("mensagem", f"Encontramos múltiplos resultados para '{ambiguity.get('valor_original')}'. Por favor, escolha.")
            # Usar o original_field_name_issue se disponível para o contexto, senão o campo_issue
            context_field_name_for_log = original_field_name_issue if original_field_name_issue else campo_issue
            context = f"resolve_ambiguity_{context_field_name_for_log.lower().replace(' ', '_')}"

            # Prepara os dados para guardar no estado pending_ambiguity
            self.state["pending_ambiguity"] = {
                "field": campo_issue, # O campo a ser atualizado diretamente com o código da opção
                "original_field_name": original_field_name_issue, # O nome original do campo que gerou a ambiguidade (ex: "Cliente")
                "options": opcoes,
                "context": context,
                "original_question": question
            }
            logger.info(f"Ambiguidade detectada para '{original_field_name_issue if original_field_name_issue else campo_issue}'. Solicitando esclarecimento ao usuário.")
            self.state["last_question_context"] = context
            self.state["last_asked_fields"] = None
            self.state["pending_confirmation"] = False
            logger.debug(f"Formatando pergunta de ambiguidade diretamente. Contexto='{context}'")
            return {"status": "needs_input", "message": question}

        return None

    def _validate_cliente_info(self, request_data: dict, missing_keys: list, missing_msgs: list):
        """Valida a presença de informações do cliente.
        Se CNPJ/CPF estiver faltando, será pego pela validação global.
        Esta função garante que, se não tivermos Código do Cliente NEM CNPJ/CPF,
        o Nome do Cliente seja solicitado.
        """
        cliente_cod = request_data.get("Código do cliente")
        cliente_cnpj = request_data.get("CNPJ/CPF")
        # Considera tanto o nome mapeado (Nome do cliente) quanto o nome extraído (Cliente)
        cliente_nome_mapeado = request_data.get("Nome do cliente")
        cliente_nome_extraido = request_data.get("Cliente")
        nome_presente = bool(cliente_nome_mapeado or cliente_nome_extraido)

        # Se não temos um código de cliente E não temos um CNPJ/CPF,
        # então o nome do cliente é essencial para prosseguir (seja para novo cadastro ou tentativa de busca).
        if not cliente_cod and not cliente_cnpj:
            if not nome_presente:
                if "Cliente" not in missing_keys: # Usar "Cliente" como chave para pedir ao LLM
                    missing_keys.append("Cliente")
                    missing_msgs.append("Nome do cliente (Nome é obrigatório se cliente for novo e não tiver CPF/CNPJ)")
                logger.debug("Validação Cliente: Código e CNPJ/CPF ausentes. Nome do cliente é necessário.")
            else:
                logger.debug("Validação Cliente: Código e CNPJ/CPF ausentes, mas nome presente. OK por ora (CNPJ/CPF será pego pela validação global se faltar).")
        elif not cliente_cod and cliente_cnpj and not nome_presente:
            # Temos CNPJ, mas nem código nem nome. O nome ainda seria útil.
            if "Cliente" not in missing_keys:
                missing_keys.append("Cliente")
                missing_msgs.append("Nome do cliente (recomendado para confirmação/novo cadastro)")
            logger.debug("Validação Cliente: Código ausente, CNPJ presente, mas Nome ausente. Solicitando Nome.")
        else:
            logger.debug(f"Validação Cliente: Código='{cliente_cod}', CNPJ/CPF='{cliente_cnpj}', Nome Presente='{nome_presente}'. Verificações de cliente OK ou delegadas.")

    def _validate_frete_condicional(self, request_data: dict, missing_keys: list, missing_msgs: list):
        """Valida a obrigatoriedade do Preço Frete com base no Incoterms."""
        incoterms_value = request_data.get('Incoterms')
        incoterms = str(incoterms_value).upper() if incoterms_value else ''
        preco_frete = request_data.get('Preço Frete')

        # Define o que é considerado frete ausente (None, vazio, 'null', 0)
        frete_ausente = False
        if preco_frete is None:
            frete_ausente = True
        elif isinstance(preco_frete, str):
            valor_str_limpo = preco_frete.strip()
            if not valor_str_limpo or valor_str_limpo.lower() == 'null':
                frete_ausente = True
        elif isinstance(preco_frete, (int, float)) and preco_frete == 0:
            # Consideramos 0 como ausente para fins de obrigatoriedade,
            # a menos que o Incoterm seja FOB/TPD.
            # Se for CIF 0, ainda pode ser um valor explícito.
            # Se for FOB 0 ou TPD 0, é aceitável.
            if incoterms not in ['FOB', 'TPD']:
                frete_ausente = True
            # Se for FOB/TPD e o preço é 0, não consideramos ausente para a lógica de validação abaixo.
            # A formatação cuidará de exibir "R$ 0,00 (FOB)" ou "N/A (FOB)" se fosse None.


        logger.debug(f"Validação Frete: Incoterms='{incoterms}', PrecoFrete='{preco_frete}', Ausente={frete_ausente}")

        # MODIFICAÇÃO: Adiciona 'TPD' à lista de Incoterms que não exigem frete
        if incoterms and incoterms not in ['FOB', 'TPD'] and frete_ausente:
            if "Preço Frete" not in missing_keys:
                logger.debug(f"Adicionando Preço Frete (Obrigatório para Incoterms: {incoterms})")
                missing_keys.append("Preço Frete")
                missing_msgs.append(f"Preço Frete (obrigatório para Incoterms '{incoterms}')")

    def _validate_mapped_fields(self, request_data: dict, missing_keys: list, missing_msgs: list):
         """Verifica se campos que exigem mapeamento válido realmente foram mapeados."""
         logger.debug("Verificando avisos de mapeamento para campos mapeáveis obrigatórios.")
         avisos_mapeamento = self.state.get("mapping_issues", {}).get("avisos", [])
         campos_com_aviso_invalido = set()

         for aviso in avisos_mapeamento:
             campo_aviso = aviso.get("campo")
             # Verifica se o aviso é sobre um campo que precisa de mapeamento válido E se ele ainda está presente no request_data
             # (pode ter sido corrigido em interação anterior mas o aviso persistiu no estado)
             if campo_aviso in self.fields_requiring_valid_mapping and request_data.get(campo_aviso) == aviso.get("valor_original"):
                  campos_com_aviso_invalido.add(campo_aviso)
                  logger.warning(f"Campo '{campo_aviso}' preenchido com valor inválido '{aviso.get('valor_original')}' (conforme aviso de mapeamento).")

         for campo in campos_com_aviso_invalido:
              if campo not in missing_keys:
                   missing_keys.append(campo)
                   valor_original = request_data.get(campo, 'N/A')
                   missing_msgs.append(f"{campo} (valor '{valor_original}' inválido ou não encontrado)")
                   # Remove mensagens genéricas para este campo se houver
                   missing_msgs = [msg for msg in missing_msgs if msg != campo]


    def _validate_and_format_cadencia(self, request_data: dict, missing_keys: list, missing_msgs: list):
        """Formata a cadência e valida o resultado."""
        logger.info("Formatando e validando cadência.")
        cadencia_original = request_data.get("Cadência")
        qtd_total_str = request_data.get("Quantidade Total")
        data_negociacao = request_data.get("Data de Negociação")

        # Extrai parte numérica da quantidade total, se houver
        qtd_total_num_str = None
        if qtd_total_str:
            match = re.search(r'(\d[\d.,]*)', str(qtd_total_str))
            if match:
                qtd_total_num_str = re.sub(r'[.,](?=\d{3})', '', match.group(1)) # Remove milhar
                qtd_total_num_str = qtd_total_num_str.replace(',', '.') # Ajusta decimal

        # Tenta formatar
        cadencia_formatada = format_cadencia(cadencia_original, qtd_total=qtd_total_num_str, data_negociacao=data_negociacao)

        # Atualiza no request_data (mesmo se for None, para ficar consistente)
        request_data["Cadencia_Formatada"] = cadencia_formatada
        self.state["request_data"] = request_data # Atualiza estado

        # Valida se a formatação falhou para uma cadência original não vazia
        if cadencia_original and cadencia_formatada is None:
            if "Cadência" not in missing_keys:
                logger.warning(f"Formatação da Cadência falhou para o input: '{cadencia_original}'. Solicitando novamente.")
                missing_keys.append("Cadência")
                missing_msgs.append("Cadência (formato inválido ou não reconhecido)")
                # Remove mensagem genérica de "Cadência" se houver
                missing_msgs = [msg for msg in missing_msgs if msg != "Cadência"]

    def _build_missing_fields_question(self, missing_keys: list[str], missing_msgs: list[str]) -> dict:
        """Constrói a pergunta para o usuário sobre os campos faltantes/inválidos."""
        campos_unicos_msg = sorted(list(dict.fromkeys(missing_msgs))) # Remove duplicatas de mensagens
        fields_str = ', '.join(campos_unicos_msg)

        # Determina um contexto mais específico para a pergunta, se possível
        context = "missing_fields"
        first_msg = campos_unicos_msg[0] if campos_unicos_msg else ""
        if any("Cliente" in f or "CNPJ/CPF" in f for f in first_msg):
             context = "missing_client_info"
        elif first_msg.startswith("Preço Frete"):
             context = "missing_frete_for_incoterm"
        elif first_msg.startswith("Cadência"):
             context = "invalid_cadencia_format"
        elif "inválido" in first_msg:
             context = "invalid_mapped_field_value"

        # Monta a pergunta
        question = f"Quase lá! Ainda preciso destas informações: {fields_str}."
        # Personaliza a pergunta com base no contexto, se aplicável
        if context == "missing_frete_for_incoterm":
             incoterms = self.state["request_data"].get('Incoterms', 'N/A')
             question = f"Como o Incoterms é '{incoterms}', por favor, informe o Preço Frete."
        elif context == "invalid_cadencia_format":
             cadencia_original = self.state["request_data"].get('Cadência', '')
             question = f"Não consegui entender o formato da Cadência fornecida ('{cadencia_original}'). Poderia informar novamente? (Ex: 50 FEV, 100 MAR/25, etc.)"
        elif context == "invalid_mapped_field_value":
             invalid_msg = next((msg for msg in campos_unicos_msg if 'inválido' in msg), fields_str)
             question = f"Por favor, corrija a informação: {invalid_msg}"

        # Retorna a pergunta formatada, passando as CHAVES únicas faltantes para o contexto da próxima extração
        campos_faltantes_keys_unicos = sorted(list(dict.fromkeys(missing_keys)))
        return self._format_user_question(question, context=context, missing_fields=campos_faltantes_keys_unicos)

    def _run_post_mapping_validation(self) -> dict:
        """Executa todas as validações após o mapeamento e retorna a próxima ação."""
        logger.debug("Executando validação pós-mapeamento.")
        logger.debug(f"Estado atual para validação: {json.dumps(self.state, indent=2, ensure_ascii=False)}")

        request_data_ref = self.state.get("request_data", {})
        if request_data_ref.get("Data de Negociação") is None:
            current_date_str = datetime.now().strftime("%d/%m/%Y")
            request_data_ref["Data de Negociação"] = current_date_str
            # self.state["request_data"] já é uma referência para request_data_ref,
            # mas para clareza ou se a estrutura mudar, pode ser explícito:
            # self._update_request_data({"Data de Negociação": current_date_str})
            # No entanto, modificar request_data_ref diretamente é mais eficiente aqui.
            logger.info(f"Data de Negociação ausente. Padrão definido para data atual: {current_date_str}")

        # 1. Verifica Erros e Ambigüidades do Mapeamento
        issue_response = self._validate_mapping_issues()
        if issue_response:
            return issue_response # Retorna pergunta de ambiguidade ou erro

        # Se não há erros/ambiguidades críticas, procede com validações de dados
        request_data = self.state.get("request_data", {})
        missing_post_mapping_keys = []
        campos_faltantes_msg = []

        # 2. Validação do Cliente
        self._validate_cliente_info(request_data, missing_post_mapping_keys, campos_faltantes_msg)

        # 3. Validação dos campos obrigatórios genéricos
        missing_generic = self._check_missing_fields(self.mandatory_fields_post_mapping)
        missing_post_mapping_keys.extend(k for k in missing_generic if k not in missing_post_mapping_keys)
        campos_faltantes_msg.extend(k for k in missing_generic if k not in campos_faltantes_msg)

        # 4. Validação Condicional do Frete
        self._validate_frete_condicional(request_data, missing_post_mapping_keys, campos_faltantes_msg)

        # 5. Validação de Campos que Precisam de Mapeamento Válido
        self._validate_mapped_fields(request_data, missing_post_mapping_keys, campos_faltantes_msg)

        # 6. Formatação e Validação da Cadência (atualiza request_data internamente)
        self._validate_and_format_cadencia(request_data, missing_post_mapping_keys, campos_faltantes_msg)

        # 7. Montar e Retornar Pergunta se Algo Falta
        logger.debug(f"Validação Final: Missing Keys = {missing_post_mapping_keys}")
        logger.debug(f"Validação Final: Mensagens Faltantes = {campos_faltantes_msg}")
        if missing_post_mapping_keys:
            return self._build_missing_fields_question(missing_post_mapping_keys, campos_faltantes_msg)

        # 8. Tudo OK - Preparar Confirmação Final
        logger.info("Validações pós-mapeamento concluídas sem pendências.")
        logger.info("Preparando resumo final para confirmação.")
        # Passa o request_data atualizado (com cadência formatada) para o resumo
        return self._format_final_summary(self.state["request_data"])


    # --- Método Principal de Processamento ---
    def process_user_input(self, user_text: str, metadata: dict = None) -> dict:
        """
        Processa a entrada do usuário, gerenciando o estado da conversa,
        extração, mapeamento e validação. Prioriza respostas a perguntas pendentes.
        """
        logger.info(f"--- Ciclo Orquestrador: Processando Input ---")
        logger.info(f"Texto do Usuário: '{user_text}'")
        self.state["current_original_input_text"] = user_text
        logger.debug(f"Estado ANTES do processamento (current_original_input_text SET): {json.dumps(self.state, indent=2, ensure_ascii=False)}")


        # --- PASSO 0: TRATAR RESPOSTAS PENDENTES PRIMEIRO ---
        if self.state.get("pending_confirmation") and self.state.get("last_question_context") == "confirmation_response":
            logger.info(">>> Estado é 'pending_confirmation'. Chamando _handle_confirmation_response.")
            confirmation_result = self._handle_confirmation_response(user_text) # user_text aqui é Sim/Não/Correção
            # Se for correção, current_original_input_text será o texto da correção, que é o esperado por _handle_user_edit
            if confirmation_result:
                return confirmation_result
            else:
                 logger.warning("!!! _handle_confirmation_response retornou None inesperadamente.")
                 return self._format_error("Ocorreu um problema ao processar sua resposta de confirmação.")

        if self.state.get("pending_ambiguity"):
            logger.info(">>> Estado é 'pending_ambiguity'. Chamando _handle_ambiguity_response.")
            ambiguity_result = self._handle_ambiguity_response(user_text) # user_text é a escolha da ambiguidade
            if ambiguity_result:
                return ambiguity_result
            else:
                 logger.warning("!!! _handle_ambiguity_response retornou None inesperadamente.")
                 return self._format_error("Ocorreu um problema ao processar sua resposta para a ambiguidade.")


        if self.state.get("last_question_context") == "awaiting_user_correction_text":
            logger.info(">>> Contexto é 'awaiting_user_correction_text'. Chamando _handle_user_edit com o texto da correção.")
            edit_result = self._handle_user_edit(user_text) 
            return edit_result

        # --- PASSO 1: EXTRAÇÃO (Só executa se não for resposta a confirmação/ambiguidade) ---
        logger.info(">>> Input não é confirmação/ambiguidade direta. Iniciando extração normal.")
        last_asked = self.state.get("last_asked_fields")
        extracted_data = self.extraction_agent.extract(user_text, context_fields=last_asked)

        # --- Tratamento de Falha na Extração (mantém a lógica atual) ---
        if extracted_data is None:
            logger.error(f"Extração retornou None para o input '{user_text[:50]}...' mesmo após retentativas.")
            if user_text.strip(): 
                if last_asked: 
                    fields_str = ', '.join(last_asked)
                    return self._format_user_question(
                        f"Desculpe, tive um problema temporário ao processar sua resposta para '{fields_str}'. Poderia tentar fornecer novamente?",
                        context=self.state.get("last_question_context"), 
                        missing_fields=last_asked 
                    )
                else: 
                    return self._format_user_question(
                        "Desculpe, tive um problema temporário para processar sua solicitação. Poderia tentar enviar os dados novamente?",
                        context="extraction_failed_initial"
                    )
            else: 
                return self._format_user_question("Por favor, insira os dados do pedido.", context="empty_input")
        # --- FIM DO TRATAMENTO DE FALHA NA EXTRAÇÃO ---


        # --- PASSO 2: FILTRAGEM DA EXTRAÇÃO BASEADA NO CONTEXTO ---
        # Usa a função _filter_extraction_by_context que já existe e limpa o contexto internamente
        logger.debug(">>> Aplicando filtro de contexto (se aplicável).")
        filtered_data = self._filter_extraction_by_context(extracted_data)
        
        original_last_context = self.state.get("last_question_context") # Captura antes de _filter_extraction_by_context limpar
        original_last_asked = self.state.get("last_asked_fields")     # Captura antes de _filter_extraction_by_context limpar
        # _filter_extraction_by_context já limpou o contexto do ESTADO se ele foi usado.

        if original_last_context and original_last_asked and not filtered_data and extracted_data:
            logger.warning("[Filtro Contexto Pós-Execução] Filtragem removeu todos os campos extraídos válidos.")
            fields_str = ', '.join(original_last_asked)
            # Repergunta, restaurando o contexto para a nova pergunta.
            return self._format_user_question(
                f"Não consegui identificar a informação para '{fields_str}' na sua resposta. Poderia fornecer novamente?",
                context=original_last_context, 
                missing_fields=original_last_asked 
            )

        # --- PASSO 3: ATUALIZAÇÃO DO ESTADO (request_data) ---
        data_to_update = filtered_data if (original_last_context and original_last_asked) else extracted_data
        logger.debug(f">>> Atualizando estado com dados {'filtrados' if (original_last_context and original_last_asked) else 'extraídos'}.")
        if metadata: # Adiciona metadados ANTES de atualizar com dados da extração, para que possam ser sobrescritos se necessário
            logger.debug(f"Processando com metadata: {metadata}")
            if 'vendedor_id' in metadata:
                current_req_data_for_meta = self.state.get("request_data", {})
                if not current_req_data_for_meta.get("Vendedor"): # Só define se não existir
                    current_req_data_for_meta["Vendedor"] = metadata['vendedor_id']
                    # Não chama _update_request_data aqui, pois data_to_update será usado em seguida
                    self.state["request_data"] = current_req_data_for_meta 
                    logger.debug(f"Set Vendedor from metadata: {metadata['vendedor_id']}")
        
        self._update_request_data(data_to_update)


        # --- PASSO 4 & 5: MAPEAMENTO E VALIDAÇÃO PÓS-MAPEAMENTO ---
        logger.info(">>> Iniciando mapeamento e validação final.")
        # _run_mapping_and_validation usará self.state["current_original_input_text"]
        return self._run_mapping_and_validation() # Não precisa mais passar user_text

--- Conteúdo de src/prompts/extraction_prompt.txt ---

Você é um agente especializado em extrair informações de pedidos de clientes a partir de textos desestruturados.
Sua tarefa é identificar e extrair os seguintes campos do texto fornecido e retornar um JSON válido.

{context_instruction}

Campos a extrair:
- Cliente: O nome do cliente como aparece no texto.
- CNPJ/CPF: O número do Cadastro Nacional da Pessoa Jurídica ou Cadastro de Pessoa Física. Remova qualquer pontuação (pontos, traços, barras).
- Código do cliente: O código numérico do cliente, se fornecido explicitamente (ex: 111062, Cód. cliente: 98765). Extraia apenas o número.
- Planta: O código ou nome da unidade/local de produção/entrega (ex: PDL).
- Condição de Pagamento: O termo ou código referente ao prazo/modo de pagamento (ex: 15 dias, Z015).
- Forma de Pagamento: O método de pagamento (ex: boleto, D, Antecipação).
- Código do Material: O nome ou código do produto (ex: FS Ouro, 300002). Extraia o nome se fornecido textualmente.
- Quantidade Total: A quantidade total de produto mencionada separadamente da cadência detalhada (ex: 300 TONS, Quantidade Total: 50). Extraia o valor numérico e a unidade se presentes (ex: "50 TONS", "2100").
- Cadência: A descrição da programação de entrega, incluindo quantidades, meses e anos se disponíveis (ex: 40 fev 20 mar 58 abr, MAR/25).
     - **IMPORTANTE:** Se a cadência for detalhada em múltiplas linhas, como:
        FEV/25 100
        Mar/25 100
        abr/25 100
       Preserve EXATAMENTE este formato multi-linha na string extraída, incluindo os valores corretos (ex: 100) de cada linha. Ignore o campo "Quantidade Total" ao extrair estas linhas detalhadas.
    - Se o formato for diferente (ex: 40 fev 20 mar 58 abr), extraia como está, nesse exemplo seriam 3 meses para o mesmo ano de negociação.
    - Outra forma que a cadência pode aparecer é:
         30 tons 04/25
         30 tons 05/25
      Note que o valor das toneladas é 30 e temos dois meses de negociação para 2025
    - Se apenas o mês/ano for fornecido (ex: MAR/25), extraia apenas isso. A "Quantidade Total" será usada posteriormente se disponível. NÃO tente combinar aqui.
- Vendedor: O nome ou identificador da pessoa responsável pela venda, conforme fornecido no texto. Exemplos: "Vendedor: Herculano Franco", "Vendedor: 999998888", "Contato: Maria". Extraia o texto que identifica o vendedor. Se não houver menção a um vendedor no texto, retorne null.
- Cidade: A cidade de destino ou do cliente.
- Data de Negociação: A data em que o acordo foi feito (ex: 10/03/2025).
- Incoterms: O termo de comércio internacional (ex: CIF, FOB, TPD). **Se o Incoterm for mencionado junto com o valor do frete (ex: "Frete: FOB R$ 0,00" ou "CIF 150"), extraia o Incoterm (FOB, CIF, TPD etc.) para este campo E o valor numérico para o campo 'Preço Frete'. Se o Incoterm for FOB ou TPD e nenhum valor de frete for explicitamente mencionado, o 'Preço Frete' deve ser `null`.**
- Preço Frete: O valor numérico **explicitamente mencionado como frete**. Extraia apenas o número (remova R$, etc.). Ex: "CIF 350,00" -> extraia 350,00. "Frete: 55" -> extraia 55. **NÃO** use o valor do campo 'Valor' principal para preencher 'Preço Frete', mesmo que Incoterms seja CIF, a menos que o texto explicitamente o associe (ex: "CIF com frete de 170"). Se nenhum valor de frete for mencionado explicitamente, ou se o Incoterm for FOB ou TPD sem menção explícita de valor de frete, retorne `null`.
- Valor: O valor total ou preço principal do produto/negociação. Remova R$, etc. **Este é geralmente o valor principal do pedido, distinto do frete.**
- Campanha: O nome ou código da campanha de vendas, se fornecido explicitamente (ex: Campanha Dia das Mães, CAMP2025). Se não mencionado, retorne `null`.
- Email do vendedor: O endereço de e-mail do vendedor, se fornecido explicitamente no texto, pode aparecer apenas como email na entrada.

Regras Importantes:
- Tente separar "Condição de Pagamento" e "Forma de Pagamento" mesmo que apareçam juntos no texto (ex: "Boleto 15 dias", "TED a vista").
- Se um campo não for encontrado no texto, retorne o campo com valor `null`.
- Vendedor pode ser um nome, número de telefone ou qualquer valor de placeholder, caso seja explicitamente mencionado no texto
- Exemplos de como extrair formas e condições juntas:
- Input: "Pagamento: Boleto 30 dias" -> Extração: `"Forma de Pagamento": "Boleto", "Condição de Pagamento": "30 dias"`
- Input: "TED a vista" -> Extração: `"Forma de Pagamento": "TED", "Condição de Pagamento": "a vista"`
- Input: "Prazo 60 dias. Forma: Pix" -> Extração: `"Forma de Pagamento": "Pix", "Condição de Pagamento": "60 dias"`
- Para CNPJ/CPF, retorne APENAS os números. Ex: "040.074.561-51" deve ser retornado como "04007456151".
- Na condição de pagamento, é possível aparecer "a vista" ou "A vista" ou "Avista" que correspondem a "À vista" na planilha.
- O material (produto) possui alguns tipos que possuem acento, como úmido, fique atento para fazer a correspondência com umido caso essa seja a entrada.
- Para CNPJ/CPF, retorne APENAS os números. Ex: "040.074.561-51" deve ser retornado como "04007456151".
- Na condição de pagamento, é possível aparecer "a vista" ou "A vista" que correspondem a "À vista" na planilha.
- O material (produto) possui alguns tipos que possuem acento, como úmido, fique atento para fazer a correspondência com umido caso essa seja a entrada.
- Tente separar "Condição de Pagamento" e "Forma de Pagamento" mesmo que apareçam juntos no texto (ex: "Boleto 15 dias").
- **CRÍTICO:** O campo 'Preço Frete' só deve ser preenchido se o texto mencionar explicitamente um valor para o frete (usando palavras como "frete", "preço frete", ou associado diretamente ao Incoterms como em "CIF 170"). Não assuma que o 'Valor' principal do pedido é o 'Preço Frete'. Se não houver menção explícita ao valor do frete, ou se o Incoterm for FOB ou TPD, retorne `null` para 'Preço Frete' (a menos que um valor de frete seja *explicitamente* associado a FOB/TPD, o que é incomum mas possível).
- Retorne estritamente um objeto JSON e nada mais.
- **Cadência:** Preste muita atenção ao formato. Preserve múltiplas linhas e os valores corretos quando o formato for `Mes/Ano Valor`.
- Não inferir nenhum campo que não tenha sido enviado
- SEMPRE baseie sua resposta APENAS no 'Input do Usuário' fornecido abaixo. NUNCA use informações de interações anteriores ou conhecimento geral para preencher campos que não estão presentes no input atual. Se um campo não for encontrado no 'Input do Usuário' atual, retorne null para esse campo, sem exceções.

Input do Usuário:
{input_text}

JSON Extraído:

--- Conteúdo de src/utils/formatting.py ---

# src/utils/formatting.py
import re
from datetime import datetime
from unidecode import unidecode
import logging

logger = logging.getLogger(__name__)

mes_map = {
    'jan': '01', 'janeiro': '01', 'fev': '02', 'fevereiro': '02',
    'mar': '03', 'marco': '03', 'março': '03', 'abr': '04', 'abril': '04',
    'mai': '05', 'maio': '05', 'jun': '06', 'junho': '06',
    'jul': '07', 'julho': '07', 'ago': '08', 'agosto': '08',
    'set': '09', 'setembro': '09', 'out': '10', 'outubro': '10',
    'nov': '11', 'novembro': '11', 'dez': '12', 'dezembro': '12'
}

def _clean_valor(valor_str: str) -> str | None:
    if not valor_str: return None
    # Remove R$, espaços extras, converte vírgula para ponto como decimal
    # e remove pontos de milhar.
    cleaned = re.sub(r'[R$\s]', '', valor_str).strip()
    
    # Se tem '.' e ',' , trata como BR (1.234,56 -> 1234.56) ou US (1,234.56 -> 1234.56)
    if '.' in cleaned and ',' in cleaned:
        if cleaned.rfind('.') > cleaned.rfind(','): # Formato US: 1,234.56
            cleaned = cleaned.replace(',', '')
        else: # Formato BR: 1.234,56
            cleaned = cleaned.replace('.', '').replace(',', '.')
    elif ',' in cleaned: # Só vírgula: 1234,56
        cleaned = cleaned.replace(',', '.')
    # Se tem múltiplos pontos e o último não é decimal, remove os anteriores (ex: 1.234.567)
    # Esta parte pode ser simplificada se o LLM já envia algo mais limpo.
    # Por ora, uma limpeza básica:
    if cleaned.count('.') > 1:
        parts = cleaned.split('.')
        if len(parts[-1]) < 3: # Heurística: se a última parte tem menos de 3 dígitos, é decimal
            cleaned = "".join(parts[:-1]) + "." + parts[-1]
        else: # Provavelmente separador de milhar
            cleaned = "".join(parts)
            
    try:
        float(cleaned) # Valida se é um float válido
        return cleaned
    except ValueError:
        logger.warning(f"Valor de cadência '{valor_str}' resultou em '{cleaned}' que não é um float válido.")
        return None

def _determine_year(ano_str: str | None, mes_num_int: int, current_year_ref: int, previous_month_num_ref: int) -> tuple[int, int, int]:
    """
    Determina o ano a ser usado para a cadência.
    Retorna (year_to_use, new_current_year, new_previous_month_num)
    """
    year_to_use = current_year_ref
    new_current_year = current_year_ref
    new_previous_month_num = previous_month_num_ref

    if ano_str:
        try:
            if len(ano_str) == 2: ano_int = int(f"20{ano_str}")
            elif len(ano_str) == 4: ano_int = int(ano_str)
            else: raise ValueError("Formato de ano inválido")

            if 2000 <= ano_int < 2100:
                year_to_use = ano_int
                # Se o ano explícito é diferente do current_year, reseta a contagem de mês
                if ano_int != new_current_year:
                    new_previous_month_num = 0 
                new_current_year = ano_int # Atualiza o ano base para próximas inferências SEM ano explícito
            else: # Ano explícito inválido, tenta inferir
                if mes_num_int < new_previous_month_num and new_previous_month_num != 0:
                    year_to_use = new_current_year + 1
                    new_current_year +=1 # O ano base para inferência avança
                    new_previous_month_num = 0 # Resetou o ano, reseta o mês anterior
        except ValueError: # Ano não é número ou formato errado, tenta inferir
            if mes_num_int < new_previous_month_num and new_previous_month_num != 0:
                year_to_use = new_current_year + 1
                new_current_year += 1
                new_previous_month_num = 0
    else: # Sem ano explícito, infere
        if mes_num_int < new_previous_month_num and new_previous_month_num != 0:
            year_to_use = new_current_year + 1
            new_current_year +=1
            new_previous_month_num = 0
            
    new_previous_month_num = mes_num_int # Atualiza o último mês processado com o mês atual
    return year_to_use, new_current_year, new_previous_month_num

def _parse_multi_item_line(line_text: str, ano_base: int, current_year_ref: int, previous_month_num_ref: int) -> tuple[list[dict], int, int]:
    """
    Tenta parsear uma linha que pode conter múltiplos pares de (valor, mês) ou (mês, valor).
    Ex: "40 fev 20 mar 58 abr"
    Retorna (lista_de_itens_cadencia, novo_ano_inferido, novo_mes_anterior)
    """
    items_found = []
    new_current_year = current_year_ref
    new_previous_month = previous_month_num_ref
    
    regex_vm_multi = re.compile(r"(\d[\d.,]*)\s*(?:t(?:ons?)?|toneladas)?\s*([a-zA-Zç]+)", re.IGNORECASE)
    
    last_match_end = 0
    matched_vm = False
    temp_items_vm = []

    for match in regex_vm_multi.finditer(line_text):
        matched_vm = True
        valor_str, mes_str = match.groups()
        
        mes_norm = unidecode(mes_str.strip().lower())
        mes_num_str = mes_map.get(mes_norm)
        valor_limpo = _clean_valor(valor_str)

        if not mes_num_str or not valor_limpo:
            logger.warning(f"[_parse_multi_item_line VM] Mês ou valor inválido: '{mes_str}', '{valor_str}'. Pulando item.")
            continue

        mes_num_int = int(mes_num_str)
        year_for_this_item, updated_year_ref, updated_prev_month = _determine_year(
            None, mes_num_int, new_current_year, new_previous_month
        )
        new_current_year = updated_year_ref
        new_previous_month = updated_prev_month
        
        temp_items_vm.append({
            "mes": mes_num_int,
            "ano": year_for_this_item,
            "texto": f"{mes_num_str}.{year_for_this_item}:{valor_limpo} ton"
        })
        last_match_end = match.end()

    if matched_vm and last_match_end > len(line_text) * 0.7: 
        logger.debug(f"[_parse_multi_item_line] Padrão 'Valor Mês (multi)' aplicado à linha. Itens: {len(temp_items_vm)}")
        return temp_items_vm, new_current_year, new_previous_month

    logger.debug(f"[_parse_multi_item_line] Nenhum padrão multi-item forte encontrado para: '{line_text}'")
    return [], current_year_ref, previous_month_num_ref


def format_cadencia(cadencia_str: str | None, qtd_total: str | None = None, data_negociacao: str | None = None) -> str | None:

    known_single_item_connectors = [
        " toneladas em ", " tonelada em ", " ton em ", " t em "," un em ",
        " toneladas para ", " tonelada para ", " ton para ", " t para ", " un para "
    ]

    raw_qtd_total_param = qtd_total # Preserva o parâmetro original se necessário para logs
    cleaned_qtd_total_for_extractors = _clean_valor(raw_qtd_total_param) if raw_qtd_total_param else None
    logger.debug(f"[format_cadencia] Qtd Total (parâmetro original): '{raw_qtd_total_param}', Limpo para extratores: '{cleaned_qtd_total_for_extractors}'")

    if not cadencia_str:
        return None
    
    cadencia_padronizada = cadencia_str.strip().replace(';', '\n')

    cadencia_padronizada = re.sub(r'\)\s+', ')\n', cadencia_padronizada)


    initial_lines = [line.strip() for line in cadencia_padronizada.split('\n') if line.strip()]

    processed_sub_lines = []

    item_cadencia_regex_text = (
        r"([a-zA-Zç]+|\d{1,2})"  # Mês (texto ou número)
        r"(?:\s*[/.\-]\s*(\d{2,4}))?"  # Ano opcional (ex: /25)
        r"\s+"  # Espaço obrigatório antes do valor
        r"(\d[\d.,]*)"  # Valor (número)
        r"(?:\s*(?:t|ton|tons?|toneladas))?"  # Unidade opcional (t, ton, etc.)
    )

    item_cadencia_regex_valor_com_fim = item_cadencia_regex_text + r"(?:\b|$)"
    
    _temp_pattern_mes_parenteses_valor = re.compile(
        r'([a-zA-Zç]+|\d{1,2})'                         # Mês (texto ou número)
        r'(?:\s*[/.\-]\s*(\d{2,4}))?'                  # Ano opcional (ex: /25)
        r'\s*\('                                        # Abre parêntese
        r'([\d.,]+)'                                   # Valor (número)
        r'\s*(?:t|ton|tons?|toneladas)?'                # Unidade opcional (t, ton, etc.)
        r'\s*\)',                                       # Fecha parêntese
        re.IGNORECASE
    )


    for line in initial_lines:
        # Tenta dividir linhas como "maio (300 t), junho (300 t) e julho (300 t)"
        # Verifica se a linha parece conter múltiplos itens "mes (valor t)" separados por vírgula ou "e"
        # Heurística: presença de parênteses, "t)", e (vírgula ou " e ")
        if '(' in line and 't)' in line.lower() and (',' in line or ' e ' in line.lower()):
            logger.debug(f"[format_cadencia PreProc] Linha '{line}' candidata a split por vírgula/e para formato 'mes (valor t)'.")
            
            # Substitui " e " por "," para ter um delimitador único (vírgula)
            # Cuidado para não substituir "e" dentro de nomes de meses como "setembro"
            # Usar \s+e\s+ é mais seguro.
            temp_line = re.sub(r'\s+e\s+', ',', line, flags=re.IGNORECASE)
            
            parts = [p.strip() for p in temp_line.split(',') if p.strip()]
            
            all_parts_valid = True
            if not parts: # Se o split não resultou em nada
                all_parts_valid = False

            for part in parts:
                # Verifica se cada parte individual corresponde ao padrão "mes (valor t)"
                if not _temp_pattern_mes_parenteses_valor.fullmatch(part): # Usa fullmatch para a parte individual
                    all_parts_valid = False
                    logger.debug(f"[format_cadencia PreProc] Parte '{part}' da linha '{line}' não validou como 'mes (valor t)' isolado. Não aplicando split especial.")
                    break
            
            if all_parts_valid:
                logger.info(f"[format_cadencia PreProc] Linha '{line}' dividida em sub-itens: {parts}")
                processed_sub_lines.extend(parts)
                continue # Pula para a próxima linha original
        
        # Lógica de split por ". " existente (se necessário, ajuste a condição)
        if re.search(r'\s+E\s+', line, flags=re.IGNORECASE) and not _temp_pattern_mes_parenteses_valor.search(line):
            logger.debug(f"[format_cadencia PreProc] Linha '{line}' candidata a split por ' E '.")
            
            # Tenta dividir por " E " (case insensitive)
            # O re.split com grupo de captura ( ) mantém o delimitador, mas não queremos o "E"
            # Então usamos um split mais simples e validamos as partes.
            potential_parts = re.split(r'\s+E\s+', line, flags=re.IGNORECASE)
            parts = [p.strip() for p in potential_parts if p.strip()]

            all_parts_valid_for_E_split = True if parts else False
            if len(parts) <= 1: # Se não dividiu em pelo menos 2 partes, não é o caso "item E item"
                all_parts_valid_for_E_split = False
                
            for part_idx, part_str in enumerate(parts):
                # Valida cada parte com um regex que busca "MÊS/ANO VALOR" ou "MÊS VALOR"
                # Usar re.fullmatch para garantir que a parte inteira é um item de cadência
                if not re.fullmatch(item_cadencia_regex_valor_com_fim, part_str, flags=re.IGNORECASE):
                    all_parts_valid_for_E_split = False
                    logger.debug(f"[format_cadencia PreProc] Parte '{part_str}' (de split por 'E' da linha '{line}') não validou como item de cadência. Não aplicando split por 'E'.")
                    break
            
            if all_parts_valid_for_E_split:
                logger.info(f"[format_cadencia PreProc] Linha '{line}' dividida por ' E ' em sub-itens: {parts}")
                processed_sub_lines.extend(parts)
                continue # Pula para a próxima linha original

        # Se nenhum split especial foi aplicado, adiciona a linha como está
        processed_sub_lines.append(line)

    input_lines = [l for l in processed_sub_lines if l]
    if not input_lines: # Se o pré-processamento resultou em nenhuma linha válida
        logger.warning(f"[format_cadencia] Após pré-processamento, nenhuma linha de cadência válida encontrada para: '{cadencia_str}'")
        return None

    ano_negociacao_base = datetime.now().year
    logger.debug(f"[format_cadencia] Ano padrão inicializado como: {ano_negociacao_base}")
    if data_negociacao:
        try:
            partes_data = [p.strip() for p in data_negociacao.split('/') if p.strip()]
            if len(partes_data) == 3:
                ano_parte = partes_data[2]
                ano_neg = None
                if len(ano_parte) == 4: ano_neg = int(ano_parte)
                elif len(ano_parte) == 2: ano_neg = int(f"20{ano_parte}")
                if ano_neg is not None and 2000 <= ano_neg < 2100:
                    ano_negociacao_base = ano_neg
            elif len(partes_data) == 2: # Pode ser DD/MM ou MM/YY
                try:
                    p0_val = int(partes_data[0])
                    p1_val = int(partes_data[1])

                    # Heurística: se p0 é dia (1-31) e p1 é mês (1-12), é DD/MM
                    # Neste caso, não alteramos o ano_negociacao_base (ele permanece o ano atual)
                    if (1 <= p0_val <= 31) and (1 <= p1_val <= 12):
                        logger.debug(f"[format_cadencia] Data de negociação '{data_negociacao}' interpretada como DD/MM. Ano base ({ano_negociacao_base}) não será alterado por esta parte.")
                    else:
                        # Caso contrário, pode ser MM/YY ou algo inválido que tentaremos tratar como MM/YY
                        # Aqui, partes_data[1] é o candidato a ano.
                        ano_neg_str = partes_data[1] # Usamos o segundo elemento como potencial ano
                        ano_neg = None
                        if len(ano_neg_str) == 4: ano_neg = int(ano_neg_str)
                        elif len(ano_neg_str) == 2: ano_neg = int(f"20{ano_neg_str}")

                        if ano_neg is not None and 2000 <= ano_neg < 2100:
                            ano_negociacao_base = ano_neg
                            logger.debug(f"[format_cadencia] Data de negociação '{data_negociacao}' interpretada como MM/YY. Ano base definido para {ano_negociacao_base}.")
                        # else: não foi possível determinar um ano válido a partir de MM/YY, mantém o ano_negociacao_base atual.
                except ValueError:
                    # Se as partes não forem inteiros, não é um formato DD/MM ou MM/YY numérico simples.
                    logger.debug(f"[format_cadencia] Partes de '{data_negociacao}' não são puramente numéricas para DD/MM ou MM/YY. Ano base ({ano_negociacao_base}) não alterado.")
            elif len(partes_data) == 1 and partes_data[0].lower() not in mes_map and partes_data[0].count(' ') > 0 :
                 match_ano_texto = re.search(r'(\d{4})', partes_data[0]) # Tenta achar 4 digitos de ano
                 if match_ano_texto:
                     ano_neg = int(match_ano_texto.group(1))
                     if 2000 <= ano_neg < 2100:
                         ano_negociacao_base = ano_neg
        except (ValueError, IndexError, TypeError) as e:
             logger.warning(f"[format_cadencia] Erro ao processar data_negociacao '{data_negociacao}': {e}. Usando ano atual: {ano_negociacao_base}")
    logger.debug(f"[format_cadencia] Ano base da negociação definido como {ano_negociacao_base}")

    output_cadencia_items = []
    current_inferred_year = ano_negociacao_base
    last_processed_month_num = 0

    pattern1_regex = re.compile(r'([a-zA-Zç]+)\s*[/.\-]\s*(\d{2,4})\s*-\s*(\d[\d.,]*)\s*(?:t|ton|tons?|toneladas)\b', re.IGNORECASE)
    def extractor1(match): return match.group(1), match.group(2), match.group(3)

    pattern2_regex = re.compile(
        r"^\s*(\d[\d.,]*)"                                
        r"\s*(?:t|ton|tons?|toneladas)?\b"                
        r"\s+"                                            
        r"([a-zA-Zç]+|\d{1,2})"                           
        r"(?:\s*[/.\-]\s*(\d{2,4}))?",                    
        re.IGNORECASE
    )
    def extractor2(match):
        return match.group(2), match.group(3), match.group(1)


    pattern3_regex = re.compile(
        r'^\s*([a-zA-Zç]+|\d{1,2})'                         
        r'(?:\s*[/.\-]\s*(\d{2,4}))?'                  
        r'\s+'                                          
        r'(\d[\d.,]*)'                                 
        r'\s*(?:t|ton|tons?|toneladas)?\b',             
        re.IGNORECASE
    )
    def extractor3(match):
        return match.group(1), match.group(2), match.group(3)

    pattern_mes_ano_opcional_parenteses_valor_regex = _temp_pattern_mes_parenteses_valor

    def extractor_mes_ano_opcional_parenteses_valor(match):
        return match.group(1), match.group(2), match.group(3)

    pattern_mes_ano_colon_valor_regex = re.compile(
        r'^\s*'                                         
        r'([a-zA-Zç]+|\d{1,2})'                         
        r'\s*[/.\-]\s*'                                 
        r'(\d{2,4})'                                   
        r'\s*:\s*'                                      
        r'(\d[\d.,]*)'                                 
        r'\s*(?:t|ton|tons?|toneladas)?'                
        r'\s*$',                                        
        re.IGNORECASE
    )
    def extractor_mes_ano_colon_valor(match):
        return match.group(1), match.group(2), match.group(3)
    
    pattern_mes_de_ano_qtotal_regex = re.compile(
        r'^\s*([a-zA-Zç]+|\d{1,2})'      # Grupo 1: Mês
        r'\s+de\s+'                     # " de "
        r'(\d{2,4})\s*$',               # Grupo 2: Ano
        re.IGNORECASE
    )

    def extractor_mes_de_ano_qtotal(match):
        # cleaned_qtd_total_for_extractors é acessível aqui por closure
        if not cleaned_qtd_total_for_extractors:
            logger.warning(f"[ExtractorMesDeAnoQtotal] Quantidade total limpa não disponível (original: '{raw_qtd_total_param}').")
            return None
        # Retorna (mês, ano, valor_da_qtd_total_limpa)
        return match.group(1), match.group(2), cleaned_qtd_total_for_extractors

    pattern6_regex = re.compile(r'^\s*([a-zA-Zç]+|\d{1,2})\s*[/.\-]\s*(\d{2,4})\s*$', re.IGNORECASE)
    def extractor6(match):
        if not cleaned_qtd_total_for_extractors:
            logger.warning(f"[Extractor6] Quantidade total limpa não disponível (original: '{raw_qtd_total_param}').")
            return None
        # Retorna (mês, ano, valor_da_qtd_total_limpa)
        return match.group(1), match.group(2), cleaned_qtd_total_for_extractors
    
    pattern_valor_mes_ano_flex_regex = re.compile(
        r"^\s*"                                        
        r"(?P<valor>\d[\d.,]*)"                        
        r"\s*(?:t|ton|tons?|toneladas|kg|un)\b"        
        r"(?:\s+(?:em|para|no|na|para o|para a))?"    
        r"\s+"                                         
        r"(?P<mes>[a-zA-Zç]+|\d{1,2})"                 
        r"(?:\s+(?:de|do))?"                           
        r"\s+"                                         
        r"(?P<ano>\d{2,4})"                            
        r"\s*$",                                       
        re.IGNORECASE
    )

    def extractor_valor_mes_ano_flex(match):
        valor_str = match.group("valor")
        mes_input_original = match.group("mes")
        ano_str_capturado = match.group("ano")
        return mes_input_original, ano_str_capturado, valor_str
    

    pattern_mes_de_ano_valor_regex = re.compile(
        r"^\s*"                                        
        r"(?P<mes>[a-zA-Zç]+)"                         
        r"(?:\s+(?:de|do)\s+(?P<ano_de>\d{2,4}))?"     
        r"(?:\s+(?P<ano_direto>\d{2,4}))?"             
        r"(?:(?:\s*,\s*)|\s+)"                         
        r"(?P<valor>\d[\d.,]*)"                        
        r"\s*(?:t|ton|tons?|toneladas|kg|un)?\b"       
        r"\s*$",                                       
        re.IGNORECASE
    )

    def extractor_mes_de_ano_valor(match):
        mes_input = match.group("mes")
        ano_capturado = match.group("ano_de") if match.group("ano_de") else match.group("ano_direto")
        valor_str = match.group("valor")
        return mes_input, ano_capturado, valor_str
    
    pattern_valor_unidade_em_mes_ano_opcional_regex = re.compile(
        r"^\s*"                                       # Início da string com espaços opcionais
        r"(?P<valor>\d[\d.,]*)"                       # Grupo 'valor': números, pontos, vírgulas
        r"\s*(?:t|ton|tons?|tonelada|toneladas|kg|un)\b" # Unidade (t, ton, etc.) com word boundary
        r"\s+(?:em|para)\s+"                          # Obrigatório "em" ou "para" cercado por espaços
        r"(?P<mes>[a-zA-Zç]+|\d{1,2})"                # Grupo 'mes': nome do mês ou número
        r"(?:\s*[/.\-]?\s*(?P<ano>\d{2,4}))?"         # Grupo 'ano' opcional, precedido por / . - ou espaço opcional
        r"\s*$",                                      # Fim da string com espaços opcionais
        re.IGNORECASE
    )

    def extractor_valor_unidade_em_mes_ano_opcional(match):
        valor_str = match.group("valor")
        mes_input_original = match.group("mes")
        ano_str_capturado = match.group("ano") # Pode ser None
        return mes_input_original, ano_str_capturado, valor_str

    single_line_patterns = [
        ("Valor Unidade EM Mês [Ano Opcional]", pattern_valor_unidade_em_mes_ano_opcional_regex, extractor_valor_unidade_em_mes_ano_opcional),
        ("Mês/Ano - Valor T", pattern1_regex, extractor1),
        ("Mês de Ano, Valor T", pattern_mes_de_ano_valor_regex, extractor_mes_de_ano_valor),
        ("Valor T Mês[/Ano]", pattern2_regex, extractor2),
        ("Mês[/Ano] (Valor T)", pattern_mes_ano_opcional_parenteses_valor_regex, extractor_mes_ano_opcional_parenteses_valor),
        ("Mês/Ano: Valor T", pattern_mes_ano_colon_valor_regex, extractor_mes_ano_colon_valor),
        ("Mês[/Ano] Valor T", pattern3_regex, extractor3), 
        ("Mês de Ano (usa qtd_total)", pattern_mes_de_ano_qtotal_regex, extractor_mes_de_ano_qtotal),
        ("Mês/Ano (usa qtd_total)", pattern6_regex, extractor6),
        ("Valor Unidade Mês Ano (Flexível)", pattern_valor_mes_ano_flex_regex, extractor_valor_mes_ano_flex),
    ]
    
    for line_text in input_lines:
        logger.debug(f"[format_cadencia] Processando linha (pós-pré-processamento): '{line_text}'")

        if line_text.upper() in ["CADÊNCIA", "CADENCIA", "CADÊNCIA LRV", "CADÊNCIA PDL", "CADENCIA SRS"]:
            logger.info(f"[format_cadencia] Ignorando linha de cabeçalho: '{line_text}'")
            continue

        matched_line = False
        contains_explicit_year_pattern = bool(re.search(r'[/.\-]\s*\d{2,4}\b', line_text))
        
        # Modificada a condição para is_simple_multi_item_candidate
        contains_known_single_item_connector = any(connector in line_text.lower() for connector in known_single_item_connectors)

        is_simple_multi_item_candidate = (
            not contains_explicit_year_pattern and
            not contains_known_single_item_connector and # <-- NOVA CONDIÇÃO AQUI
            " de " not in line_text.lower() and
            ("," not in line_text or (line_text.lower().rfind(',') < line_text.lower().rfind("toneladas") if "toneladas" in line_text.lower() else True)) and
            ("," not in line_text or (line_text.lower().rfind(',') < line_text.lower().rfind("ton") if "ton " in line_text.lower() else True)) and
            line_text.count(" ") > 1 # Garante que há pelo menos duas "palavras" (ex: "valor mes")
        )

        if is_simple_multi_item_candidate:
            logger.debug(f"[format_cadencia] Linha '{line_text}' candidata para _parse_multi_item_line.")
            multi_items, updated_year, updated_month = _parse_multi_item_line(
                line_text, ano_negociacao_base, current_inferred_year, last_processed_month_num
            )
            if multi_items:
                output_cadencia_items.extend(multi_items)
                current_inferred_year = updated_year
                last_processed_month_num = updated_month
                logger.info(f"[format_cadencia] Linha '{line_text}' processada por _parse_multi_item_line. Itens: {len(multi_items)}")
                matched_line = True
        
        if not matched_line: 
            for pattern_name, regex, extractor_fn in single_line_patterns:
                # IMPORTANTE: Usar regex.fullmatch() aqui porque cada line_text agora deve ser um item de cadência completo.
                # Se usar search(), pode pegar partes de uma linha maior que não foi corretamente dividida.
                # No entanto, o log original mostra que `search` foi usado e funcionou para o PRIMEIRO item.
                # Se `line_text` agora é "maio (300 t)", `fullmatch` seria mais apropriado.
                # Vamos manter `search` por enquanto, pois é o que está no seu log, mas `fullmatch` é algo a se considerar
                # se `input_lines` contiver exatamente um item de cadência por string.
                match = regex.search(line_text) # Ou regex.fullmatch(line_text) se apropriado
                
                if match:
                    if pattern_name == "Mês/Ano (usa qtd_total)":
                        if line_text[:match.start()].strip():
                           logger.debug(f"[format_cadencia] Padrão '{pattern_name}' encontrou '{match.group(0)}' mas há prefixo não-espaço: '{line_text[:match.start()]}'. Pulando.")
                           continue 
                    
                    logger.debug(f"[format_cadencia] Linha '{line_text}' teve correspondência (search) com padrão: {pattern_name}, match: '{match.group(0)}'")
                    
                    extracted_info = extractor_fn(match)
                    if not extracted_info:
                        logger.debug(f"[DEBUG EXTRACTED_INFO] Padrão: {pattern_name}, Linha: '{line_text}', Match Group0: '{match.group(0)}', Extracted Info: {extracted_info}")
                        logger.debug(f"[format_cadencia] Extrator para '{pattern_name}' retornou None. Tentando próximo padrão.")
                        continue

                    mes_input_original, ano_str_capturado, valor_str = extracted_info
                    
                    mes_numero_canonico_str = None
                    if isinstance(mes_input_original, str) and mes_input_original.isdigit():
                        num_mes = int(mes_input_original)
                        if 1 <= num_mes <= 12:
                            mes_numero_canonico_str = str(num_mes).zfill(2)
                            logger.debug(f"[format_cadencia SL] Mês numérico '{mes_input_original}' normalizado para '{mes_numero_canonico_str}'.")
                        else:
                            logger.warning(f"[format_cadencia SL] Mês numérico '{mes_input_original}' fora do intervalo 1-12 na linha '{line_text}'. Pulando item.")
                            continue 
                    elif isinstance(mes_input_original, str): 
                        nome_mes_norm = unidecode(mes_input_original.strip().lower())
                        mes_numero_canonico_str = mes_map.get(nome_mes_norm)
                        if mes_numero_canonico_str:
                            logger.debug(f"[format_cadencia SL] Nome de mês '{mes_input_original}' (norm: {nome_mes_norm}) mapeado para '{mes_numero_canonico_str}'.")
                        else:
                            logger.warning(f"[format_cadencia SL] Nome de mês '{mes_input_original}' (norm: {nome_mes_norm}) não encontrado no mes_map na linha '{line_text}'. Pulando item.")
                            continue
                    else:
                        logger.warning(f"[format_cadencia SL] Tipo de mês inesperado: {type(mes_input_original)} com valor '{mes_input_original}' na linha '{line_text}'. Pulando item.")
                        continue 

                    valor_limpo = _clean_valor(valor_str)
                    if not valor_limpo:
                        logger.warning(f"[format_cadencia SL] Valor '{valor_str}' inválido na linha '{line_text}'. Pulando este match.")
                        continue
                    
                    mes_num_int = int(mes_numero_canonico_str)

                    year_for_this_entry, next_inferred_year, next_last_month = _determine_year(
                        ano_str_capturado, 
                        mes_num_int,
                        current_inferred_year, 
                        last_processed_month_num
                    )
                    current_inferred_year = next_inferred_year
                    last_processed_month_num = next_last_month
                    
                    output_cadencia_items.append({
                        "mes": mes_num_int,
                        "ano": year_for_this_entry,
                        "texto": f"{mes_numero_canonico_str}.{year_for_this_entry}:{valor_limpo} ton"
                    })
                    matched_line = True
                    logger.info(f"[format_cadencia SL] Linha processada: {output_cadencia_items[-1]['texto']} (Padrão: {pattern_name})")
                    break 
        
        if not matched_line:
            logger.warning(f"[format_cadencia] Nenhum padrão de cadência RELEVANTE encontrado para a linha: '{line_text}'")

    if not output_cadencia_items:
        logger.warning(f"[format_cadencia] Nenhuma linha de cadência pôde ser processada para o input: '{cadencia_str}'")
        return None

    output_cadencia_items.sort(key=lambda x: (x["ano"], x["mes"]))
    return "\n".join([item["texto"] for item in output_cadencia_items])


# --- O restante do arquivo (format_output_python, format_final_summary_text) ---
def format_output_python(mapped_data, cadencia_formatada):
    incoterm_val = mapped_data.get('Incoterms')
    frete_val = mapped_data.get('Preço Frete', None)
    display_frete_info = "N/A"

    if frete_val is not None and str(frete_val).strip():
        frete_str = str(frete_val)
        display_frete_info = frete_str
        if incoterm_val:
            incoterm_upper = incoterm_val.upper()
            if incoterm_upper == 'CIF': display_frete_info += " (CIF)"
            elif incoterm_upper == 'FOB': display_frete_info += " (FOB - Valor Informativo)"
            elif incoterm_upper == 'TPD': display_frete_info += " (TPD - Valor Informativo)" # MODIFICADO
            else: display_frete_info += f" (Incoterms: {incoterm_val})"
        else: display_frete_info += " (Incoterms não especificado)"
    elif incoterm_val:
        incoterm_upper = incoterm_val.upper()
        if incoterm_upper == 'CIF': display_frete_info = "N/A (CIF - Valor não informado)"
        elif incoterm_upper == 'FOB': display_frete_info = "N/A (FOB)"
        elif incoterm_upper == 'TPD': display_frete_info = "N/A (TPD)" # MODIFICADO
        else: display_frete_info = f"N/A (Incoterms: {incoterm_val})"

    cadencia_html = cadencia_formatada.replace('\n', '<br>') if cadencia_formatada is not None else 'N/A (Formato Inválido)'

    output = f"""
Data da solicitação: {datetime.now().strftime("%d/%m/%Y")}<br>
Vendedor: {mapped_data.get('Vendedor', 'N/A')}<br>
CNPJ/CPF: {mapped_data.get('CNPJ/CPF', 'N/A')}<br>
Cidade: {mapped_data.get('Cidade', 'N/A')}<br>
Email do vendedor: {mapped_data.get('Email do vendedor', 'N/A')}<br>
Planta: {mapped_data.get('Planta', 'N/A')}<br>
Nome do cliente: {mapped_data.get('Nome do cliente', 'N/A')}<br>
Código do cliente: {mapped_data.get('Código do cliente', 'N/A')}<br>
Campanha: {mapped_data.get('Campanha') or 'SEM REF'}<br>
Data da negociação: {mapped_data.get('Data de Negociação', 'N/A')}<br>
Condição de pagamento: {mapped_data.get('Condição de Pagamento', 'N/A')}<br>
Forma de pagamento: {mapped_data.get('Forma de Pagamento', 'N/A')}<br>
Incoterms: {mapped_data.get('Incoterms', 'N/A')}<br>
Preço frete: {display_frete_info}<br>
Preço: {mapped_data.get('Valor', 'N/A')}<br>
Código do material: {mapped_data.get('Código do Material', 'N/A')}<br>
-- Cadência Formatada --<br>
{cadencia_html}
"""
    return output.strip()

def format_final_summary_text(mapped_data, cadencia_formatada):
    incoterm_val = mapped_data.get('Incoterms')
    frete_val = mapped_data.get('Preço Frete', None)
    display_frete_info = "N/A"

    if frete_val is not None and str(frete_val).strip() and float(frete_val) != 0: # Se frete for 0.0, tratamos como N/A para FOB/TPD
        try: frete_str = f"{float(frete_val):.2f}".replace('.', ',') if isinstance(frete_val, (int, float)) else str(frete_val)
        except ValueError: frete_str = str(frete_val)
        display_frete_info = frete_str
        if incoterm_val:
            incoterm_upper = incoterm_val.upper()
            if incoterm_upper == 'CIF': display_frete_info += " (CIF)"
            elif incoterm_upper == 'FOB': display_frete_info += " (FOB - Valor Informativo)"
            elif incoterm_upper == 'TPD': display_frete_info += " (TPD - Valor Informativo)" # MODIFICADO
            else: display_frete_info += f" (Incoterms: {incoterm_val})"
        else: display_frete_info += " (Incoterms não especificado)"
    elif incoterm_val: # Se frete_val é None, vazio, ou 0.0
        incoterm_upper = incoterm_val.upper()
        if incoterm_upper == 'CIF':
            display_frete_info = "N/A (CIF - Valor não informado)"
        elif incoterm_upper == 'FOB':
            display_frete_info = "N/A (FOB)"
        elif incoterm_upper == 'TPD':
            display_frete_info = "N/A (TPD)" # MODIFICADO
        else:
            display_frete_info = f"N/A (Incoterms: {incoterm_val})"
    # Se incoterm_val também for None, display_frete_info permanece "N/A" (já inicializado)

    cadencia_plain = cadencia_formatada if cadencia_formatada is not None else 'N/A (Formato Inválido ou Não Reconhecido)'

    valor_str = "N/A"
    valor_val = mapped_data.get('Valor')
    if valor_val is not None:
        try: valor_str = f"{float(valor_val):.2f}".replace('.', ',') if isinstance(valor_val, (int, float)) else str(valor_val)
        except ValueError: valor_str = str(valor_val)

    output = f"""Data da solicitação: {datetime.now().strftime("%d/%m/%Y")}
Vendedor: {mapped_data.get('Vendedor', 'N/A')}
CNPJ/CPF: {mapped_data.get('CNPJ/CPF', 'N/A')}
Cidade: {mapped_data.get('Cidade', 'N/A')}
Email do vendedor: {mapped_data.get('Email do vendedor', 'N/A')}
Planta: {mapped_data.get('Planta', 'N/A')}
Nome do cliente: {mapped_data.get('Nome do cliente', '<não encontrado>')}
Código do cliente: {mapped_data.get('Código do cliente', 'N/A')}
Campanha: {mapped_data.get('Campanha') or 'SEM REF'}
Data da negociação: {mapped_data.get('Data de Negociação', 'N/A')}
Condição de pagamento: {mapped_data.get('Condição de Pagamento', 'N/A')}
Forma de pagamento: {mapped_data.get('Forma de Pagamento', 'N/A')}
Incoterms: {mapped_data.get('Incoterms', 'N/A')}
Preço frete: {display_frete_info}
Preço: {valor_str}
Código do material: {mapped_data.get('Código do Material', 'N/A')}
-- Cadência --
{cadencia_plain}"""

    output_lines = [line.strip() for line in output.split('\n')]
    return "\n".join(output_lines).strip()

--- Conteúdo de src/utils/normalization.py ---

# src/utils/normalization.py
import re
from unidecode import unidecode
import logging # Adicionado para logar erros potenciais

logger = logging.getLogger(__name__)

def normalize_string(text: str | None, remove_hyphens: bool = False) -> str | None:
    if text is None:
        return None
    try:
        text_str = str(text)
        normalized = unidecode(text_str).lower()
        normalized = normalized.strip()
        normalized = re.sub(r'\s+', ' ', normalized)
        normalized = re.sub(r'(?<=[a-z])\.(?=[a-z])', ' ', normalized)

        if remove_hyphens:
            normalized = normalized.replace('-', ' ')
            normalized = re.sub(r'\s+', ' ', normalized).strip()

        # Adicionar tratamento específico para "avista" -> "a vista"
        if normalized == "avista":
            normalized = "a vista"

        return normalized
    except Exception as e:
        logger.error(f"Erro ao normalizar texto '{text}': {e}", exc_info=True)
        return str(text)

--- Conteúdo de src/app.py ---

import streamlit as st
import os
import sys
import time
import logging
from streamlit_mic_recorder import mic_recorder

logger = logging.getLogger(__name__) 
logging.basicConfig(
    level=logging.DEBUG,  # Mude para logging.INFO se quiser menos verbosidade
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    # handlers=[logging.StreamHandler(sys.stdout)] # Opcional: Forçar para stdout
)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Importações
try:
    from agents.extraction_agent import ExtractionAgent
    from agents.mapping_agent import MappingAgent
    from agents.orchestration_agent import OrchestrationAgent
    from utils.transcription import AudioTranscriber
    import config
except ImportError as e:
    st.error(f"Erro ao importar módulos: {e}.")
    st.stop()

# --- Cache ---
@st.cache_resource
def load_stateless_components():
    # ... (código existente para carregar extraction, mapping, transcriber) ...
    print("--- Carregando Agente de Extração (cacheado) ---")
    extraction_agent = None
    try:
        extraction_agent = ExtractionAgent()
    except Exception as e:
        st.error(f"Falha ao carregar Agente de Extração: {e}")

    print("--- Carregando Agente de Mapeamento (cacheado) ---")
    mapping_agent = None
    try:
        mapping_agent = MappingAgent(artifacts_dir=config.ARTIFACTS_DIR)
        if not mapping_agent.data_loaded_successfully:
             st.warning("Agente de Mapeamento inicializado, mas houve falha ao carregar os arquivos CSV.")
    except Exception as e:
        st.error(f"Falha ao carregar Agente de Mapeamento: {e}")

    print("--- Carregando Transcritor de Áudio (cacheado) ---")
    audio_transcriber = None
    try:
        if config.OPENAI_API_KEY:
            audio_transcriber = AudioTranscriber()
        else:
            st.warning("Chave da API OpenAI não configurada. Transcrição de áudio desabilitada.")
    except Exception as e:
        st.error(f"Falha ao carregar AudioTranscriber: {e}")

    return extraction_agent, mapping_agent, audio_transcriber

# --- Inicialização do App ---
st.set_page_config(page_title="Chatbot de Pedidos", layout="wide")
st.title("🤖 Chatbot de Processamento de Pedidos")
st.caption("Use este chat para inserir dados via texto, áudio ou upload.")

extraction_agent, mapping_agent, audio_transcriber = load_stateless_components()

# --- Gerenciamento de Estado ---
# ESSENCIAL: Inicializa as variáveis de estado PENDING
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Olá! Informe os dados do pedido."}]
if "orchestrator" not in st.session_state:
    if extraction_agent and mapping_agent:
        st.session_state.orchestrator = OrchestrationAgent(extraction_agent, mapping_agent)
    else:
        st.session_state.orchestrator = None
if 'run_id' not in st.session_state:
    st.session_state.run_id = 0
# Novas variáveis para desacoplar widget de processamento
if 'pending_upload_data' not in st.session_state:
    st.session_state.pending_upload_data = None
if 'pending_mic_data' not in st.session_state:
    st.session_state.pending_mic_data = None
if 'pending_text_input' not in st.session_state:
    st.session_state.pending_text_input = None
# Flag para indicar se o processamento ocorreu nesta execução
if 'input_processed_flag' not in st.session_state:
    st.session_state.input_processed_flag = False


# --- Exibição do Histórico ---
# (É importante exibir o histórico antes dos widgets de input)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Sidebar ---
with st.sidebar:
    st.header("Instruções")
    # ... (markdown das instruções) ...
    st.markdown("""
        1.  **Insira os Dados:**
            *   Digite na caixa abaixo.
            *   OU clique no microfone (🎤).
            *   OU use o Uploader abaixo para enviar um arquivo de áudio (`wav`, `mp3`, etc.).
        2.  **Responda:** Se necessário, responda ao bot por texto, áudio ou upload.
        3.  **Confirme:** Revise o resumo final e responda 'Sim' ou 'Não'.
        4.  **Novo Pedido:** Use o botão abaixo para limpar o histórico.
    """)
    st.divider()
    if st.button("✨ Iniciar Novo Pedido"):
        # Limpa o estado específico da sessão
        st.session_state.messages = [{"role": "assistant", "content": "Ok, vamos começar um novo pedido. Informe os dados."}]
        if st.session_state.get("orchestrator"):
            st.session_state.orchestrator._reset_state_data()
        # Limpa os estados pendentes
        st.session_state.pending_upload_data = None
        st.session_state.pending_mic_data = None
        st.session_state.pending_text_input = None
        st.session_state.input_processed_flag = False
        st.session_state.run_id += 1
        print("--- Estado resetado via botão ---")
        st.rerun()

    st.divider()
    st.header("Upload de Áudio (Teste)")
    # File Uploader - Apenas DEFINE o estado pendente
    uploaded_file = st.file_uploader(
        "Enviar arquivo de áudio",
        type=['wav', 'mp3', 'm4a', 'ogg', 'aac', 'flac'],
        key=f'file_uploader_{st.session_state.run_id}'
    )
    # Se um novo arquivo for carregado, armazena seus dados para processamento posterior
    if uploaded_file is not None:
        # CORREÇÃO AQUI: Compara pelo NOME, não por um ID inexistente
        # Armazena se não houver nada pendente OU se o nome do arquivo atual
        # for DIFERENTE do nome do arquivo já pendente.
        if st.session_state.pending_upload_data is None or \
           st.session_state.pending_upload_data.get("name") != uploaded_file.name:
            logger.info(f"Arquivo '{uploaded_file.name}' detectado pelo uploader. Armazenando em pending_upload_data.")
            try:
                # Lê os bytes aqui para armazenar
                file_bytes = uploaded_file.read()
                st.session_state.pending_upload_data = {
                    "bytes": file_bytes,
                    "name": uploaded_file.name
                    # Removido o campo "id"
                }
                logger.debug(f"pending_upload_data definido com nome: {uploaded_file.name}")
                # Opcional: Adicionar um st.rerun() aqui pode forçar a lógica de consumo
                # a rodar imediatamente, mas pode causar piscadas na UI. Testar sem primeiro.
                # st.rerun()
            except Exception as e:
                 logger.error(f"Erro ao ler bytes do arquivo '{uploaded_file.name}': {e}", exc_info=True)
                 st.error(f"Erro ao tentar ler o arquivo {uploaded_file.name}.")
                 st.session_state.pending_upload_data = None # Limpa se a leitura falhar

# --- Widgets de Input (Mic e Texto) ---
# Mic Recorder - Apenas DEFINE o estado pendente
if audio_transcriber and st.session_state.orchestrator:
    audio_info = mic_recorder(
        start_prompt="🎤 Gravar",
        stop_prompt="⏹️ Parar",
        just_once=False, # Permite gravar novamente sem refresh completo
        use_container_width=True,
        key=f'mic_recorder_widget_{st.session_state.run_id}'
    )
    if audio_info and isinstance(audio_info, dict) and 'bytes' in audio_info:
        audio_bytes = audio_info['bytes']
        # Verifica se o áudio é novo (evita reprocessar bytes vazios ou o mesmo áudio repetidamente)
        if audio_bytes and st.session_state.pending_mic_data is None:
             logger.info(f"Áudio gravado ({len(audio_bytes)} bytes) detectado. Armazenando em pending_mic_data.")
             st.session_state.pending_mic_data = {"bytes": audio_bytes}
             # NÃO processa aqui

elif not audio_transcriber:
    st.warning("Gravação/Transcrição de áudio desabilitada (verifique API Key).")

# Chat Input - Apenas DEFINE o estado pendente
prompt = st.chat_input("Digite os dados ou sua resposta aqui...")
if prompt and st.session_state.pending_text_input is None:
    logger.info(f"Texto '{prompt}' detectado pelo chat_input. Armazenando em pending_text_input.")
    st.session_state.pending_text_input = prompt
    # NÃO processa aqui

# --- Lógica Central de Processamento (Executa UMA VEZ por ciclo de interação) ---
input_to_process = None
input_source = None
source_info = None # Para logging e display (ex: nome do arquivo)

# Verifica e CONSOME o input pendente (prioridade: upload > mic > texto)
if st.session_state.pending_text_input:
    logger.info("Processando pending_text_input...")
    input_source = 'text'
    input_to_process = st.session_state.pending_text_input
    st.session_state.pending_text_input = None # CONSOME o input
    st.session_state.input_processed_flag = True
    source_info = "texto digitado"

elif st.session_state.pending_upload_data:
    logger.info("Processando pending_upload_data...")
    input_source = 'file'
    upload_data = st.session_state.pending_upload_data
    st.session_state.pending_upload_data = None # CONSOME o input
    st.session_state.input_processed_flag = True # Marca que processamos algo
    source_info = upload_data['name']
    if audio_transcriber:
        with st.spinner(f"Transcrevendo arquivo '{source_info}'..."):
            input_to_process = audio_transcriber.transcribe_audio(upload_data['bytes'], filename=source_info)
            if not input_to_process:
                 st.error(f"Falha ao transcrever '{source_info}'.")
                 logger.warning(f"Transcrição falhou para upload {source_info}")
    else:
        st.error("Transcritor não disponível para processar upload.")

elif st.session_state.pending_mic_data:
    logger.info("Processando pending_mic_data...")
    input_source = 'audio'
    mic_data = st.session_state.pending_mic_data
    st.session_state.pending_mic_data = None # CONSOME o input
    st.session_state.input_processed_flag = True
    source_info = "áudio gravado"
    if audio_transcriber:
        with st.spinner("Transcrevendo áudio gravado..."):
             input_to_process = audio_transcriber.transcribe_audio(mic_data['bytes'], filename="mic_audio.wav")
             if not input_to_process:
                  st.error("Falha ao transcrever áudio gravado.")
                  logger.warning("Transcrição falhou para áudio gravado")
    else:
        st.error("Transcritor não disponível para processar áudio gravado.")

# Só executa a lógica do orchestrator se um input válido foi consumido e processado
if input_to_process and input_source:
    logger.info(f"Input consumido e pronto para o Orchestrator. Fonte: {input_source}, Info: {source_info}")

    # Mostra input do usuário no chat ANTES de chamar o bot
    display_content = input_to_process
    if input_source == "audio":
        display_content = f"(Áudio🎙️): {input_to_process}"
    elif input_source == "file":
        display_content = f"(Arquivo 📁 '{source_info}'): {input_to_process}"
    # Adiciona ao histórico ANTES de exibir, para manter ordem
    st.session_state.messages.append({"role": "user", "content": display_content})

    # Exibe na interface (Atualização da UI)
    with st.chat_message("user"):
        st.write(display_content)

    # Chama o Orchestrator
    if st.session_state.orchestrator:
        # --- DEBUG ADICIONAL: Logar estado ANTES de chamar process_user_input ---
        logger.debug(f"Orchestrator state ANTES de processar '{input_source}': {st.session_state.orchestrator.get_state_dict()}")
        # -------------------------------------------------------------------------
        with st.spinner("Processando..."):
            try:
                response = st.session_state.orchestrator.process_user_input(input_to_process)
                bot_message_content = response.get('message', "Desculpe, ocorreu um erro interno.")
                status = response.get("status", "error")
                # --- DEBUG ADICIONAL: Logar estado DEPOIS de chamar process_user_input ---
                logger.debug(f"Orchestrator state DEPOIS de processar '{input_source}': {st.session_state.orchestrator.get_state_dict()}")
                logger.debug(f"Orchestrator response: Status={status}, Msg='{bot_message_content[:100]}...'")
                # --------------------------------------------------------------------------
            except Exception as e:
                bot_message_content = f"Erro durante processamento pelo chatbot: {e}"
                status = "error"
                logger.error(f"Erro no orchestrator.process_user_input: {e}", exc_info=True)

        # Mostra resposta do bot
        display_message = bot_message_content
        if status == "confirmed_for_creation":
             display_message = "Pedido confirmado! (Simulando criação de chamado)."

        with st.chat_message("assistant"):
            resposta_formatada_md = display_message.replace('\n', '  \n')
            st.markdown(resposta_formatada_md)
        st.session_state.messages.append({"role": "assistant", "content": display_message})

        # Reset do estado para fluxos terminais
        if status in ["completed", "aborted", "confirmed_for_creation", "error"]:
             st.info(f"Processo finalizado (status: {status}). Pronto para novo pedido.")
             if st.session_state.get("orchestrator"):
                 st.session_state.orchestrator._reset_state_data()
             st.session_state.pending_upload_data = None
             st.session_state.pending_mic_data = None
             st.session_state.pending_text_input = None
             st.session_state.input_processed_flag = False
             st.session_state.run_id += 1
             logger.info(f"Estado resetado após status terminal: {status}. Forçando rerun.")
             st.rerun()

    else:
        error_msg = "Chatbot indisponível (erro de inicialização)."
        with st.chat_message("assistant"): st.error(error_msg)
        st.session_state.messages.append({"role": "assistant", "content": error_msg})

# --- FIM DA LÓGICA DE PROCESSAMENTO ---

# Reseta o flag de processamento ao final de cada execução completa do script
# Isso garante que na próxima interação (se não for terminal), o sistema esteja
# pronto para detectar um NOVO input.
st.session_state.input_processed_flag = False

# Adiciona espaço no final
st.markdown("<div style='margin-bottom: 50px;'></div>", unsafe_allow_html=True)

--- Conteúdo de src/config.py ---

# src/config.py
import os
from dotenv import load_dotenv

load_dotenv() # Carrega variáveis do arquivo .env

# Configurações Gerais
ARTIFACTS_DIR = os.getenv("ARTIFACTS_DIR", "../artifacts") # Diretório onde estão os CSVs

# --- Configurações LLM ---
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai").lower() # "openai" ou "google_genai"
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", 0.0))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", 1024)) # Max tokens para a resposta do LLM

# OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL_NAME = os.getenv("OPENAI_MODEL_NAME", "gpt-3.5-turbo-instruct") # Modelo de completions

# Google Gemini
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL_NAME", "gemini-2.5-flash-preview-04-17") # Modelo Gemini (Flash é bom para velocidade/custo)
                                                                         # Poderia ser "gemini-pro" se preferir

AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY") # Sua API Key do Azure OpenAI
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT") # Ex: "https://your-resource-name.openai.azure.com/"
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME") # O nome da SUA implantação do gpt-3.5-turbo-instruct
AZURE_MODEL_NAME = os.getenv("AZURE_MODEL_NAME") # O nome do modelo a ser usado no Azure OpenAI
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION") # A API version correta, ex: "2024-02-15-preview"

--- Conteúdo de src/topdesk_client.py ---

import requests
from requests.auth import HTTPBasicAuth
import os
import dotenv
import html
import json
import logging # Adicionado logging

dotenv.load_dotenv()
logger = logging.getLogger(__name__) # Adicionado logger

TOPDESK_API_URL = "https://fsbioenergia.topdesk.net/tas/api/incidents"
# IDs fixos (considerar mover para config.py ou .env se mudarem com frequência)
DEFAULT_CALLER_ID = "81341708-e497-4334-ae3a-7a38282761e8"
DEFAULT_CATEGORY_ID = "26b435cc-dc0c-4634-92e6-fc8958302647"
DEFAULT_SUBCATEGORY_ID = "5cce0afb-3c41-4e10-873f-35480e3b08b9"
DEFAULT_CALLTYPE_ID = "b46bd95d-1b4b-5667-bf6a-86531696c8cc"
DEFAULT_IMPACT_ID = "b919251c-22ce-5384-8377-8f220eb8e76e"
DEFAULT_URGENCY_ID = "5ed5103a-176b-4bc3-bc3c-a0058e855d6b"
DEFAULT_OPERATOR_ID = "a690d68b-0fd1-4ecb-8b71-c271f8d774b6" # Mesmo ID para grupo/operador? Verificar.
DEFAULT_OPERATOR_GROUP_ID = "a690d68b-0fd1-4ecb-8b71-c271f8d774b6"

class TopDeskClient:
    def __init__(self, username=None, password=None, base_url=TOPDESK_API_URL):
        self.username = username or os.getenv("TOPDESK_USERNAME")
        self.password = password or os.getenv("TOPDESK_PASSWORD")
        self.base_url = base_url

        if not self.username or not self.password:
            raise ValueError("Credenciais do TopDesk (usuário/senha) não fornecidas via argumento ou .env")

    def _get_auth(self):
        """Retorna o objeto de autenticação."""
        return HTTPBasicAuth(self.username, self.password)

    def _build_payload(self, request_text: str) -> dict:
        """Constrói o payload JSON para a criação do incidente."""

        request_html = html.escape(request_text).replace("\n", "<br>")


        payload = {
            "caller": {"id": DEFAULT_CALLER_ID},
            "status": "firstLine",
            "briefDescription": "PREÇO FIXO",
            "request": request_html,
            "category": {"id": DEFAULT_CATEGORY_ID},
            "subcategory": {"id": DEFAULT_SUBCATEGORY_ID},
            "entryType": {"name": "Portal"},
            "callType": {"id": DEFAULT_CALLTYPE_ID},
            "impact": {"id": DEFAULT_IMPACT_ID},
            "urgency": {"id": DEFAULT_URGENCY_ID},
            "processingStatus": {"name": "Registrado"},
            "operator": {"id": DEFAULT_OPERATOR_ID},
            "operatorGroup": {"id": DEFAULT_OPERATOR_GROUP_ID}
        }
        return payload

    def create_incident(self, request_text: str) -> str | None:
        """
        Cria um incidente no TopDesk com a descrição fornecida.

        Args:
            request_text: A descrição textual do incidente (será formatada para HTML).

        Returns:
            O número do ticket criado em caso de sucesso, ou None em caso de erro.
        """
        auth = self._get_auth()
        payload = self._build_payload(request_text)
        headers = {'Content-Type': 'application/json'}

        try:
            logger.debug(f"Enviando requisição para TopDesk: URL={self.base_url}, Payload={json.dumps(payload)}")
            response = requests.post(self.base_url, headers=headers, json=payload, auth=auth)
            response.raise_for_status() # Lança exceção para erros HTTP 4xx/5xx

            response_json = response.json()
            ticket_number = response_json.get("number")
            if ticket_number:
                logger.info(f"Incidente criado com sucesso no TopDesk. Ticket: {ticket_number}")
                return ticket_number
            else:
                logger.error(f"Resposta OK do TopDesk, mas sem número do ticket. Resposta: {response_json}")
                return None

        except requests.exceptions.HTTPError as http_err:
            logger.error(f"Erro HTTP ao criar incidente no TopDesk: {http_err.response.status_code} - {http_err.response.text}", exc_info=True)
            return None
        except requests.exceptions.RequestException as req_err:
            logger.error(f"Erro de requisição ao criar incidente no TopDesk: {req_err}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"Erro inesperado ao criar incidente no TopDesk: {e}", exc_info=True)
            return None

# Exemplo de uso (opcional)
if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG) # Habilita debug para teste
    try:
        client = TopDeskClient()
        ticket = client.create_incident("Este é um chamado de teste\nCom múltiplas linhas\nCriado via script.")
        if ticket:
            print(f"Ticket de teste criado: {ticket}")
        else:
            print("Falha ao criar ticket de teste.")
    except ValueError as ve:
        print(f"Erro de configuração: {ve}")
    except Exception as ex:
        print(f"Erro durante teste: {ex}")


--- Conteúdo de src/whatsapp_integration.py ---

# src/whatsapp_integration.py

import os
import json
import time
import logging
from flask import Flask, request, jsonify
import requests
from dotenv import load_dotenv
import io # Necessário para BytesIO

# Importações da arquitetura
from agents.extraction_agent import ExtractionAgent
from agents.mapping_agent import MappingAgent
from agents.orchestration_agent import OrchestrationAgent
from utils.formatting import format_final_summary_text
from utils.transcription import AudioTranscriber # Importa o transcritor
from topdesk_client import TopDeskClient
import config

# Configuração do Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()
app = Flask(__name__)

# --- Constantes e Configurações ---
WEBHOOK_VERIFY_TOKEN = os.getenv("WEBHOOK_VERIFY_TOKEN")
GRAPH_API_TOKEN = os.getenv("GRAPH_API_TOKEN")
PORT = int(os.getenv("WEBHOOK_PORT", 8000))
FACEBOOK_GRAPH_URL = f"https://graph.facebook.com/v18.0" # Ajuste a versão se necessário
SESSION_DIR = "whatsapp_sessions"
SESSION_TTL = 3600 * 4 # 4 horas

# Cria diretório de sessão se não existir
os.makedirs(SESSION_DIR, exist_ok=True)

# --- Inicialização dos Componentes Stateless ---
STATELESS_AGENTS_READY = False
audio_transcriber = None # Inicializa como None

try:
    extraction_agent = ExtractionAgent()
    mapping_agent = MappingAgent(artifacts_dir=config.ARTIFACTS_DIR)
    topdesk_client = TopDeskClient()

    if config.OPENAI_API_KEY: # Verifica se a chave existe para o transcritor
        audio_transcriber = AudioTranscriber()
        logger.info("AudioTranscriber inicializado.")
    else:
        logger.warning("OPENAI_API_KEY não configurada. Transcrição de áudio via WhatsApp estará desabilitada.")

    STATELESS_AGENTS_READY = mapping_agent.data_loaded_successfully # Checa se mapeamento carregou
    if not STATELESS_AGENTS_READY:
         logger.warning("Agente de Mapeamento não carregou dados CSV. Funcionalidade limitada.")
    elif audio_transcriber is None and config.OPENAI_API_KEY: # Se a key existe mas o transcritor falhou
        logger.error("AudioTranscriber falhou na inicialização mesmo com API Key. Verifique logs anteriores.")
        # Não impede STATELESS_AGENTS_READY, mas a transcrição não funcionará.
    elif audio_transcriber:
        logger.info("Todos os agentes principais (incluindo AudioTranscriber, se configurado) estão prontos.")


except (ValueError, FileNotFoundError, IOError) as e:
    logger.critical(f"Falha crítica ao inicializar agentes stateless ou cliente TopDesk: {e}", exc_info=True)
    STATELESS_AGENTS_READY = False
except Exception as e: # Pega outras exceções inesperadas na inicialização
    logger.critical(f"Erro inesperado durante inicialização: {e}", exc_info=True)
    STATELESS_AGENTS_READY = False


# --- Funções Auxiliares de Persistência de Sessão ---
# (Nenhuma mudança aqui, _get_session_path, save_state, load_state, clear_state permanecem iguais)
def _get_session_path(wa_id: str) -> str:
    """Retorna o caminho completo para o arquivo de sessão do usuário."""
    # Sanitiza wa_id para evitar problemas com caracteres especiais no nome do arquivo, se necessário
    safe_wa_id = "".join(c for c in wa_id if c.isalnum() or c in ('-', '_')).rstrip()
    return os.path.join(SESSION_DIR, f"session_{safe_wa_id}.json")

def save_state(wa_id: str, state_dict: dict):
    """Salva o dicionário de estado do usuário em um arquivo JSON."""
    filepath = _get_session_path(wa_id)
    try:
        state_dict['_timestamp'] = time.time() # Adiciona timestamp para TTL
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(state_dict, f, ensure_ascii=False, indent=4)
        logger.debug(f"Estado salvo para {wa_id} em {filepath}")
    except IOError as e:
        logger.error(f"Erro de IO ao salvar estado para {wa_id}: {e}", exc_info=True)
    except TypeError as e:
        logger.error(f"Erro de tipo ao serializar estado para {wa_id}: {e}. Estado: {state_dict}", exc_info=True)

def load_state(wa_id: str) -> dict | None:
    """Carrega o dicionário de estado do arquivo JSON, verificando TTL."""
    filepath = _get_session_path(wa_id)
    if not os.path.exists(filepath):
        return None
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            state_dict = json.load(f)

        timestamp = state_dict.get('_timestamp', 0)
        if time.time() - timestamp > SESSION_TTL:
            logger.info(f"Estado para {wa_id} expirou (TTL {SESSION_TTL}s). Removendo.")
            clear_state(wa_id)
            return None

        # Remove o timestamp antes de retornar para o orchestrator
        state_dict.pop('_timestamp', None)
        logger.debug(f"Estado carregado para {wa_id} de {filepath}")
        return state_dict
    except (IOError, json.JSONDecodeError) as e:
        logger.error(f"Erro ao carregar/decodificar estado para {wa_id} de {filepath}: {e}. Removendo arquivo corrompido.", exc_info=True)
        clear_state(wa_id)
        return None

def clear_state(wa_id: str):
    """Remove o arquivo de estado do usuário."""
    filepath = _get_session_path(wa_id)
    if os.path.exists(filepath):
        try:
            os.remove(filepath)
            logger.info(f"Estado removido para {wa_id} ({filepath}).")
        except OSError as e:
            logger.error(f"Erro ao remover arquivo de estado {filepath}: {e}", exc_info=True)


# --- Função Auxiliar de Envio WhatsApp ---
# (Nenhuma mudança aqui, send_whatsapp_message permanece igual)
def send_whatsapp_message(phone_number_id, to_wa_id, message_body=None, interactive_payload=None, context_message_id=None):
    """Envia uma mensagem (texto ou interativa) via WhatsApp Graph API."""
    if not GRAPH_API_TOKEN:
        logger.error("GRAPH_API_TOKEN não configurado.")
        return False
    if not phone_number_id:
        logger.error("phone_number_id não fornecido para envio.")
        return False

    url = f"{FACEBOOK_GRAPH_URL}/{phone_number_id}/messages"
    headers = {"Authorization": f"Bearer {GRAPH_API_TOKEN}", "Content-Type": "application/json"}
    json_data = {
        "messaging_product": "whatsapp",
        "recipient_type": "individual",
        "to": to_wa_id,
    }
    # Adiciona contexto (para responder a uma mensagem específica) se fornecido
    if context_message_id:
        json_data["context"] = {"message_id": context_message_id}

    # Define o tipo e o conteúdo da mensagem
    if interactive_payload:
        json_data["type"] = "interactive"
        json_data["interactive"] = interactive_payload
    elif message_body:
        json_data["type"] = "text"
        json_data["text"] = {"body": message_body}
    else:
        logger.warning(f"Tentativa de enviar mensagem para {to_wa_id} sem corpo ou payload interativo.")
        return False

    # Envia a requisição
    try:
        response = requests.post(url, headers=headers, json=json_data, timeout=15) # Adicionado timeout
        response.raise_for_status()
        logger.info(f"Mensagem enviada para {to_wa_id}. Status: {response.status_code}.")
        logger.debug(f"Detalhe envio Wpp: URL={url} Payload={json.dumps(json_data)} Resp={response.text}")
        return True
    except requests.exceptions.Timeout:
         logger.error(f"Timeout ao enviar mensagem para {to_wa_id}", exc_info=True)
         return False
    except requests.exceptions.RequestException as e:
        logger.error(f"Erro ao enviar mensagem para {to_wa_id}: {e}", exc_info=True)
        if e.response is not None:
             logger.error(f"Resposta do erro: {e.response.status_code} - {e.response.text}")
        return False

# --- Nova Função: Download de Mídia do WhatsApp ---
def _download_whatsapp_media(media_id: str) -> bytes | None:
    """Baixa um arquivo de mídia do WhatsApp usando seu ID."""
    if not GRAPH_API_TOKEN:
        logger.error("GRAPH_API_TOKEN não configurado para download de mídia.")
        return None

    # 1. Obter a URL da mídia
    media_info_url = f"{FACEBOOK_GRAPH_URL}/{media_id}"
    headers = {"Authorization": f"Bearer {GRAPH_API_TOKEN}"}
    media_url = None
    mime_type = None

    try:
        response_info = requests.get(media_info_url, headers=headers, timeout=10)
        response_info.raise_for_status()
        media_data = response_info.json()
        media_url = media_data.get("url")
        mime_type = media_data.get("mime_type")
        logger.debug(f"Informações da mídia {media_id}: {media_data}")
        if not media_url:
            logger.error(f"Não foi possível obter a URL de download para media_id {media_id}. Resposta: {media_data}")
            return None
    except requests.exceptions.Timeout:
        logger.error(f"Timeout ao obter URL da mídia {media_id}.", exc_info=True)
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"Erro ao obter URL da mídia {media_id}: {e}", exc_info=True)
        if e.response is not None:
            logger.error(f"Resposta do erro (media URL): {e.response.status_code} - {e.response.text}")
        return None

    # 2. Baixar o arquivo de mídia
    try:
        # O token também é necessário para acessar a URL da mídia
        response_download = requests.get(media_url, headers=headers, timeout=20) # Timeout maior para download
        response_download.raise_for_status()
        logger.info(f"Mídia {media_id} (tipo: {mime_type}) baixada com sucesso ({len(response_download.content)} bytes).")
        return response_download.content
    except requests.exceptions.Timeout:
        logger.error(f"Timeout ao baixar mídia de {media_url}.", exc_info=True)
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"Erro ao baixar mídia de {media_url}: {e}", exc_info=True)
        if e.response is not None:
            logger.error(f"Resposta do erro (download): {e.response.status_code} - {e.response.text}")
        return None

# --- Funções de Lógica de Negócio ---
# (get_orchestrator_instance, _create_topdesk_ticket_and_reply, _handle_orchestrator_response permanecem iguais)
def get_orchestrator_instance(wa_id: str) -> OrchestrationAgent:
    """Cria uma instância do OrchestrationAgent e carrega seu estado."""
    if not STATELESS_AGENTS_READY:
        raise RuntimeError("Agentes stateless não estão prontos ou falharam na inicialização.")

    orchestrator = OrchestrationAgent(extraction_agent, mapping_agent)
    persisted_state = load_state(wa_id)
    if persisted_state:
        orchestrator.load_state(persisted_state)
        logger.info(f"Estado anterior restaurado para Orchestrator de {wa_id}.")
    else:
        logger.info(f"Nenhum estado anterior encontrado para {wa_id}. Usando estado padrão.")
    return orchestrator

def _create_topdesk_ticket_and_reply(payload: dict, wa_id: str, business_phone_number_id: str) -> bool:
    """Tenta criar o ticket no TopDesk e envia a resposta ao usuário."""
    success = False
    reply_body = ""
    try:
        cadencia_fmt = payload.get("Cadencia_Formatada", "") # Cadência já formatada pelo orchestrator
        formatted_string = format_final_summary_text(payload, cadencia_fmt)
        logger.debug(f"String formatada para TopDesk (ID: {wa_id}):\n{formatted_string}")

        ticket_id = topdesk_client.create_incident(formatted_string)
        if ticket_id:
            reply_body = f"Chamado criado com sucesso! Número: {ticket_id}"
            logger.info(f"Ticket {ticket_id} criado para {wa_id}.")
            success = True
        else:
            reply_body = "Ocorreu um erro ao criar o chamado no TopDesk. A equipe responsável foi notificada."
            logger.error(f"Falha ao criar ticket no TopDesk para {wa_id} (API retornou None/False).")
            # Aqui poderia ter uma notificação interna para a equipe de suporte sobre a falha
            success = False # Indica que o ticket não foi criado
    except Exception as e:
        logger.error(f"Exceção ao formatar ou chamar TopDesk para {wa_id}: {e}", exc_info=True)
        reply_body = "Ocorreu um erro interno ao processar a criação do chamado. Tente novamente mais tarde."
        success = False

    # Envia resposta final ao usuário
    send_whatsapp_message(phone_number_id=business_phone_number_id, to_wa_id=wa_id, message_body=reply_body)
    return success

def _handle_orchestrator_response(response_dict: dict, wa_id: str, message_id: str, business_phone_number_id: str):
    """Processa a resposta do orchestrator e envia a mensagem apropriada ao usuário."""
    status = response_dict.get("status")
    reply_body = response_dict.get("message", "Ocorreu um erro inesperado.")
    payload = response_dict.get("payload") # Payload para criação do ticket

    logger.info(f"Resposta do Orquestrador para {wa_id} - Status: {status}")

    if status == "needs_confirmation":
        # Prepara botões interativos
        interactive = {
            "type": "button",
            "body": {"text": reply_body}, # Mensagem vem do orchestrator com instruções
            "action": {
                "buttons": [
                    {"type": "reply", "reply": {"id": "confirm_yes", "title": "Sim, Confirmar"}},
                    {"type": "reply", "reply": {"id": "confirm_edit", "title": "Corrigir/Alterar"}},
                    {"type": "reply", "reply": {"id": "confirm_full_cancel", "title": "Cancelar e Novo"}}
                    # Poderia ter botão "Cancelar" explícito:
                    # {"type": "reply", "reply": {"id": "confirm_cancel", "title": "Cancelar Pedido"}}
                ]
            }
        }
        send_whatsapp_message(
            phone_number_id=business_phone_number_id, to_wa_id=wa_id,
            interactive_payload=interactive
        )
        # Estado é salvo após a chamada a esta função

    elif status == "needs_input":
        send_whatsapp_message(
            phone_number_id=business_phone_number_id, to_wa_id=wa_id,
            message_body=reply_body, context_message_id=message_id
        )
        # Estado é salvo após a chamada a esta função

    elif status == "confirmed_for_creation":
        logger.info(f"Orquestrador confirmou dados para {wa_id}. Tentando criar ticket.")
        if payload:
            ticket_created = _create_topdesk_ticket_and_reply(payload, wa_id, business_phone_number_id)
            # O estado já foi limpo pelo orchestrator ao retornar este status
            # e será salvo como vazio (ou não salvo se clear_state for chamado explicitamente)
            if ticket_created:
                 logger.info(f"Fluxo concluído com sucesso (ticket criado) para {wa_id}.")
                 clear_state(wa_id)
                 # clear_state(wa_id) # Garante limpeza se o reset do orchestrator falhar
            else:
                 logger.warning(f"Fluxo concluído com falha na criação do ticket para {wa_id}.")
                 clear_state(wa_id)
                 # clear_state(wa_id) # Limpa estado mesmo se falhou em criar ticket
        else:
            logger.error(f"Status 'confirmed_for_creation' para {wa_id} sem payload!")
            send_whatsapp_message(phone_number_id=business_phone_number_id, to_wa_id=wa_id, message_body="Erro interno: dados finais não encontrados.")
            clear_state(wa_id) # Limpa estado em caso de erro interno grave

    elif status == "completed": # Sucesso sem criação de ticket (ex: só consulta) - não usado atualmente
        send_whatsapp_message(phone_number_id=business_phone_number_id, to_wa_id=wa_id, message_body=reply_body)
        clear_state(wa_id) # Limpa estado ao completar

    elif status == "aborted":
        send_whatsapp_message(phone_number_id=business_phone_number_id, to_wa_id=wa_id, message_body=reply_body)
        clear_state(wa_id) # Estado já limpo pelo orchestrator

    elif status == "error":
        logger.error(f"Orquestrador retornou erro para {wa_id}: {reply_body}")
        send_whatsapp_message(phone_number_id=business_phone_number_id, to_wa_id=wa_id, message_body=reply_body)
        clear_state(wa_id) # Limpa estado em caso de erro

    else:
        logger.warning(f"Status desconhecido '{status}' do Orquestrador para {wa_id}.")
        send_whatsapp_message(phone_number_id=business_phone_number_id, to_wa_id=wa_id, message_body="Desculpe, algo inesperado aconteceu.")
        clear_state(wa_id)

# _process_text_message permanece igual
def _process_text_message(user_text: str, wa_id: str, message_id: str, business_phone_number_id: str):
    """Processa uma mensagem de texto recebida."""
    if not user_text:
        logger.info(f"Mensagem de texto vazia de {wa_id}. Ignorando.")
        return

    logger.info(f"Processando texto de {wa_id}: '{user_text}'")
    try:
        orchestrator = get_orchestrator_instance(wa_id)
        metadata = {'vendedor_id': wa_id}
        response_dict = orchestrator.process_user_input(user_text, metadata=metadata)

        # Envia a resposta ao usuário com base no status
        # _handle_orchestrator_response AGORA lida com clear_state
        _handle_orchestrator_response(response_dict, wa_id, message_id, business_phone_number_id)

        # Salva o estado ATUALIZADO do orchestrator, A MENOS QUE tenha sido um fluxo terminal
        # que já limpou o estado (confirmed_for_creation, aborted, completed, error).
        # O estado do orchestrator na memória já foi resetado por ele mesmo nesses casos.
        # Se clear_state foi chamado, o arquivo não existe mais.
        # Se o arquivo ainda existe (ex: needs_input, needs_confirmation), salvamos.
        status = response_dict.get("status")
        if status not in ["confirmed_for_creation", "completed", "aborted", "error"]:
            # Se o orchestrator resetou seu estado interno (ex: em _format_confirmed_for_creation ANTES de _create_topdesk_ticket_and_reply)
            # E nós NÃO chamamos clear_state(), então save_state() vai salvar um estado limpo.
            # Se NÓS chamamos clear_state(), então o arquivo já foi removido, e save_state() irá recriá-lo como limpo.
            # Isso é seguro. O importante é que após um fluxo terminal, o estado salvo seja o inicial.
            save_state(wa_id, orchestrator.get_state_dict())
        else:
            # Se o clear_state foi chamado em _handle_orchestrator_response, o arquivo já foi removido.
            # Se não foi (ex: falha na criação do ticket e você decidiu não limpar),
            # o estado interno do orchestrator já foi resetado, então save_state
            # (se fosse chamado aqui) salvaria um estado limpo.
            # Como queremos garantir que o arquivo é removido nesses casos,
            # o clear_state() dentro do _handle_orchestrator_response é a melhor abordagem.
            logger.debug(f"Estado para {wa_id} não será salvo explicitamente aqui pois status é terminal ({status}), clear_state já foi chamado se necessário.")


    except RuntimeError as e: # Erro ao carregar/instanciar agentes
        logger.error(f"Erro de Runtime ao processar texto para {wa_id}: {e}", exc_info=True)
        send_whatsapp_message(
             phone_number_id=business_phone_number_id, to_wa_id=wa_id,
             message_body="Desculpe, o serviço está temporariamente indisponível devido a um erro interno.",
             context_message_id=message_id)
        clear_state(wa_id) # Limpa estado em caso de falha grave
    except Exception as e:
        logger.error(f"Erro inesperado ao processar texto '{user_text}' de {wa_id}: {e}", exc_info=True)
        send_whatsapp_message(
             phone_number_id=business_phone_number_id, to_wa_id=wa_id,
             message_body="Desculpe, ocorreu um erro interno inesperado. Tente novamente mais tarde.",
             context_message_id=message_id)
        clear_state(wa_id) # Limpa estado

# _handle_interactive_message permanece igual
def _handle_interactive_message(message_data: dict, wa_id: str, message_id: str, business_phone_number_id: str):
    """Processa uma resposta de botão interativo."""
    interactive_data = message_data.get("interactive", {})
    button_reply = interactive_data.get("button_reply")
    if not button_reply:
        logger.warning(f"Mensagem interativa de {wa_id} sem button_reply.")
        return

    button_id = button_reply.get("id")
    button_title = button_reply.get("title")
    logger.info(f"Botão '{button_title}' (ID: {button_id}) pressionado por {wa_id}.")

    # Mapeia IDs de botão para texto que o orchestrator entende
    text_equivalent = None
    if button_id == "confirm_yes":
        # Processa confirmação como antes
        _process_text_message("Sim", wa_id, message_id, business_phone_number_id)

    elif button_id == "confirm_edit":
        # === NOVO TRATAMENTO PARA O BOTÃO CORRIGIR ===
        logger.info(f"Botão 'Corrigir' (confirm_edit) detectado para {wa_id}. Solicitando input de correção.")
        orchestrator_state = load_state(wa_id) # Carrega o estado atual

        if orchestrator_state and orchestrator_state.get("pending_confirmation"):
            # Modifica o estado para aguardar a correção
            orchestrator_state["pending_confirmation"] = False # Sai do loop de confirmação
            orchestrator_state["last_question_context"] = "awaiting_user_correction_text" # Novo contexto
            orchestrator_state["last_asked_fields"] = None # Não está pedindo campos específicos agora
            # Mantém orchestrator_state["pending_confirmation_payload"] intacto!

            save_state(wa_id, orchestrator_state) # Salva o estado modificado

            # Envia a instrução para o usuário digitar a correção
            reply_body = "Ok, por favor, digite APENAS a informação que deseja corrigir (ex: 'Cidade é Cuiabá', 'Preço Frete 500')."
            send_whatsapp_message(
                phone_number_id=business_phone_number_id, to_wa_id=wa_id,
                message_body=reply_body, context_message_id=message_id # Responde à msg do botão
            )
        else:
            logger.warning(f"Botão 'confirm_edit' recebido para {wa_id}, mas estado não era 'pending_confirmation' ou falhou ao carregar.")
            # Fallback: Informar erro ao usuário
            send_whatsapp_message(
                phone_number_id=business_phone_number_id, to_wa_id=wa_id,
                message_body="Houve um problema ao tentar iniciar a correção. Por favor, tente confirmar ou cancelar novamente.",
                context_message_id=message_id
            )
            # Considerar limpar o estado aqui se a situação for irrecuperável
            # clear_state(wa_id)

    elif button_id == "confirm_cancel": # Se você adicionar um botão "Cancelar"
         _process_text_message("Não", wa_id, message_id, business_phone_number_id)

    elif button_id == "confirm_full_cancel": # Novo tratamento
        logger.info(f"Botão 'Cancelar Pedido' (ID: {button_id}) pressionado por {wa_id}. Resetando estado.")
        clear_state(wa_id) # Limpa o estado da sessão do usuário
        reply_body = "Seu pedido foi cancelado. Para iniciar um novo pedido, por favor, envie os detalhes."
        send_whatsapp_message(
            phone_number_id=business_phone_number_id, to_wa_id=wa_id,
            message_body=reply_body, context_message_id=message_id # Responde à mensagem do botão
        )

    else:
        logger.warning(f"ID de botão não reconhecido '{button_id}' de {wa_id}.")
        # Enviar mensagem de erro ao usuário
        send_whatsapp_message(
            phone_number_id=business_phone_number_id, to_wa_id=wa_id,
            message_body="Desculpe, não reconheci essa opção.",
            context_message_id=message_id
        )

# --- Nova Função: Processar Mensagens de Áudio ---
def _process_audio_message(message_data: dict, wa_id: str, message_id: str, business_phone_number_id: str):
    """Processa uma mensagem de áudio recebida."""
    if not audio_transcriber:
        logger.warning(f"Mensagem de áudio recebida de {wa_id}, mas o AudioTranscriber não está disponível.")
        send_whatsapp_message(business_phone_number_id, wa_id,
                              "Desculpe, o processamento de áudio está temporariamente indisponível.", message_id)
        return

    audio_object = message_data.get("audio")
    if not audio_object:
        logger.warning(f"Mensagem tipo áudio de {wa_id} sem objeto 'audio'.")
        return

    media_id = audio_object.get("id")
    if not media_id:
        logger.warning(f"Mensagem de áudio de {wa_id} sem 'id' de mídia.")
        return

    logger.info(f"Processando áudio de {wa_id} (Media ID: {media_id}). Baixando...")
    audio_bytes = _download_whatsapp_media(media_id)

    if not audio_bytes:
        logger.error(f"Falha ao baixar áudio {media_id} de {wa_id}.")
        send_whatsapp_message(business_phone_number_id, wa_id,
                              "Não consegui baixar seu áudio. Por favor, tente enviar novamente.", message_id)
        return

    # O WhatsApp geralmente envia áudio em formato ogg com codec opus.
    # O Whisper lida bem com ogg, então podemos usar um nome de arquivo genérico.
    filename_for_transcription = f"whatsapp_audio_{media_id}.ogg"
    logger.info(f"Áudio baixado. Transcrevendo com {filename_for_transcription}...")

    # Envia uma mensagem de "processando" para o usuário (opcional, mas bom para UX)
    send_whatsapp_message(business_phone_number_id, wa_id,
                          "Recebi seu áudio, estou processando...", message_id)

    transcribed_text = audio_transcriber.transcribe_audio(audio_bytes, filename=filename_for_transcription)

    if transcribed_text is None or not transcribed_text.strip(): # Verifica se a transcrição não foi vazia
        logger.warning(f"Transcrição falhou ou resultou em texto vazio para áudio {media_id} de {wa_id}.")
        send_whatsapp_message(business_phone_number_id, wa_id,
                              "Não consegui entender o áudio. Poderia tentar novamente ou digitar os detalhes?", message_id)
        return

    logger.info(f"Áudio transcrito para {wa_id}: '{transcribed_text}'")
    # Agora, processa o texto transcrito como se fosse uma mensagem de texto normal
    _process_text_message(transcribed_text, wa_id, message_id, business_phone_number_id)


# --- Rotas do Webhook ---
@app.route("/webhook", methods=["POST"])
def webhook_post():
    """Recebe notificações de mensagens do WhatsApp."""
    if not STATELESS_AGENTS_READY:
         logger.critical("Webhook recebido, mas agentes não estão prontos. Retornando erro 503.")
         # Retorna 503 para indicar que o serviço está indisponível temporariamente
         return jsonify({"status": "error", "message": "Service temporarily unavailable"}), 503

    data = request.json
    logger.info("Webhook POST recebido.")
    logger.debug("Webhook payload: %s", json.dumps(data))

    # Validação básica do payload
    if not data or "entry" not in data or not data["entry"]:
        logger.warning("Payload do webhook vazio ou mal formatado.")
        return jsonify({"status": "error", "message": "Invalid payload"}), 400 # Bad request

    try:
        entry = data["entry"][0]
        changes = entry.get("changes", [{}])[0]
        value = changes.get("value", {})
        messages = value.get("messages")
        metadata_wpp = value.get("metadata", {}) # Renomeado para evitar conflito
        business_phone_number_id = metadata_wpp.get("phone_number_id")

        if not messages:
            # Pode ser um status de mensagem, etc. Ignorar por enquanto.
            logger.info("Notificação sem 'messages' recebida (ex: status de entrega). Ignorando.")
            return jsonify({"status": "ok", "message": "Notification received, no action needed"}), 200

        message = messages[0]
        wa_id = message.get("from")
        message_id = message.get("id")
        message_type = message.get("type")

        if not wa_id or not message_id or not business_phone_number_id:
             logger.warning(f"Mensagem recebida com dados essenciais faltando: wa_id={wa_id}, msg_id={message_id}, phone_id={business_phone_number_id}")
             return jsonify({"status": "error", "message": "Missing essential message data"}), 400

        # Processa com base no tipo de mensagem
        if message_type == "text":
            user_text = message.get("text", {}).get("body")
            _process_text_message(user_text, wa_id, message_id, business_phone_number_id)
        elif message_type == "interactive":
            _handle_interactive_message(message, wa_id, message_id, business_phone_number_id)
        elif message_type == "audio": # <<< NOVO HANDLER
            _process_audio_message(message, wa_id, message_id, business_phone_number_id)
        elif message_type == "image":
            caption = message.get("image", {}).get("caption")
            if caption:
                 logger.info(f"Imagem com legenda recebida de {wa_id}. Processando legenda.")
                 _process_text_message(caption, wa_id, message_id, business_phone_number_id)
            else:
                 logger.info(f"Imagem sem legenda recebida de {wa_id}. Ignorando.")
                 # Opcional: Enviar mensagem avisando que precisa de legenda
                 send_whatsapp_message(business_phone_number_id, wa_id, "Recebi sua imagem, mas preciso que você envie os detalhes do pedido como texto na legenda, por favor.", context_message_id=message_id)
        else:
            logger.info(f"Tipo de mensagem não tratado '{message_type}' recebido de {wa_id}. Ignorando.")
            # Opcional: Enviar mensagem avisando que só aceita texto/imagem com legenda
            send_whatsapp_message(business_phone_number_id, wa_id, "Desculpe, só consigo processar mensagens de texto, áudio ou imagens com os detalhes na legenda.", context_message_id=message_id)

        return jsonify({"status": "ok", "message": "Webhook processed"}), 200

    except IndexError:
         logger.warning("Estrutura inesperada no payload do webhook (sem 'entry' ou 'changes').")
         return jsonify({"status": "error", "message": "Invalid payload structure"}), 400
    except Exception as e:
        logger.error(f"Erro geral não capturado no processamento do webhook: {e}", exc_info=True)
        # Retorna 200 para evitar reenvios do WhatsApp em caso de erro interno não recuperável
        return jsonify({"status": "ok", "message": "Internal server error processing webhook"}), 200

# webhook_get permanece igual
@app.route("/webhook", methods=["GET"])
def webhook_get():
    """Verifica o token do webhook (necessário para configuração inicial)."""
    mode = request.args.get("hub.mode")
    token = request.args.get("hub.verify_token")
    challenge = request.args.get("hub.challenge")

    if mode == "subscribe" and token == WEBHOOK_VERIFY_TOKEN:
        logger.info("Webhook verificado com sucesso!")
        return challenge, 200
    else:
        logger.warning(f"Falha na verificação do webhook. Modo: {mode}, Token recebido: {token}")
        return "Forbidden", 403

# --- Inicialização do App ---
if __name__ == "__main__":
    if not WEBHOOK_VERIFY_TOKEN or not GRAPH_API_TOKEN:
        logger.warning("!!! Variáveis WEBHOOK_VERIFY_TOKEN ou GRAPH_API_TOKEN não definidas no .env !!!")
        # Não sair, mas alertar

    if not STATELESS_AGENTS_READY:
         logger.critical("!!! Agentes stateless não inicializados corretamente. O chatbot pode não funcionar. Verifique logs anteriores. !!!")
         # Não sair, mas alertar criticamente
    elif not config.OPENAI_API_KEY: # Adiciona um alerta específico se a chave para transcrição não estiver lá
        logger.warning("!!! OPENAI_API_KEY não configurada. A funcionalidade de transcrição de áudio via WhatsApp estará desabilitada. !!!")


    logger.info(f"Iniciando servidor Flask na porta {PORT}...")
    # Use Gunicorn ou outro servidor WSGI em produção em vez de app.run(debug=True)
    # Ex: gunicorn --bind 0.0.0.0:8000 whatsapp_integration:app
    # Para desenvolvimento local:
    app.run(host="0.0.0.0", port=PORT, debug=False) # debug=False é mais seguro

